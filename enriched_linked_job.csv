,company,company_url,job_title,job_url,job_id,location,work_type,salary,posted_at,is_easy_apply,applicant_count,description,apply_url,posted_ago
0,Motion Recruitment,https://www.linkedin.com/company/motion-recruitment-partners/life,Data Engineer,https://www.linkedin.com/jobs/view/4261001152,4261001152,"Mountain View, CA",Hybrid,$75/hr,2025-07-02 18:01:43,True,over 100 applicants,"Our client, a leader in tax software with amazing culture, is hiring for a Data Engineer II. This is a hybrid position with 3 days onsite per week in Mountain View, California.
This role will be responsible for developing and translating computer algorithms into prototype code and maintaining, organizing, and identifying trends in large data sets. Proficiency in SQL database design, proficiency in creating process documentation, strong written and verbal communication skills, and the ability to work independently and on teams. Familiarity with the computer coding languages Python, Java, Kafka, Hive, or Storm may be required in order to oversee real-time business metric aggregation, data warehousing and querying, schema and data management, and related duties. Should have knowledge of algorithms, data structures, and performance optimism and experience with processing and interpreting data sets. Develop technical solutions to improve access to data and data usage. Understand data needs and advise company on technological resources. Aggregate and analyze various data sets to provide actionable insight. Develop reports, dashboards, and tools for business-users.
Contract Duration: 12 Months
What You'll Be Doing:Design, build, and maintain scalable data pipelines using Databricks and Spark, ensuring optimal data extraction, transformation, and loading from diverse data sources.Employ robust data modeling techniques to develop and optimize databases and large-scale processing systems.Use GitHub for version control, ensuring efficient collaboration and codebase management within our development teams.Ensure high-quality data reliability and streamline data flow to support advanced querying and analytics.Collaborate with data scientists and architects to enhance system performance and data integrity.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Troubleshoot and debug issues in data models, guaranteeing high performance and accessibility of our data systems.
Required Experience/Skills:Minimum of 3-4 years of experience as a Data Engineer.Proficiency in Databricks, Spark, SQL, and Python—must have solid experience in scripting and data manipulation.Experienced in building and maintaining data pipeline architectures.Strong familiarity with GitHub for version control and collaborative development.Excellent problem-solving, analytical, and communication skills.Ability to work effectively in a team environment.
Preferred Qualifications: Experience with cloud services like AWS, Azure, or Google Cloud Platform.",https://www.linkedin.com/jobs/view/4261001152,5
62,Centraprise,https://www.linkedin.com/company/centraprise/life,Tableau Developer (USC/GC/GC EAD),https://www.linkedin.com/jobs/view/4259560789,4259560789,"Plano, TX",Hybrid,,2025-07-02 18:40:03,True,27 applicants,"Job Role: Tableau Developer Location : Newark, DE / Plano, TX (Onsite Hybrid)Job Type: Full time/ Permanent
Job Description:5+ years of Experience in Tableau development (creating and publishing medium to complex dashboards).3+ years of experience in Hive, Impala.Understand Business/user requirements to create and maintain tableau dashboards based on the requirements.Optimize SQL queries for improving performances as part of Tableau dashboard Development.Provide dynamic service in identifying and solving issues ranging from extracts, user access and Dashboard development in Tableau Manage Tableau servers including access control, publishing and maintaining dashboards.Manage Tableau licenses for developers, end users and other stake holders in the Bank.Work with Tableau partners and CoE to upgrade tableau servers as per regulatory standards.Coordinates and facilitates routines to support delivery of technology solutions – e.g. kick-offs, status reviews, stakeholders meetings, change controls, and tollgates.Plans and coordinates delivery and dependencies across multiple technology teams.Facilitates dependency management, risk managements, and impediment removal for the defined deliverables.Promotes and facilitates communication and collaboration across organizations to support the deliverable completion and timeline. Articulate clear updates and critical path.Gathers and facilitates project updates for the deliverables to stakeholders and leadership pertaining to deliver, risks/issues and schedule.Ensures that execution is aligned with deliverable requirements by working with the sponsor and stakeholders.Identify any emerging risks/issues, escalated as needed, and identify critical path to resolve.Execute appropriate due diligence and financial management routines to deliver against financial commitments.Ensures deliverables comply with Enterprise Change Management standards and maintain evidence and systems for record for change.Supports resource planning for delivery/execution.Strong MS Excel skills to ensure cost reconciled with other systems of records.
Thanks & RegardsShaikh Mujahed Desk: 469-923-6788Mail: mujahed@centraprise.com",https://www.linkedin.com/jobs/view/4259560789,5
61,"LanceSoft, Inc.",https://www.linkedin.com/company/lancesoft/life,Data Engineer,https://www.linkedin.com/jobs/view/4259580572,4259580572,"Austin, TX",On-site,$50/hr - $65/hr,2025-07-02 19:30:16,True,93 applicants,"Location:- Austin,TXHourly Pay Rate Range - $50.00 - $65.00 /HR (Depending on working experience)
Note:- This only W2 role , No C2C
THE ROLE:This position will be within a central organization and tasked with designing, establishing, and maintaining the architecture necessary for Client, Graphics, and Semi-Custom teams to efficiently manage and leverage their data resources.
THE PERSON:We are seeking an experienced Data Engineer with a solid blend of technical expertise, analytical capabilities, and business acumen. The ideal candidate will possess the strategic thinking skills necessary to develop and implement robust data architectures that meet complex business requirements. They will be responsible for designing, building, and maintaining data pipelines and infrastructure, optimizing data storage solutions, and ensuring the seamless integration of data from various sources.
KEY RESPONSIBILITIES:• Perform duties of database administrator for snowflake, Postgres and MySQL databases.• Collaborate with data analysts, and other stakeholders to understand data requirements and provide the necessary infrastructure.• Design, build, and maintain robust data pipelines to collect, process, and transfer data from various sources to data storage solutions.• Create data models that facilitate efficient data storage, retrieval, and analysis in data warehouse.• Manage and optimize data warehouse, ensuring performance, security, and scalability.• Develop and implement ETL and ELT processes to integrate data from multiple sources, ensuring data quality and consistency.• Manage cloud-based data solution (Azure) to enhance data storage and processing capabilities• Create and maintain comprehensive documentation for data architecture, processes, and standards to ensure transparency and maintainability.
PREFERRED EXPERIENCE:• Experience working within semiconductor industry or related high-tech sectors• At least 5+ years of experience in SQL, including writing complex queries, optimizing performance, and managing large datasets.• At least 5+ years of experience in Python or similar languages, focusing on developing data processing scripts, automating data workflows, and implementing data pipelines.• Experience in administering and optimizing a Data Warehouse, including data loading, query performance tuning, and managing user access and security protocols (Snowflake is preferred).• Experience with big data technologies and cloud-based analytics platforms (Azure is preferred).• Experience in using data flow tools to design and manage data flows, ensuring efficient data ingestion, transformation, and routing between various data sources and systems. (Apache Nifi is preferred).• Experience collaborating with international teams, particularly in China and India.• Strong project management skills and experience leading cross-functional teams• Strong communication skills and the ability to collaborate effectively with semi-conductor engineers, data scientists, and business stakeholders
ACADEMIC CREDENTIALS:• Bachelor’s or Master's degree in Computer Science or Engineering preferred
Employee Benefits:At LanceSoft, full time regular employees who work a minimum of 30 hours a week or more are entitled to the following benefits:
Four options of medicalInsuranceDental and VisionInsurance401k ContributionsCritical IllnessInsuranceVoluntary Permanent LifeInsuranceAccident InsuranceOther Employee Perks
About LanceSoftLanceSoft is rated as one of the largest staffingfirms in the US by SIA. Our mission is to establish global cross-culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of our global network to connect businesses with the right people, and people with the right businesses without bias. We provide Global Workforce Solutions with a human touch.
EEO EmployerLanceSoft is a certifiedMinority Business Enterprise (MBE) and an equal opportunity employer. Weprohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state ,or local laws.This policy applies to all employment practices within our organization, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. LanceSoft makes hiring decisions based solely on qualifications, merit, and business needs at the time.Want to read more about LanceSoft?
Click here to visit our website - www.lancesoft.com",https://www.linkedin.com/jobs/view/4259580572,5
59,Kforce Inc,https://www.linkedin.com/company/kforce/life,Data Engineer,https://www.linkedin.com/jobs/view/4261013372,4261013372,"Smithfield, RI",Hybrid,$61/hr - $71/hr,2025-07-02 19:20:00,True,43 applicants,"Responsibilities

Kforce has a client in Smithfield, RI that is seeking a Data Engineer who will be working as part of an Agile Scrum team. Responsibilities:

 Building scalable and robust ETL data flows and databases using a range of technologies Recognize technology trends in the cloud space and assist in adopting fresh solutions as offered by cloud service providers Identifying and resolving issues within the production and non-production environments Collaborating with internal and external teams to deliver technology solutions for the business needs Documenting & sharing technical solutions and diagrams Designing and implementing solutions that align with the wider technology strategy Maintaining an atmosphere of collaboration, and approachability every day

Requirements

 8+ years of proven experience in database design and development using Oracle/PostgreSQL, NoSQL databases (DynamoDB, Aerospike) 5+ years of proven experience at programming in languages like SQL, R, and/or Python 4+ years of strong experience in ETL/ELT tools: Informatica or Snaplogic 3+ years of proven experience using AWS services S3, EMR, EC2, Lambda, Athena, CFT 3+ years of strong experience in Snowflake 3+ years of strong experience in Python, Java application development using Spring Boot 3+ years of experience in messaging technologies (Kafka, Kinesis, SNS, SQS) Good working knowledge in using Data Visualization tools - Tableau/Qlik/Power BI Strong knowledge in Java, J2EE, Spring MVC, Spring Core, Python Strong understanding of CI/CD tools such as Jenkins, Artifactory, Deploying applications in DevOps environment Deep understanding of API design, including versioning, API documentation (Swagger) Understanding of developing highly scalable distributed systems using Open-source technologies Good Understanding of E2E ALM tools like JIRA, gitStash, FishEye, Crucible, Maven, Jenkins, uDeploy Comfortable with Code Quality/Coverage tools (Sonar) Solid understanding of public/private cloud capabilities including compute, storage and scaling will be desirable Ability to deal with ambiguity and work in fast paced environment Ability to think out of box and design end-to-end solutions Passion and intellectual curiosity to learn new technologies and business areas Deep experience supporting critical applications quickly Excellent interpersonal skills, both through written and verbal channels Strong collaboration skills to work with multiple teams in the organization

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.

By clicking “Apply Today” you agree to receive calls, AI-generated calls, text messages or emails from Kforce and its affiliates, and service providers. Note that if you choose to communicate with Kforce via text messaging the frequency may vary, and message and data rates may apply. Carriers are not liable for delayed or undelivered messages. You will always have the right to cease communicating via text by using key words such as STOP.",,5
58,Marathon TS,https://www.linkedin.com/company/marathon-ts/life,Data Engineer,https://www.linkedin.com/jobs/view/4260893728,4260893728,United States,Remote,$115K/yr - $135K/yr,2025-07-02 17:27:34,True,over 100 applicants,"Senior Data EngineerPosition Purpose:· Senior Data Engineers are critical to designing, implementing, and optimizing scalable data infrastructure to support the data needs of THD. This role involves managing complex data pipelines, ensuring data quality and accessibility, and enabling data-driven insights that drive business decisions. With expertise in Google Cloud Platform (GCP), SQL, and data engineering best practices, this role will guide and mentor junior engineers, collaborate cross-functionally, and make large impacts to advancing the company's data architecture and strategy. The engineering process is highly dynamic, and engineers are expected to collaborate through user stories and support products as they evolve. Activities include using specific HD process techniques, integration, design, testing, and development. The role will interface with Business Stakeholders, Technology Infrastructure teams, and Development teams to ensure that business requirements are properly met within a data solution. The role may also be involved in performance tuning, testing, and product monitoring.· Engineers should be able to operate independently with minimum guidance from others, although will typically work as part of a team with varying skill levels to create, support, and deploy production applications. This role will review submitted code and provide feedback to improve, based on best practices.· Major Tasks and Responsibilities· 60% Delivery and Execution – Collaborates with other team members to develop, optimize, and maintain scalable ETL/ELT workflows and data pipelines. Works with team members to troubleshoot and resolve data issues. Implements best practices for data quality, security, and efficiency. Documents, reviews, and ensures that all quality and change control standards are met. Works with Product Team to ensure user stories that are developer-ready, easy to understand, and testable. Configures commercial off the shelf solutions to align with evolving business needs. Creates meaningful dashboards, logging, alerting, and responses to ensure that issues are captured and addressed proactively.· 30% Support and Enablement - Mentor junior engineers, fostering skill development and adherence to best practices. Fields questions from other product teams or support teams. Monitors tools and participates in conversations to encourage collaboration across product teams. Provides application support for software running in production. Proactively monitors production Service Level Objectives for products. Proactively reviews the Performance and Capacity of all aspects of production: code, infrastructure, data, message processing, and quality.· 10% Learning - Participates in learning activities around modern software design, data engineering, and development core practices (communities of practice). Proactively views articles, tutorials, and videos to learn about new technologies and best practices being used within other technology organizations.
Qualifications:3-5 years of relevant work experienceBachelor's degree in computer science, data engineering, or related field (or equivalent experience).Strong proficiency in SQL, data processing, and a programming language (Python preferred).Deep expertise in effective data warehouse engineering practices and platforms such as BigQuery, Snowflake, Synapse, etc.Experience in version control systems (preferable Git)Experience with Google Cloud Platform and data engineering components such as Cloud Composer, Cloud Functions, Dataproc, etc.Experience with CI/CD (Jenkins)Experience with production systems design including High Availability, Disaster Recovery, Performance, Efficiency, and SecurityStrong problem-solving skills and attention to detail.
Knowledge, Skills, Abilities and Competencies:Global PerspectiveManages AmbiguityNimble LearningSelf-DevelopmentCollaboratesCultivates InnovationSituational AdaptabilityCommunicates Effectively",https://www.linkedin.com/jobs/view/4260893728,5
57,Vortalsoft Inc.,https://www.linkedin.com/company/vortalsoft-inc-/life,Data Reporting Analyst (QlickSense),https://www.linkedin.com/jobs/view/4259580170,4259580170,"Mountain View, CA",On-site,$45/hr - $47/hr,2025-07-02 19:22:22,True,12 applicants,"Job Title: data Reporting Analyst (qlickSense & SQL)Location: Mountain View, CA - Must be onsite 3 days/week Job Summary:We are seeking a skilled Reporting Analyst with strong experience in report consolidations, ClickSense, and SQL. The ideal candidate will be responsible for merging multiple reports into a single, functional report while maintaining the accuracy and integrity of the data and user interface (UI).Key Responsibilities:Perform report consolidations, combining multiple data reports into a single, streamlined report.Use ClickSense and SQL to build, modify, and maintain reporting solutions.Analyze existing reports to ensure functionality and data accuracy is preserved during the merge process.Maintain and troubleshoot the UI layer sheets post-merger to ensure optimal user experience.Identify and resolve errors that may arise during the report merging process.Required Skills & Qualifications:Proven experience in report consolidations.Strong proficiency in QlickSense and SQL (Must-have).Solid understanding of report analysis and UI design within business intelligence tools.Excellent problem-solving skills and attention to detail.Strong communication and documentation abilities.",https://www.linkedin.com/jobs/view/4259580170,5
56,Wiraa,https://www.linkedin.com/company/wiraa/life,Data Engineer,https://www.linkedin.com/jobs/view/4261015146,4261015146,United States,Remote,,2025-07-02 19:36:29,False,,"About The Company

Claritev is a forward-thinking organization dedicated to transforming healthcare through innovation, technology, and data-driven solutions. Our team comprises passionate professionals committed to excellence and service delivery.

We aim to bend the cost curve in healthcare, ensuring accessible and efficient care for all stakeholders. Our culture fosters boldness, innovation, accountability, diversity, and empowerment, creating an environment where every team member can thrive and contribute to meaningful change. As part of our transformational journey, we aspire to become a leading voice in healthcare technology, data, and innovation, continuously pushing the boundaries to improve healthcare outcomes.

About The Role

We are seeking a skilled Data Engineer to join our dynamic team at Claritev. In this role, you will be instrumental in designing, developing, and maintaining scalable data pipelines and infrastructure that support our healthcare data initiatives. You will collaborate closely with business users, data science teams, and technology partners to understand data needs, implement robust data structures, and ensure high data quality and security.

The ideal candidate will have a strong background in data modeling, SQL, and data processing systems, along with experience working with large datasets and advanced analytics tools. This role is pivotal in enabling data-driven decision-making, predictive modeling, and automation across our organization, all while adhering to strict compliance standards such as HIPAA.

Qualifications

Minimum high school diploma with four (4) years of relevant experience, including three (3) years working with OOP, SQL, schema designing, data modeling, and data processing systemsBachelor’s degree in Computer Science, Information Technology, or a related field (highly preferred)Experience with advanced analytics tools such as Python and PySparkProficiency in SQL, SPARK, and Azure Data Factory (ADF)Experience triaging data issues and analyzing end-to-end data pipelinesKnowledge of data governance, data quality, and data security standards, including collaboration with data stewards and security officersExperience with Databricks, SSIS, Hive, Impala, Kafka, and Big Data development is advantageousExposure to machine learning, data science, artificial intelligence, and related mathematical disciplinesExcellent communication skills, both verbal and writtenStrong problem-solving skills with the ability to work effectively in a cross-functional teamAbility to prioritize multiple projects, meet deadlines, and work under pressureAttention to detail in identifying data relationships, trends, and anomaliesUnderstanding of long-term impacts of design decisions and handling failure scenarios

Responsibilities

Understand and model business processes within various systems to support data initiativesCollaborate with business users, technical teams, and executives to gather and analyze data requirementsDesign, implement, and maintain data structures, workflows, and integrations across enterprise platformsDevelop and optimize scalable data pipelines to handle increasing data volume and complexityCreate and manage data warehouses, databases, tables, SQL queries, and ingestion pipelines for reporting, dashboards, and analyticsWrite complex, efficient queries for data transformation and modeling to support reporting and predictive analyticsPrepare data for machine learning, predictive, and prescriptive modeling effortsIdentify, analyze, and resolve data patterns, inconsistencies, and anomalies to improve data reliability and qualityWork closely with analytics, data science, and engineering teams to automate data analysis and visualization processesEnsure compliance with HIPAA and other relevant data security and privacy standardsMaintain documentation of data processes, pipelines, and infrastructure configurationsContinuously evaluate and improve data infrastructure to support organizational growth and innovationCommunicate technical concepts effectively to non-technical stakeholders

Benefits

Medical, dental, and vision insurance with low deductibles and copaysLife insurance and short/long-term disability coveragePaid parental leave and generous paid time off, accrued based on years of service401(k) plan with company match and Employee Stock Purchase PlanTuition reimbursement and professional development programsFlexible Spending Account and Employee Assistance Program10 paid company holidays annuallySick time benefits, with accruals based on hours workedOpportunities for career growth and advancement within a supportive work environment

Equal Opportunity

Claritev is an Equal Opportunity Employer. We are committed to creating an inclusive environment for all employees and applicants. We do not discriminate based on age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected characteristic.",https://www.wiraa.com/job-description/us9B30D8E0D8D579894138FE7E281969FA?source=Linkedin,5
55,Jobot Consulting,https://www.linkedin.com/company/jobot-consulting/life,AI & Data Engineer - Python,https://www.linkedin.com/jobs/view/4257185163,4257185163,"Cincinnati, OH",Remote,$75/hr - $100/hr,2025-07-02 17:25:16,True,over 100 applicants,"Want to learn more about this role and Jobot Consulting? Click our Jobot Consulting logo and follow our LinkedIn page!

Job details

About Us

We leverage the latest in AI and data to provide highly specific and accurate results to our clients across the country.

Job Details

Are you a good fit?

 Work with cutting-edge AI and data technologies to deliver precise results. Collaborate in a serverless-first, containerized environment. Utilize Python, SQL, Spark, AWS, Amazon Athena, CDK, Terraform, and Docker.Proficiency in Python and SQLExperience with AWS and DockerFamiliarity with infrastructure-as-code tools like Terraform

Want to learn more about this role and Jobot Consulting?

Click our Jobot Consulting logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257185163,5
52,Applied Systems,https://www.linkedin.com/company/applied-systems/life,Sr. Data Engineer,https://www.linkedin.com/jobs/view/4259563342,4259563342,United States,Remote,,2025-07-02 19:01:19,False,,"Job Description

 Amazing Career Moments Happen Here 

Transforming the insurance industry is ambitious, we know. That’s why at Applied, we’re building a team that shows up every day ready to learn, willing to try new things, and driven to deliver innovative software and services that make us indispensable to our customers – all within a culture built on values that make us indispensable to each other too. With 40+ years of experience in the insurtech game, we’re not just redefining what’s achievable, we’re creating a place where amazing career moments are made possible. 

Position Overview

We're seeking a Senior Data Engineer to build and enhance data solutions and AI initiatives for the business of insurance. In this role, you will work closely with the Principal Data Architect, Data Scientists, and Software Engineers to build, model, and maintain data solutions as we optimize data architecture and accessibility of large-scale datasets for our global teams.

What You’ll Do

 Code BigQuery procedures, functions, and other database objects by applying expert knowledge in BigQuery SQL and ANSI SQL  Implement scalable and efficient data models within our data lake, with a focus on BigQuery  Manage and optimize data storage, partitioning, and clustering strategies to ensure high performance and reliability of our data infrastructure  Develop and implement features and enhancements in BigQuery, levering your expertise in SQL and cloud-based data warehousing technologies  Collaborate with cross-functional teams to understand requirements and deliver solutions aligned with business objectives, security requirements, and guidelines for data governance  Develop documentation for the team to support design discussions  Ensure data integrity and quality by implementing robust data validation and error-handling mechanisms. Maintain and advocate for these standards through code review  Identify and implement improvements across the full lifecycle of data management, from ingestion to ETL processes and final reporting layers, to increase productivity on the team  Continuously build knowledge of industry trends and advancements in data engineering and big data technologies 

We’re Excited To Learn More About You

We’re looking for someone who:

 Can work remotely or from an Applied office 

Your experience should include some or all of the following:

 8+ years focused on modeling, building, and maintaining data solutions  Proven experience with BigQuery and cloud-based data warehousing solutions  Advanced knowledge of data modeling, data warehousing, and big data architecture  Experience managing and optimizing BigQuery and Google Cloud Platform data services  Proficiency in SQL and Python to manipulate, store, manage, or retrieve data assets  Knowledge of Agile frameworks, ideally Scrum, and tools like Jira and Confluence  Ability to communicate with global team members to clarify requirements, confirm priorities, and deliver solutions within committed timelines  Advanced analytical and problem-solving skills, with a detail-oriented mindset  Bachelor-level degree in Computer Science, MIS, or CIS, or equivalent experience  We proudly support and encourage people with military experience, as well as military spouses, to apply 

 When You Join Team Applied, You Can Expect: 

 A culture that values who you are  and recognizes that you aren’t just an employee; you are a teammate, and you matter. We thrive on the benefits of our different experiences and celebrate the uniqueness our teammates bring to work with them every day. 

 We flex our time together  , collaborating remotely and in-person to empower our teams to work in the ways that work best for them. 

 A comprehensive benefits and compensation package  that centers our teammates and helps them to bring their best to work every day: 

 Medical, Dental, and Vision Coverage   Holiday and Vacation Time   Health & Wellness Days   A Bonus Day for Your Birthday  

Learn more about the people behind our products at https://www1.appliedsystems.com/en-us/about-us/jobs/

 Your Security Matters: 

Our candidates’ personal information and online safety are top of mind for us. At Applied, we proactively protect your personal information and only communicate with candidates via a secure @appliedsystems.com email or through our official careers portal. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers. 

 EEO Statement 

Applied Systems is proud to be an Equal Employment Opportunity Employer. Diversity and Inclusion is a business imperative and is a part of building our brand and reputation. At Applied, we don’t discriminate, and we are committed to recruit, develop, retain , and promote regardless of race, religion, color, national origin, sexual orientation, gender identity, disability, age, veteran status, and other protected status as required by applicable law.

",https://careers-appliedsystems.icims.com/jobs/6711/sr.-data-engineer/job?hub=15&in_iframe=1&mode=job&iis=LinkedIn,5
50,ROI Solutions,https://www.linkedin.com/company/roi-solutions/life,Data Platform Engineer (Snowflake),https://www.linkedin.com/jobs/view/4261012739,4261012739,United States,Remote,,2025-07-02 19:27:54,False,,"About the Position:
As a Data Platform Engineer at ROI Solutions, you will play a critical role in leading the migration from legacy systems to Snowflake and ensuring we design a future-ready data platform in the process. This role is focused on internal collaboration rather than direct client interaction, and your work will directly support the strategic objectives of ROI Solutions. You will work closely with internal stakeholders to ensure that the architecture you design not only meets today's requirements but also anticipates and supports future business needs, in alignment with ROI's mission to empower nonprofits. 
Skills and Experience Preferred:
Experience in cloud-based data platforms, with a focus on Snowflake (2+ years).Experience working with AWS Services (2+ years), particularly in the context of Snowflake integration, performance optimization, and security.Experience (2+ years) with data integration tools such as Fivetran and DBT, particularly in migration and cloud-oriented architecture scenarios.Broad experience (1+ years) working with Relational Databases such as Oracle and Postgres.Proficiency in data modeling and modern design approaches.Excellent problem-solving skills.Strong communication skills, with the ability to articulate complex technical concepts and strategic visions to a diverse audience.Excellent time management skills with a proven ability to meet deadlines. Proficient with Microsoft Office 365 Suite or related software preferred. 
Other Nice to Have Qualifications:
Strong knowledge of data governance, security, and compliance, especially in nonprofit data management.Knowledge of Java, particularly in data processing, integration, and supporting complex data workflows.Excellent interpersonal skills.Excellent organizational skills and attention to detail. Bachelor’s degree in Computer Science, Information Technology, or a related field. 
What You Will Be Doing In This Role: 
Contribute to the migration from legacy data tools (Oracle, Redshift, Windows services, and Glue) to reimagined toolset (Snowflake, DBT, Fivetran, etc). Ensuring minimal disruption and maximum data integrity throughout the process.Support data systems via ongoing monitoring, maintenance and day to day tuning.Collaborate closely with internal teams to understand business needs, translating them into technical solutions that support both current and future objectives.Maintain robust data security measures, including encryption, access controls, and compliance with relevant regulations, particularly within nonprofit data needs.Travel for in-person work meetings and company gatherings is required (very infrequently).
If you don't meet every requirement mentioned above, don't worry. We strongly believe in creating a diverse and inclusive work environment. So, if you find this job opportunity interesting but don't exactly fit every qualification mentioned in the job description, we encourage you to apply anyway. You might be the perfect candidate for this or other similar roles.
PLEASE NOTE:
This role is ONLY available for work in the following (21) locations: AL, AR, CO, FL, IL, KY, MA, MD, ME, MN, NC, NH, NJ, PA, SC, VA, VT, WA, WI, WV, and DC.This role will be working on Eastern Standard Time.This role is posted as remote but could be hybrid or in-office if that fits your best working style.
Who We Are: 
ROI Solutions was founded in 1999 to help nonprofit organizations change the world through innovative technology solutions and services. We are focused on sustainable growth, hiring staff committed to working with the nonprofit sector, and constantly evolving our technology and services to help nonprofits succeed in their missions with passion and purpose. This is our calling. 
We’re a proudly independent company, meaning we answer only to our clients rather than shareholders and investors. Our clients inspire our product and solution roadmaps, as well as our strategic direction. We’re committed to this model and the opportunity it provides us to develop our solutions based on needs in the nonprofit sector.  We truly believe our clients are partners in our desire to improve the world. As a result, we work only with nonprofit organizations and socially responsible companies whose values and missions align with ours. They are the most influential and progressive organizations in animal rights & welfare, social justice & civil rights, equality, environmental protection, gun control, health & human services, and public media. We’re proud to empower their efforts and their impact. 
We constantly evaluate our products and services to ensure they evolve to meet our clients needs. Our innovative solutions and services are based on best practices that help lower costs, increase constituent involvement, improve operational efficiency, provide insights, and allow organizations to adopt new methods and approaches to their work. As a result, we embrace technological advances that significantly impact our ability to serve the nonprofit sector.",https://roisolutions.bamboohr.com/careers/84?source=aWQ9MTk%3D,5
98,Insight Global,https://www.linkedin.com/company/insight-global/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4259575012,4259575012,"New York, United States",Hybrid,$200K/yr - $220K/yr,2025-07-02 19:04:30,True,37 applicants,"The RoleResponsibilities:Articulate our data capabilities clearly to clients, guide them towards our services, and lead conversations that demonstrate the value of our data solutions.Provider governance for the successful delivery of enterprise data projects from ideation through deployment.Act as Synechron’s data ambassador, delivering impactful presentations, engaging in industry dialogues, and enhancing our market presence.Spearhead ideas for new accelerators, tools, frameworks, and research to ensure our data practice remains at the forefront of industry developments.Mentor and grow a high-performing data team, fostering a collaborative, innovative, and results-oriented culture – through proactive training across the organizationLeverage Synechron Data Partnerships to grow commercial engagement and to enhance our offerings and enable our Data Practice employees with certificationsPromote and drive internal adoption of data, and externally for our clients, establishing and sharing best practices, guidelines, and metrics demonstrating the effectiveness and adoption of data solutions.

Requirements:University Degree (BSc, Master’s or Ph.D.) in Computer Science, or a closely related discipline is required.Strong background in data architecture, with familiarity in data warehousing, data lakes, and data mesh solutions.Proven ability to lead strategic discussions on data management—including governance, master data management (MDM), controls, and metadata management—and articulate the value of metadata.Deep understanding of analytics, focusing on how organizations can unlock actionable insights and extract maximum value from their data assets.Experience engaging with clients on data strategy, data management practices, and analytics-driven value propositions.Travel is expected on short notice to Synechron’s locations across North America meet clients and teamsThis success of this role will be through winning new business, and hands-on execution of strategic programsStrategic thinker capable of aligning data initiatives with business objectives.Exceptional communication skills, confidence in public speaking and industry representation.Strong leadership and mentorship capabilities.Strong knowledge of financial services data use casesProactive and entrepreneurial mindset, with the foresight to build solutions to win new businessCollaborative mindset and proven ability to lead cross-functional teams.Continuous learner, keeping abreast of industry trends and technological advancements.Extensive experience (minimum 20+ years) in the data domain within the financial services industry, with a strong understanding of banking, asset management, corporate banking, capital markets, and insurance.Innovative problem solver committed to delivering high-impact solutions.Consulting experience highly desirable, with proven track record in client delivery, client management, and navigating commercial conversationsThe base salary for this position will vary based on geography and other factors. In accordance with law, the base salary for this role if filled within New York, NY/Charlotte, NC is $200k - $220k/year & benefits (see below).",https://www.linkedin.com/jobs/view/4259575012,5
47,COGENT DATA SOLUTIONS LLC,https://www.linkedin.com/company/cogent-data-solutions-llc/life,Informatica Cloud Services (IICS),https://www.linkedin.com/jobs/view/4257608610,4257608610,"Texas, United States",Hybrid,,2025-07-02 19:24:57,True,5 applicants,"Hello,
Cogent Data Solutions LLC trying to reach you regarding a job opportunity. The role is  Informatica Cloud Services (IICS) ( Systems Analyst ) for one of our direct clients. Please take a look at the job description below and please send your updated resume.
Role:  Informatica Cloud Services (IICS) ( Systems Analyst )Location:  Hybrid - (Remote-Tuesdays and Fridays) -Mondays, Wednesdays-Thursdays Onsite - Austin, TX ( Local Only )
Client Name:  Texas Health and Human Services Commission ( State client )Job Qualifications/ Skills:Minimum 8 years experience inInformatica Cloud Services (IICS) informatica PowerCenter/IICS development experience.HIPAA regulations and handling protected health information (PHI) and personally identifiable information (PII)
Extensive experience in designing, implementing, and managing Informatica IICS/snowflake solutions.E2E implementation experience involving phases from design, realization, testing, to go-liveDeep understanding of Informatica data modeling, workflow, and integration capabilities and technologies, APIs, web services, and Informatica ETL tools.Strong knowledge of data governance, data quality, and data integration principles.Expertise in Azure Cloud and Snowflake Cloud Data warehouseStrong knowledge in SQL Queries and Query optimizationAbility to interact at a technical and non-technical level with Infrastructure, Network, Development, BA and QADevelopment experience in high transaction/high availability systems.Experience with analyzing and recommending solutions for Production issues short-term and long term.Must have informatica PowerCenter/IICS development experience.Knowledge and experience of managing reference data systemsProvide technical mentorship to junior ETL developers; establish coding standards, templates, and best practices.Experience in Python with data science librariesExperience in SQL programming and writing stored procedures, functions, views, SQL queriesExperience working with HIPAA regulations and handling protected health information (PHI) and personally identifiable information (PII).Strong organizational and time management skills.",https://www.linkedin.com/jobs/view/4257608610,5
46,"York Solutions, LLC",https://www.linkedin.com/company/york-solutions-llc/life,Sr. Data Engineer,https://www.linkedin.com/jobs/view/4259566341,4259566341,"Excelsior, MN",Hybrid,$45/hr - $70/hr,2025-07-02 18:46:02,True,over 100 applicants,"***At this time, we are unable to consider candidates requiring visa sponsorship or third-party recruitment agencies for this role. We thank you for your understanding.***
Logistics:Type: 6-month contract-to-hireLocation: Excelsior, MN (Hybrid – 2–3 days onsite)Pay Rate: $45-70/hr W2
We’re hiring a Data Engineer to help lead the buildout of modern data architecture, modeling, and governance for a rapidly growing team focused on sustainability and resource efficiency. This role is ideal for someone who’s both strategic and hands-on—comfortable working across cloud infrastructure, pipelines, and analytics tools, while setting data standards that support new product innovation.You’ll work with technologies like DBT, Airflow, Postgres, Azure, and Python to build scalable, secure systems. This person will play a key role in shaping how the organization structures and leverages its data to drive long-term growth and product development.
Key Responsibilities:Design and manage data infrastructure, modeling, integration, transformation, and governanceCollaborate with product teams to align data architecture with strategic goalsSupport data quality, security, and compliance (HIPAA, GDPR, etc.)Guide migration of legacy systems to modern, scalable platformsPartner with technical teams to evaluate new tools and implement best practicesMentor other engineers and contribute to team development
What You’ll Need to Have:7+ years of experience in data engineering, software development, or technical business analysisProficiency in DBT, Airflow, and Postgres (or equivalent tools)Experience with data modeling, normalization, and transformation of complex schemasWorking knowledge of Azure data ecosystems, ADF, Databricks, and cloud infrastructureStrong Python skills and experience working with APIsFamiliarity with implementing ML pipelines, especially data tagging and labelingSolid understanding of data governance, security best practices, and complianceExcellent problem-solving, communication, and cross-functional collaboration skillsLeadership or mentoring experience a plus",https://www.linkedin.com/jobs/view/4259566341,5
45,LTIMindtree,https://www.linkedin.com/company/ltimindtree/life,Database Migration Engineer Azure ,https://www.linkedin.com/jobs/view/4259559608,4259559608,"Frisco, TX",Remote,$70K/yr - $80K/yr,2025-07-02 17:29:51,False,,"About Us:LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree a Larsen & Toubro Group company combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale.
Skills:Perform Premigration cutover and post migration activitiesFollow the Runbook steps and execute the migration tasksEnsure all scheduled migrations are completed without any business impactAddress the migration issues if any with the support from the technical team and provide input for root cause analysisImplement and operate database systems for performance and reliability in Azure.On-Prem/Cloud upgrade of SQL Server from Legacy version to latest versions in AzureGood knowledge in T-SQL scripting & PowerShell scriptGood understanding of Database Performance using various toolsConforming to client compliances and expectationsIdentify Automation opportunitiesPrepare Migration Execution reportPost migration validation test resultsBreak fix Support
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree ( LTIM ):
Benefits and Perks:Comprehensive Medical Plan Covering Medical, Dental, VisionShort Term and Long-Term Disability Coverage401(k) Plan with Company matchLife InsuranceVacation Time, Sick Leave, Paid HolidaysPaid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office:In order to comply with LTIMindtree s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree s applicable processes.",https://r.ripplehire.com/s/c2RBi,5
43,Hallmark Global Solutions Ltd,https://www.linkedin.com/company/hallmark-global-solutions/life,NEO4J Graph Database Engineer,https://www.linkedin.com/jobs/view/4259556072,4259556072,United States,Remote,,2025-07-02 16:23:35,True,35 applicants,"Job Description Neo4j Graph Database EngineerLead II - Software Engineering You Are:We are seeking a skilled Neo4j Graph Database Engineer to join our engineering team. In this role, you will be responsible for designing, implementing, optimizing, and maintaining Neo4j graph database solutions that support complex data relationships and high-performance querying. You will work closely with AI engineers, data scientists, backend engineers, and product teams to enable powerful data insights and scalable systems using graph technologies. The opportunity:· Design and architect graph database models using Neo4j platform· Develop efficient queries using Cypher· Collaborate with engineers and data scientists to integrate graph databases into production systems and analytics pipelines.· Optimize graph data storage, traversal, and indexing for performance and scalability.· Develop and enforce data integrity, security, and access policies within graph systems.· Monitor, troubleshoot, and tune graph database performance.· Maintain documentation of data models, configurations, and integration patterns.· Stay current with emerging trends in graph technologies, data modeling techniques, and query optimization. This position description identifies the responsibilities and tasks typically associated with the performance of the position. Other relevant essential functions may be required. What you need:· Mandatory Skill· Experience working with graph databases and data modeling.· Strong proficiency in at least one graph query language (e.g., Cypher, Gremlin).· Solid understanding of graph theory and its practical applications.· Experience with one or more graph database platforms (Neo4j, Amazon Neptune, TigerGraph, etc.).· Proficiency in a general-purpose programming language such as Python, or JavaScript.· Experience with data integration and ETL pipelines.· Experience with Azure cloud services and deployment· Familiarity with RDF, OWL, or knowledge graph principles.· Exposure to projects using graph data.· Familiarity with RESTful APIs and microservices architectures.· 3+ years of experience working with graph databases and data modeling.· Strong proficiency in at least one graph query language (e.g., Cypher, Gremlin).· Solid understanding of graph theory and its practical applications.· Experience with one or more graph database platforms (Neo4j, Amazon Neptune, TigerGraph, etc.).· Proficiency in a general-purpose programming language such as Python, or JavaScript.· Experience with data integration and ETL pipelines.· Preferred:· Experience with Azure cloud services and deployment· Familiarity with RDF, OWL, or knowledge graph principles.· Exposure to projects using graph data.· Familiarity with RESTful APIs and microservices architectures.",https://www.linkedin.com/jobs/view/4259556072,5
39,SGS Consulting,https://www.linkedin.com/company/sgs-consulting/life,Machine Learning Engineer,https://www.linkedin.com/jobs/view/4259571991,4259571991,"California, United States",Remote,$70/hr - $80/hr,2025-07-02 19:20:10,True,38 applicants,"Summary:We are embarking on the most transformative change to its business and technology in company history, and our Machine Learning Engineers are at the forefront of this evolution. By leading crucial projects and initiatives that have never been done before, you have an opportunity to help us advance the way people connect around the world. The ideal candidate will have industry experience working on a range of recommendations, classification, and optimization problems. You will bring the ability to own the whole ML life cycle, define projects and drive excellence across teams. You will work alongside the world’s leading engineers and researchers to solve some of the most exciting and challenging problems in the industry.
Responsibilities:Adapt standard machine learning methods leveraging modern parallel environments (e.g. distributed clusters, multicore SMP, and GPU).Develop highly scalable classifiers and tools leveraging machine learning, data regression, and rules-based models.Suggest, collect and synthesize requirements from XFN teams.Code deliverables in tandem with the engineering team.
Minimum Qualifications:6+ years of experience in software engineering or a relevant field. 3+ years of experience if you have a PhD.2+ years of experience in one or more of the following areas: machine learning, recommendation systems, pattern recognition, data mining, artificial intelligence, or a related technical field.Experience with scripting languages such as Python, JavaScript or HackExperience with developing machine learning models at scale from inception to business impact.Experience with scripting languages such as Python, PHP, and/or shell scripts.Experience building and shipping high quality work and achieving high reliability.Track record of successful cross-functional partnerships.Experience improving quality through thoughtful code reviews, appropriate testing, proper rollout, monitoring, and proactive changes.Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
Preferred Qualifications:Master’s degree or PhD in Computer Science or another ML-related fieldExposure to architectural patterns of large-scale software applicationsExperience with scripting languages such as PyTorch and TFC++ Preferred",https://www.linkedin.com/jobs/view/4259571991,5
63,Air Apps,https://www.linkedin.com/company/airapps/life,Data Engineer,https://www.linkedin.com/jobs/view/4257187325,4257187325,"San Francisco, CA",On-site,,2025-07-02 15:37:57,False,,"About Air Apps

At Air Apps, we believe in thinking bigger—and moving faster. We’re a family-founded company on a mission to create the world’s first AI-powered Personal & Entrepreneurial Resource Planner (PRP), and we need your passion and ambition to help us change how people plan, work, and live. Born in Lisbon, Portugal in 2018—and now with offices in both Lisbon and San Francisco—we’ve remained self-funded while reaching over 100 million downloads worldwide.

Our long-term focus drives us to challenge the status quo every day, pushing the boundaries of AI-driven solutions that truly make a difference. Here, you’ll be a creative force, shaping products that empower people across the globe.

Join us on this journey to redefine resource management—and change lives along the way.

The Role

As a Data Engineer at Air Apps, you will be responsible for designing, building, and optimizing data pipelines, data warehouses, and data lakes to ensure efficient data processing and analytics. You will work closely with data analysts, scientists, and software engineers to create scalable and reliable data infrastructure that supports business intelligence and machine learning initiatives.

This role requires expertise in data architecture, ETL processes, and cloud-based data solutions to handle large volumes of structured and unstructured data.

Responsibilities

Design, build, and maintain scalable data pipelines and ETL workflows to support analytics and reporting.Develop and optimize data warehouses and data lakes using cloud platforms such as AWS, Google Cloud, or Azure.Implement real-time and batch data processing solutions for various business needs.Work with structured and unstructured data, ensuring proper data modeling and storage strategies.Ensure data reliability, consistency, and scalability through best practices in architecture and engineering.Collaborate with data analysts, scientists, and software engineers to enable efficient data access and analysis.Automate data ingestion, transformation, and validation processes to improve data quality.Monitor and optimize query performance and data processing efficiency.Implement security, compliance, and governance standards for data storage and access control.Stay up to date with emerging data engineering trends, tools, and technologies.

Requirements

Around 4+ years of experience in data engineering, software engineering, or database management.Proficiency in SQL, Python, or Scala for data processing and automation.Hands-on experience with cloud-based data solutions (AWS Redshift, Google BigQuery, Azure Synapse, Snowflake).Experience building ETL pipelines with tools such as Apache Airflow, dbt, Talend, or Fivetran.Strong understanding of data modeling, schema design, and database optimization.Experience with big data frameworks (Apache Spark, Hadoop, Kafka, Flink) is a plus.Familiarity with orchestration tools, containerization (Docker, Kubernetes), and CI/CD workflows.Knowledge of data security, governance, and compliance (GDPR, CCPA, SOC 2).Strong problem-solving and debugging skills with the ability to handle large-scale data challenges.Experience working in fast-paced, data-driven environments with cross-functional teams.

What benefits are we offering?

Apple hardware ecosystem for work.Annual Bonus.Medical Insurance (including vision & dental).Disability insurance - short and long-term.401k up to 4% contribution.Air Stipend of $3,120/year, paid over 12 monthly installments (for home office, learning, wellness, etc.).Air Conference 2025 in Las Vegas – an opportunity to meet the team, collaborate, and grow together.

Diversity & Inclusion

At Air Apps, we are committed to fostering a diverse, inclusive, and equitable workplace. We enthusiastically welcome applicants from all backgrounds, experiences, and perspectives. We celebrate diversity in all its forms and believe that varied voices and experiences make us stronger.

Application Disclaimer

At Air Apps, we value transparency and integrity in our hiring process. Applicants must submit their own work without any AI-generated assistance. Any use of AI in application materials, assessments, or interviews will result in disqualification.",https://jobs.ashbyhq.com/airapps/4927a40b-d1aa-41e9-a14e-49483969d80c?src=LinkedIn,5
38,Jobot,https://www.linkedin.com/company/jobot/life,Data Engineer,https://www.linkedin.com/jobs/view/4257179871,4257179871,"Johnson City, TN",Remote,$80K/yr - $100K/yr,2025-07-02 17:25:23,True,53 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Fully remote Data Engineer opportunity // Data Modeling experience required!

This Jobot Job is hosted by Craig Rosecrans

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $80,000 - $100,000 per year

A Bit About Us

We are currently seeking a seasoned Data Engineer to join our dynamic Tech Services team. The ideal candidate will be a data enthusiast with a solid understanding of the latest data technologies and trends coupled with a strong desire to deliver top-notch solutions to our clients. The successful candidate will be responsible for designing, developing, and maintaining data architectures, databases, and processing systems. With a focus on ETL, Data Modeling, SSIS, Tableau, and PowerBI, this role will be integral in transforming data into readable, goal-driven reports for continued innovation and growth.



Why join us?


 Competitive Base Salary Company paid health plan for employees Flexible Hours Very generous PTO Dental and Vision, FSA, HSA Small team, autonomy Many more great perks!

Job Details

Responsibilities

 Design, develop, automate, and maintain productivity tools using programming, database or scripting languages to improve software modeling and development. Design and implement ETL procedures for intake of data from both internal and outside sources; as well as ensure data is verified and quality is checked. Collaborate with data architects, modelers and IT team members on project goals. Develop and maintain databases by acquiring data from primary and secondary sources, and build scripts that will make our data evaluation process more flexible or scalable across data sets. Employ a variety of languages and tools to marry systems together or try to hunt down opportunities to acquire new data from other systems. Develop and implement data standards, ensuring data quality and consistency. Perform data profiling to identify and understand anomalies. Present information using data visualization techniques through Tableau and PowerBI. Work closely with team members, clients, project managers, and other stakeholders to ensure solutions are delivered timely and accurately.

Qualifications

 Bachelor's degree in Computer Science, Information Systems, or a related field. A minimum of 5+ years of experience in a data engineering role with a proven record of successful data manipulation. Proficiency with ETL tools, SSIS, and experience with SQL/NoSQL databases. Knowledge and experience in data modeling. Experience with business intelligence tools like Tableau, PowerBI or similar. Strong problem-solving skills, attention to detail, and ability to think critically. Excellent written and verbal communication skills, with the ability to present complex data in a clear and concise manner. Ability to work independently and with team members from different backgrounds. Excellent organizational skills and the ability to manage multiple tasks concurrently. A strong desire to learn and develop new skills, staying up to date with the latest data best practices and technologies. Ability to work in a fast-paced, deadline-driven work environment.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257179871,5
66,Acetech Group Corporation,https://www.linkedin.com/company/acetech-group-corporation/life,GCP Data Engineer/ Lead,https://www.linkedin.com/jobs/view/4260885975,4260885975,United States,Remote,,2025-07-02 16:24:03,True,over 100 applicants,"Position: GCP Data Engineer/ ArchitectLocation: Remote in DMV AreaDuration: 12+ MonthsYears of experience: 12+ Years
Top SkillsGCPBigQueryAirflowDataflowJavaPython",https://www.linkedin.com/jobs/view/4260885975,5
68,SquarePeg,https://www.linkedin.com/company/squarepeg-ai/life,Data Engineer (remote),https://www.linkedin.com/jobs/view/4261001966,4261001966,United States,Remote,,2025-07-02 18:58:13,False,,"About SquarePeg

SquarePeg uses AI to screen and score tens of thousands of job applicants—fast, fairly, and at scale. Our platform ingests messy resume and job data from multiple systems and applies advanced ranking models to help recruiters get to inbox zero with confidence. Clean, deduplicated, and well-resolved data is core to everything we do.

We’re hiring a Data Engineer with deep entity resolution experience to help us improve how we match people to jobs—especially when the inputs are ambiguous, inconsistent, or incomplete.

What you'll do:

Build and scale data pipelines that ingest, clean, and resolve person, company, and job entities across disparate datasets (ATS exports, resumes, job descriptions, data sets, APIs)Own our entity resolution layer: design logic for deduplication, disambiguation, and canonicalization of candidates and companiesImprove our internal identity graphs for people, companies, and job titles by integrating open and proprietary data sourcesImplement and refine blocking strategies, fuzzy matching, and ML-based similarity scoring to improve match precision and recallWork closely with product and Eng to test resolution accuracy and continuously tune performance for production workloadsMonitor data integrity and build systems to surface issues before they affect scoring or UX

What we're looking for: 

4+ years of experience in data engineering or applied data science, ideally working with large-scale B2B or recruiting datasetsHands-on experience with entity resolution, including rule-based and ML-based approaches (e.g., record linkage, string similarity, embeddings, supervised matching models)Proficiency in Python and SQL; experience with Spark, DuckDB, or similar frameworks is a plusStrong understanding of data quality, normalization, and the challenges of real-world messy input dataThoughtful engineering mindset: you write testable, maintainable code and think about edge cases before they bite

Nice to Have

Experience with recruiting/talent data (e.g., resumes, job postings, ATS data)Familiarity with open-source tools like Splink, Dedupe, Scikit-learn, or Faiss for similarity matchingExperience working with skills taxonomies or job-title ontologiesPrior experience in a high-velocity startup environment

Why SquarePeg?

We’re solving one of the most painful and high-volume problems in hiring: figuring out which applicants are actually worth readingYou’ll work on a small, senior team with a bias for shipping, pragmatism, and deep techYour work will directly improve the quality of our applicant scoring, customer trust, and platform performanceCompetitive compensation, early equity, and a remote-first culture that respects your time

",https://www.squarepeghires.com/jobs/87k5kj/data-engineer-entity-resolution?utm_source=linkedin&utm_medium=jobposting&utm_campaign=general,5
97,Yochana,https://www.linkedin.com/company/yochana/life,"Contract role: Data Engineer with Strong ETL Matillion exp at Secaucus, NJ (Remote)",https://www.linkedin.com/jobs/view/4259558781,4259558781,"Secaucus, NJ",Remote,,2025-07-02 17:50:29,True,over 100 applicants,"Data EngineerSecaucus, NJ (Remote)Long Term Contract 
Must Have – Matillion ETL, Snowflake, Python
Job Details:Data Pipeline Development: Design, construct, test, and maintain highly scalable data management systems. Develop and implement architectures that support the extraction, transformation, and loading (ETL) of data from various sources.Data Integration: Integrate structured and unstructured data from multiple data sources into a unified data system, ensuring data quality and consistency.Data Warehousing: Build and maintain data warehouses and data lakes to store and retrieve vast amounts of data efficiently. Optimize the performance of databases and queries to meet business needs.Data Processing: Implement data processing frameworks (e.g., Hadoop, Spark) to process large datasets in real-time or batch processing.Automation and Monitoring: Automate manual processes, optimize data delivery, and develop data monitoring systems to ensure data integrity and accuracy.Collaboration: Work closely with data scientists, analysts, and other stakeholders to understand data needs and provide technical solutions that meet business requirements.Data Governance: Ensure data governance policies are followed, including data security, data privacy, and compliance with regulations.Performance Tuning: Optimize the performance of ETL processes, databases, and data pipelines to handle large volumes of data and reduce processing times.",https://www.linkedin.com/jobs/view/4259558781,5
96,Resolve Tech Solutions,https://www.linkedin.com/company/resolve-tech-solutions/life,Senior Data Architect,https://www.linkedin.com/jobs/view/4259580243,4259580243,Dallas-Fort Worth Metroplex,Hybrid,,2025-07-02 19:23:29,True,5 applicants,"About Resolve Tech Solutions: Resolve Tech Solutions is a fast-growing technology consulting firm dedicated to helping organizations transform and scale through tailored digital solutions. We pride ourselves on our deep technical expertise, client-first approach, and commitment to innovation.Position Overview: We are seeking a skilled Data Architect with a strong background in designing and implementing data platforms. The ideal candidate will come from a consulting background and have experience architecting data solutions across multiple clients, or alternatively, have led data engineering teams in building and maintaining robust, scalable enterprise data platforms.Key Responsibilities:Design and architect modern, scalable data platforms tailored to client needs (cloud-based and on-premise).Engage directly with clients to assess data infrastructure requirements and deliver strategic solutions.Lead or collaborate with data engineering teams to implement end-to-end data architectures, including data ingestion, storage, processing, and governance.Define data standards, architecture patterns, and best practices.Evaluate and select appropriate tools and technologies (e.g., Azure, AWS, GCP, Snowflake, Databricks, etc.).Support pre-sales efforts by contributing to solution designs, estimates, and proposals when needed.Preferred Qualifications:7+ years of experience in data architecture or data engineering roles.Strong consulting experience with a demonstrated ability to work across multiple client environments and industries.Proven track record of designing and implementing enterprise-grade data platforms.Experience managing or mentoring teams of data engineers.Expertise in modern data stack components and cloud data architectures (e.g., lakehouses, data lakes, data warehouses).Excellent communication skills, with the ability to translate technical requirements into business solutions.Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.Bonus Points:Certifications in Azure, AWS, GCP, or other relevant technologies.Experience with data governance, privacy, and compliance frameworks.Why Join Us?Work with a variety of exciting clients and tech stacks.Collaborate with a team of innovative and passionate professionals.Opportunity to lead strategic initiatives and make a real impact.",https://www.linkedin.com/jobs/view/4259580243,5
95,Cornspring,https://www.linkedin.com/company/cornspring-ai/life,Data Engineer,https://www.linkedin.com/jobs/view/4260889991,4260889991,"Manhattan, NY",Hybrid,,2025-07-02 17:30:39,True,over 100 applicants,"*Unfortunately we are unable to sponsor visas at this point. Please only apply if you have the right to work in the US.*
About us:
Cornspring is an innovative start-up FinTech company, with a mission to empower Family Offices and Asset Owners with real-time, AI driven data intelligence and portfolio insights. 
The tools we are building are solving one of the most complex and valuable challenges in Finance. Our clients operate at the highest levels of global finance, managing billions in assets. Yet, legacy systems fail them: slow, fragmented, and outdated.
Cornspring is redefining Family Office services by leveraging state-of-the-art generative AI and Large Language Models to provide unparalleled insights and efficiency to Family Office investment and accounting data. 
This is a high-ownership environment, and we need exceptional engineers who bring a mix of talent and passion to join us on our incredibly exciting journey. 
We are currently looking for a top-tier Data Engineer with deep expertise in Python programming. This is a pivotal role, where you will be tackling technically challenging problems and working with cutting edge AI technology. 
You will be reporting to the Head of Data Operations and will be working alongside a team of talented engineers. 
Key responsibilities:
You will be responsible for developing and maintaining our ETL pipelines, handling diverse data formats, and ensuring data quality and consistency. You'll work on automating data processing workflows while implementing robust validation and error handling mechanisms to maintain data integrity. You will conduct testing on data movement, transformation code, and data components to ensure integrity, accuracy, and high-quality deliverables for all data processing.You will design and implement data solutions for new initiatives and provide continuous oversight for existing solutions to improve quality and performance.You will assess and integrate new data sources into pipelines to meet evolving business needs.
Key requirements:
The role requires a solid experience in Python data manipulation libraries.University degree in Computer Science, Software Engineering or equivalent. Data engineering experience, and experience or exposure to financial data. Knowledge and exposure to AWS. 
What we offer: 
Competitive salary. Hybrid working: we work from a beautiful office in Manhattan 2 days a week.Being part of a revolutionary movement in Family Office technology.Working with cutting-edge AI and data technologies. Challenging projects that make a real impact to our sector.",https://www.linkedin.com/jobs/view/4260889991,5
92,Crosscheck Staffing,https://www.linkedin.com/company/crosscheck-staffing/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4260883073,4260883073,"Denver, CO",Hybrid,$130K/yr - $175K/yr,2025-07-02 15:34:05,True,over 100 applicants,"Crosscheck Staffing is currently working with a large management consulting company with a Data Architect need.
Your ResponsibilitiesLead the implementation and performance testing of Microsoft Fabric and Databricks, with a focus on scalable data pipelines and query optimizationDesign, build, and optimize data workflows using Databricks (PySpark, Delta Lake, Notebooks) and Microsoft Fabric to ingest, transform, and deliver data from enterprise systems to a central data platformEvaluate and tune SQL and Spark-based transformations to improve performance, scalability, and cost-efficiencyDevelop and maintain data architectures that support lakehouse patterns and modern analytics workloadsApply best practices in ETL/ELT development, data engineering, and pipeline orchestration
What You Bring5+ years of experience with cloud-based data platforms, including Databricks, Microsoft Fabric, and Airflow.Hands-on experience with Databricks in productionStrong skills in ETL/ELT development using Databricks, Fabric, or similar platformsDeep understanding of modern data architectures, including data lakes, lakehouses, and cloud-native warehousesProficiency in SQL and PySpark, with experience in performance tuning and large-scale data processingStrong experience in data modeling, including dimensional models, star/snowflake schemas, and Kimball methodologiesProven ability to lead discussions and influence data architecture and modeling decisions across departmentsFamiliarity with data governance, security, and privacy standards in cloud environments",https://www.linkedin.com/jobs/view/4260883073,5
89,Gensyn,https://www.linkedin.com/company/gensynai/life,Machine Learning Engineer,https://www.linkedin.com/jobs/view/4259578675,4259578675,United States,Remote,,2025-07-02 19:45:30,False,,"Machine intelligence will soon take over humanity’s role in knowledge-keeping and creation. What started in the mid-1990s as the gradual off-loading of knowledge and decision making to search engines will be rapidly replaced by vast neural networks - with all knowledge compressed into their artificial neurons. Unlike organic life, machine intelligence, built within silicon, needs protocols to coordinate and grow. And, like nature, these protocols should be open, permissionless, and neutral. Starting with compute hardware, the Gensyn protocol networks together the core resources required for machine intelligence to flourish alongside human intelligence.

The Role

Design and implement highly decentralised training pipelines. Scope can span proof of concept development for novel ML research—e.g., our RL-Swarm reinforcement learning framework or Verde verification system—to the maintenance of highly fault-tolerant production systems

Responsibilities

Build scalable, distributed ML compute systems over uniquely decentralised and heterogeneous infrastructureDesign and develop novel machine learning algorithms and deep learning applications, and systems for Gensyn; likely from scratch or by augmenting existing systemsPartner with both researchers and production engineers to design and run novel experiments, taking research from theory to production

Competencies

Must have

Strong background in applied machine learning/engineeringComfortable working in an experimental environment, with extremely high autonomy and unpredictable timelinesProven background in training, retraining, inference or ML systemsImpeccable analytical and problem-solving skillsFamiliarity with data structures and software architecture

Preferred

Experience building highly performant, distributed systemsDemonstrated background developing or implementing novel ML researchExperience developing mission-critical, highly complex production systems, particularly with respect to improving their fault tolerance/crash-recovery

Nice to have

Experience working in a startup/scaleup environment

Compensation / Benefits

Competitive salary + share of equity and token poolFully remote work - we currently hire between the West Coast (PT) and Central Europe (CET) time zonesVisa sponsorship - available for those who would like to relocate to the US after being hired3-4x all expenses paid company retreats around the world, per yearWhatever equipment you needPaid sick leave and flexible vacationCompany-sponsored health, vision, and dental insurance - including spouse/dependents [🇺🇸 only]

Our Principles

Autonomy & Independence

Don’t ask for permission - we have a constraint culture, not a permission culture.Claim ownership of any work stream and set its goals/deadlines, rather than waiting to be assigned work or relying on job specs.Push & pull context on your work rather than waiting for information from others and assuming people know what you’re doing.Communicate to be understood rather than pushing out information and expecting others to work to understand it.Stay a small team - misalignment and politics scale super-linearly with team size. Small protocol teams rival much larger traditional teams.

Rejection of mediocrity & high performance

Give direct feedback to everyone immediately - rather than avoiding unpopularity, expecting things to improve naturally, or trading short-term pain for extreme long-term pain.Embrace an extreme learning rate - rather than assuming limits to your ability / knowledge.Don’t quit - push to the final outcome, despite any barriers.Be anti-fragile - balance short-term risk for long-term outcomes.Reject waste - guard the company’s time, rather than wasting it in meetings without clear purpose/focus, or bikeshedding.Build and design thinly.",https://job-boards.eu.greenhouse.io/gensyn/jobs/4573390101?gh_src=2b6c1912teu,5
88,Precision Technologies,https://www.linkedin.com/company/precision-technologies-corp-/life,Data Engineer,https://www.linkedin.com/jobs/view/4259550581,4259550581,"New Jersey, United States",On-site,$55/hr,2025-07-02 16:01:42,True,over 100 applicants,"Title: Data Engineer
Experience:- 7+ Years
Key Responsibilities:
Design, develop, and maintain robust, scalable, and efficient ETL/ELT pipelines to ingest, transform, and load data from diverse sources into data warehouses and data lakes.Build and optimize data models (dimensional/star schema) to support business intelligence, analytics, and reporting requirements.Work with large-scale datasets using technologies like Apache Spark, Hadoop, Hive, and Presto.Develop real-time data processing pipelines using Apache Kafka, AWS Kinesis, or Apache Flink to support streaming data applications.Ensure high levels of data integrity, quality, and consistency across multiple data sources and systems.Perform data validation, cleansing, and normalization to ensure data readiness for analytics and machine learning use cases.Optimize performance of databases and queries using SQL, SparkSQL, BigQuery, or Redshift.Collaborate with Data Scientists, Analysts, Product Managers, and Software Engineers to deliver end-to-end data solutions and analytics platforms.Implement data governance, security, and compliance policies in alignment with enterprise standards.Automate manual processes, optimize data delivery, and re-design infrastructure for greater scalability and efficiency.Monitor pipeline performance, perform root-cause analysis, and resolve production issues in a timely manner.",https://www.linkedin.com/jobs/view/4259550581,5
86,Net2Source Inc.,https://www.linkedin.com/company/n2s-global/life,Data Architect,https://www.linkedin.com/jobs/view/4261006348,4261006348,"Reading, PA",Hybrid,$70/yr - $75/yr,2025-07-02 18:51:17,True,5 applicants,"Net2Source is a Global Workforce Solutions Company headquartered at NJ, USA with its branch offices in Asia Pacific Region. We are one of the fastest growing IT Consulting company across the USA and we are hiring ""Data Architect"" for one of our clients. We offer a wide gamut of consulting solutions customized to our 450+ clients ranging from Fortune 500/1000 to Start-ups across various verticals like Technology, Financial Services, Healthcare, Life Sciences, Oil & Gas, Energy, Retail, Telecom, Utilities, Technology, Manufacturing, the Internet, and Engineering.
Position : Data ArchitectLocation: Reading PA - 3 days a week HybridType: ContractDuration: 12+ Months
 Responsibilities:Architect - Data• 14+ years of experience in IT• 12+ years of Data Engineering, Data Modeling, Data Warehousing, Master Data Management, Reference Data Management, Data Lineage, Data Governance and Meta Data Management experience required• 10+ years of experience in defining Data & Analytics architecture implementing multiple large volume projects across on Prem and on cloud environments. Preferably AWS cloud.• 7+ years of experience collaborating with Agile teams preferred• Expertise in designing, validating, and implementing multiple projects across the hybrid infrastructure (On-cloud to On-Premises and vice versa)• Expertise in setting up Data Lakes and analytical environments• Expertise with Data Engineering/Big data tools such as Python, PySpark, Kafka, API’s, Talend and BODS, etc.• Expertise with relational SQL and NoSQL databases• Experience in building data pipelines, data ingestions, data integrations, data preparations, and traditional Data warehouses and Datamarts• Experience in building processes supporting data transformation, data structures, metadata, dependency, and workload management• Strong experience with data modelling techniques utilizing tools such as Erwin, ER Studio etc.• Experience with visualization tools such as QlikView, Qlik Sense, Tableau, etc.• Experience in message queuing, stream processing, and highly scalable ‘big data’ data stores• Meet with Senior & Mid-Level Management to align business process initiatives and strategies with current and planned Penske Data Engineering & Analytics Architecture• Evaluate Data Engineering & Analytics Tools/Techniques/ Approaches and determine impact on strategic objectives and manage overall implementation• Be the primary contact and technical lead for multiple critical programs and integrations and resolve customer issues in a timely man
Why Work With Us?We believe in more than just jobs—we build careers. At Net2Source, we champion leadership at all levels, celebrate diverse perspectives, and empower you to make an impact. Think work-life balance, professional growth, and a collaborative culture where your ideas matter.Our Commitment to Inclusion & EquityNet2Source is an equal opportunity employer, dedicated to fostering a workplace where diverse talents and perspectives are valued. We make all employment decisions based on merit, ensuring a culture of respect, fairness, and opportunity for all, regardless of age, gender, ethnicity, disability, or other protected characteristics.Awards & RecognitionAmerica’s Most Honored Businesses (Top 10%)Fastest-Growing Staffing Firm by Staffing Industry AnalystsINC 5000 List for Eight Consecutive YearsTop 100 by Dallas Business JournalSpirit of Alliance Award by Agile1

Madhukar SinghEmail:madhukar@net2source.comMobile: 201-479-3246",https://www.linkedin.com/jobs/view/4261006348,5
85,Confidential Jobs,https://www.linkedin.com/company/confidential1234/life,Data Engineer,https://www.linkedin.com/jobs/view/4259567010,4259567010,"New Jersey, United States",On-site,,2025-07-02 18:19:15,True,over 100 applicants,"Job Title: Senior Data Engineer (Python)Location: Jersey City, NJ (1 Pershing Plaza, Jersey City, NJ) – Need to report 4 to 5 days/week onsiteFulltime role 
Skillset: Advanced Python programming skills with APIs. Expertise in App design and solutioning. Exp in designing and Implementing Kafka producers and consumers.Experience with Mongo collections, caching technologies like Hazel cast and performance tuning.Deep Development experience in backend data engineering and custom data warehouse applications.Experience in building ETL Pipelines using Py-Spark or Data Factory. DW experience in MS SQL Server/Oracle/Sybase/Snowflake etc.Knowledge on CI/CD, AKS, Cloud Services, Azure/DBT experience is a plus
""Centraprise is an equal opportunity employer. Applicants must be authorized to work in the U.S. U.S. citizens and Green Card holders are strongly encouraged to apply.""",https://www.linkedin.com/jobs/view/4259567010,5
84,Aditi Consulting,https://www.linkedin.com/company/aditiconsulting/life,Algorithm Developer IV,https://www.linkedin.com/jobs/view/4261008956,4261008956,"Menlo Park, CA",Remote,$60/hr - $70/hr,2025-07-02 19:25:53,True,6 applicants,"Payrate: $60.00 - $70.00/hr.
 
Summary:
The ideal candidate will have industry experience working on a range of recommendation, classification, and optimization problems. You will bring the ability to own the whole ML life cycle, define projects and drive excellence across teams. You will work alongside the world’s leading engineers and researchers to solve some of the most exciting and challenging problems in the industry.
 
Responsibilities:Adapt standard machine learning methods leveraging modern parallel environments (e.g. distributed clusters, multicore SMP, and GPU)Develop highly scalable classifiers and tools leveraging machine learning, data regression, and rules based modelsSuggest, collect and synthesize requirements from XFN teamsCode deliverables in tandem with the engineering team 
Qualifications:6+ years of experience in software engineering or a relevant field. 3+ years of experience if you have a PhD2+ years of experience in one or more of the following areas: machine learning, recommendation systems, pattern recognition, data mining, artificial intelligence, or a related technical fieldExperience with scripting languages such as Python, Javascript or HackExperience with developing machine learning models at scale from inception to business impactKnowledge developing and debugging in C/C++ and Java, or experience with scripting languages such as Python, Perl, PHP, and/or shell scriptsExperience building and shipping high quality work and achieving high reliabilityTrack record of successful cross-functional partnershipsExperience improving quality through thoughtful code reviews, appropriate testing, proper rollout, monitoring, and proactive changesBachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience 
Preferred Qualifications:Masters degree or PhD in Computer Science or another ML-related fieldExposure to architectural patterns of large scale software applicationsExperience with scripting languages such as Pytorch and TF 
Pay Transparency: The typical base pay for this role across the U.S. is: $60.00 - $70.00 /hr. Final offer amounts, within the base pay set forth above, are determined by factors including your relevant skills, education and experience and the benefits package you select. Full-time employees are eligible to select from different benefits packages. Packages may include medical, dental, and vision benefits, 10 paid days off, 401(k) plan participation, commuter benefits and life and disability insurance.
 
For information about our collection, use, and disclosure of applicant's personal information as well as applicants' rights over their personal information, please see our Privacy Policy (https://www.aditiconsulting.com/privacy-policy).
 
Aditi Consulting LLC uses AI technology to engage candidates during the sourcing process. AI technology is used to gather data only and does not replace human-based decision making in employment decisions. By applying for this position, you agree to Aditi’s use of AI technology, including calls from an AI Voice Recruiter.
 
#AditiConsulting
#25-20546
 ",https://www.linkedin.com/jobs/view/4261008956,5
83,TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,"Data Engineer 3- Scala - Chicago, IL or Reston, VA- ONSITE (4 days a week)",https://www.linkedin.com/jobs/view/4261002091,4261002091,Greater Chicago Area,Hybrid,$100.1K/yr - $150.2K/yr,2025-07-02 17:59:42,False,,"FreeWheel, a Comcast company, provides comprehensive ad platforms for publishers, advertisers, and media buyers. Powered by premium video content, robust data, and advanced technology, we're making it easier for buyers and sellers to transact across all screens, data types, and sales channels. As a global company, we have offices in nine countries and can insert advertisements around the world.Job SummaryWe are seeking a Data Engineer to design, build, and optimize scalable, high-performance data pipelines. The ideal candidate will have expertise in big data processing frameworks, data storage technologies, and monitoring platforms while thriving in a collaborative, fast-moving environment. If you are a problem solver, a mentor, and someone who enjoys pushing the boundaries of data engineering, this is the opportunity for you!Job DescriptionCore Responsibilities:Own the complete software development lifecycle-from design to deployment-of scalable batch and streaming data pipelines to ingest and transform large datasets from various sources.Optimize streaming and batch data processing to ensure performance, scalability, and cost efficiency.Monitor system performance, troubleshoot issues, and implement solutions to ensure high availability and reliability.Maintain and enhance FreeWheel's internal monitoring platform, ensuring comprehensive observability and operational excellence for the entire Streaming Hub and Audience Platform.Collaborate with product managers and stakeholders to understand data requirements and deliver solutions that drive business impact.Effectively communicate technical concepts to non-technical stakeholders.Stay ahead of emerging data engineering technologies and best practices, identifying opportunities to improve processes, tools, and infrastructure.Document data pipelines, processes, and systems to ensure clarity and maintainability.Mentor and guide junior data engineers, fostering a culture of learning, collaboration, and technical excellence.Consistent exercise of independent judgment and discretion in matters of significance.Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) as necessary.Other duties and responsibilities as assigned.
Qualifications:Bachelor's or master's degree in computer science, Engineering, or a related field.2+ years of experience in building and operating large-scale data processing systems.Proficiency in multiple programming languages, including Scala, Java, Python, and SQL.Familiarity with big data processing frameworks, such as Apache Flink, Apache Spark (Batch & Streaming), and Databricks Delta Live Tables (DLT).Understanding of data storage technologies, including AWS S3, Delta Lake, Apache Iceberg, Hadoop HDFS, and Apache Druid.Basic understanding of Data Lakehouse architectures, data modeling techniques, ETL processes, and relational databases (e.g., MySQL).Experience with AWS services, such as Glue, EMR and Lambda.Experience with workflow orchestration tools, such as Apache Airflow and Azkaban.Knowledge of NoSQL databases, including HBase.Strong problem-solving skills with a keen attention to detail.Excellent communication and collaboration skills to work effectively with technical and non-technical teams.Ability to quickly adapt and context switch among different priorities, working efficiently in a fast-paced environment.Strong generalist mindset with the ability to work across different technology stacks and polyglot programming skills, demonstrating fluency in multiple programming languages beyond primary expertise.Nice to Have:Experience managing highly scalable data infrastructures, ensuring performance, reliability, and cost-efficiency.Experience in modeling and processing complex, hierarchical data structures such as Ad Logs.Adept in planning work for self and the team while collaborating closely with product stakeholders.


Employees at all levels are expected to:Understand our Operating Principles; make them the guidelines for how you do your job.Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.Win as a team - make big things happen by working together and being open to new ideas.Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.Drive results and growth.Respect and promote inclusion & diversity.Do what's right for each other, our customers, investors and our communities.
Why Join FreeWheel?Cutting-Edge Technology: Work with modern data stacks, cloud platforms, and real-time processing frameworks.Innovation-Driven Culture: Collaborate with top engineers, data scientists, and industry leaders shaping the future of ad tech.Growth & Impact: Solve complex, large-scale data challenges with tens of billions of daily events in a high-impact role.Flexible & Inclusive Work Environment: We value diverse perspectives, work-life balance, and professional development.
About FreeWheelFreeWheel, a Comcast company, is a global leader in advanced advertising technology, enabling seamless transactions across screens, data types, and sales channels. Operating in nine countries, FreeWheel powers global ad insertion and recently launched Universal Ads, a cross-publisher TV advertising platform in collaboration with major media players like NBCUniversal, Warner Bros. Discovery, and Roku. Committed to innovation, inclusivity, and professional growth, FreeWheel fosters a dynamic work environment where diverse perspectives thrive while shaping the future of premium video advertising.
About the FreeWheel Data TeamThe FreeWheel Data Team is a global, high-impact engineering team focused on building scalable, high-performance, efficient data platforms and rich analytic user experiences. We develop solutions to ingest, transform, organize, and distribute vast volumes of advertising and audience data, enabling a Unified Data Lake and Data Warehouse that powers customer-facing products, machine learning applications, and real-time analytics. We process tens of billions of ad events daily, leveraging a modern data stack that includes Databricks, AWS, Apache Spark, ClickHouse, Snowflake, and Google Looker.
Disclaimer:This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.Skills:Data Engineering; Structured Query Language (SQL); Python (Programming Language)
Salary:Primary Location Pay Range: $100,118.14 - $150,177.21Comcast intends to offer the selected candidate base pay within this range, dependent on job-related, non-discriminatory factors such as experience. The application window is 30 days from the date job is posted, unless the number of applicants requires it to close sooner or later.

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits to eligible employees. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.
EducationBachelor's DegreeWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.Relevant Work Experience5-7 YearsPDN-9f4a6e3a-5cb6-46d0-b9c1-bfe079f508c1",https://www.adzuna.com/land/ad/5278448217?v=135BDCD0C598C276321E7DF4A1FF0417A01962DD&r=19707339&frd=c4f60cf300a8319d9d1c1d583235a505&ccd=393bf47e34f3d2e105505622394965b9&utm_source=talentally-dynamic&utm_medium=ppc&partnerb=1&chnlid=3864&title=Data%20Engineer%203-%20Scala%20-%20Chicago%2C%20IL%20or%20Reston%2C%20VA-%20ONSITE%20%284%20days%20a%20week%29&a=e&utm_content=1&utm_campaign=0.12,5
82,Harvey Nash,https://www.linkedin.com/company/harvey-nash/life,Data Movement Engineer (Talend),https://www.linkedin.com/jobs/view/4259580605,4259580605,"New Haven, CT",Remote,$55/hr - $60/hr,2025-07-02 19:31:14,True,35 applicants,"""US citizens and Green Card Holders and those authorized to work in the US are encouraged to apply. We are unable to sponsor H1b candidates at this time”

Title: Data Movement Engineer (Talend) 
Type: Long Term Contract
Location: New Haven, CT (100% Remote)

Overview:The client is embarking on the modernization of its core data platforms, currently seeking a Data Movement Engineer who can assist with modernizing client's current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as they move towards a cloud environment. The Data Movement Engineer will support data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities:Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling and testing.Translate data movement requirements into technical designs.Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.Develop data extraction and transmissions to external vendors.Develop test plans and perform unit testing.Create supporting documentation for new processes.Work closely with data analysts to gain understanding of business processes and corporate data.Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.Contribute to architectural decisions to support business processes.Provide production support for data solutions.Complete root cause analysis and contribute to remediation planning and implementation.Perform data quality analysis, report data quality issues and propose solutions for data quality management.Learn and expand upon internal controls and participate in customer support.Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.On Call and/or after hours work required.Other related duties as directed.

Skills QualificationsRequired:Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of dataMinimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementationsProficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)Some experience with cloud-based database technologies requiredWorking knowledge of data warehousing concepts, structures and ETL best practicesExperience using query tools (e.g. AQT, MS Query)Ability to problem solve using analytical thinking skillsMust work well independently - must be inquisitive and seek answers to complex questions without being promptedStrong organizational and time management skillsStrong communication skills including verbal and written to communicate effectively with clients and managementStrong project management skills to ensure that projects get done on time and within budgetEffectively participates in teams and moves the team toward completion of goals Preferred:Some experience with data visualization tools (e.g. Tableau) desirable
EducationRequired:BA or BS in Computer Science, Information Systems or related field5+ years' experience
A reasonable, good faith estimate of the $55/hr to $60/hr W2 for this position.",https://www.linkedin.com/jobs/view/4259580605,5
80,Photon,https://www.linkedin.com/company/photon-interactive/life,Python Developer,https://www.linkedin.com/jobs/view/4259581513,4259581513,"Dallas, TX",On-site,,2025-07-02 19:32:58,True,14 applicants,"Greetings from Photon!!
Who are we?Photon has emerged as one of the world’s largest and fastest-growing Digital Agencies. We work with 40% of the Fortune 100 on their Digital initiatives and are known for our ability to integrate Strategy Consulting, Creative Design, and Technology at scale. For a brief 1 minute video about us, you can check https://youtu.be/uJWBWQZEA6o.
Python DeveloperLocation : Dallas, TX (Onsite)Hiring Type : Full Time
We are looking for a Python Developer with a diverse skill set. The successful candidate will have a strong background in Python development, with experience in designing, implementing, and delivering high-quality software solutions. The primary focus will be on developing applications for acquiring, storing, and calculating on large volumes of data using mathematical formulas.Responsibilities:Design, develop, and implement complex Python data computation applications.Lead the design and implementation of key features and components of our product suite.Collaborate with cross-functional teams to define, design, and deliver new features.Ensure code quality, adherence to coding standards, and maintainability through linting and automated testing.Mentor junior developers, conduct code reviews, and ensure software deliverables meet high-quality standards.Troubleshoot and resolve software defects and other technical issues.Qualifications:Proven 9+ Years of experience as a Senior Python Developer primarily backend applicationsStrong knowledge of Python and familiarity with various Python libraries and frameworks such as Pandas, NumPy, Flask, Django, Fast APIUnderstanding of code quality, DevSecOps, and automated testing.Excellent problem-solving skills and attention to detail.Strong communication skills and the ability to work as part of a team.Experience in leading a team or mentoring junior developers is a plus.Preferred Qualifications:Familiarity with Agile development methodologies.Experience with cloud platforms like AWS, Google Cloud, or Azure.Experience with Docker or other containerization technologies.Experience Analytical platforms like SnowflakeExperience with financial services data",https://www.linkedin.com/jobs/view/4259581513,5
76,Cygnus Professionals Inc.,https://www.linkedin.com/company/cygnus-professionals-inc-/life,Data Architect and Data Architect (Not Data Engineer),https://www.linkedin.com/jobs/view/4261014333,4261014333,"Reading, PA",Hybrid,,2025-07-02 19:10:33,True,28 applicants,"Must have: On Prem to AWS migration.Must have: On Prem to AWS migration • 14+ years of experience in IT• 12+ years of Data Engineering, Data Modeling, Data Warehousing, Master Data Management, Reference Data Management, Data Lineage, Data Governance and Meta Data Management experience required• 10+ years of experience in defining Data & Analytics architecture implementing multiple large volume projects across on Prem and on cloud environments. Preferably AWS cloud.• 7+ years of experience collaborating with Agile teams preferred• Expertise in designing, validating, and implementing multiple projects across the hybrid infrastructure (On-cloud to On-Premises and vice versa)• Expertise in setting up Data Lakes and analytical environments• Expertise with Data Engineering/Big data tools such as Python, PySpark, Kafka, API’s, Talend and BODS, etc.• Expertise with relational SQL and NoSQL databases• Experience in building data pipelines, data ingestions, data integrations, data preparations, and traditional Data warehouses and Datamarts• Experience in building processes supporting data transformation, data structures, metadata, dependency, and workload management• Strong experience with data modelling techniques utilizing tools such as Erwin, ER Studio etc.• Experience with visualization tools such as QlikView, Qlik Sense, Tableau, etc.• Experience in message queuing, stream processing, and highly scalable ‘big data’ data stores• Meet with Senior & Mid-Level Management to align business process initiatives and strategies with current and planned Penske Data Engineering & Analytics Architecture• Evaluate Data Engineering & Analytics Tools/Techniques/ Approaches and determine impact on strategic objectives and manage overall implementation• Be the primary contact and technical lead for multiple critical programs and integrations and resolve customer issues in a timely man

About Cygnus Professionals, Inc.Cygnus is a Princeton, NJ-headquartered global Business IT consulting and software Services firm with offices in the USA and Asia. Cygnus offers and enables innovation and helps our clients accelerate time to market & grow their business. Over 15 years, we have taken great pride in continuing our deep relationships with our clients. 
Cygnus Belief We believe in our commitment to diversity & inclusion. 
Equal Employment Opportunity Statement Cygnus is an Equal Opportunity Employer. We ensure that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion, or sexual orientation. 
All our employment decisions are taken without looking into age, race, creed, color, religion, sex, nationality, disability status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status, or any other aspects of employment protected by federal, state, or local law. Applicants for employment in the US must have valid work authorization.",https://www.linkedin.com/jobs/view/4261014333,5
75,Enigma,https://www.linkedin.com/company/enigma-rec/life,"Senior AI Engineer | NLP | Large Language Models | Machine Learning | Remote, US",https://www.linkedin.com/jobs/view/4259582519,4259582519,United States,Remote,,2025-07-02 19:41:52,True,3 applicants,"Senior AI Engineer | NLP | Large Language Models | Machine Learning | Remote, US
We are looking for multiple Senior AI Engineer, they are responsible for building AI and machine learning models and pipelines, with a particular emphasis on generative AI, large language models (LLMs), and predictive modeling. This role involves collaborating closely with business and product stakeholders to understand data requirements and product specifications, enabling data-driven decision-making and product design. Additionally, the position requires coordination with software engineers and developers to deliver effective solutions.
A key aspect of the role involves rapidly acquiring new tools and technologies, leveraging a strong foundation in machine learning and artificial intelligence alongside basic programming and scripting skills. The individual in this position is expected to have practical experience in the design, development, management, and maintenance of systems as well as in handling large datasets. Expertise with commonly used AI and machine learning frameworks and a solid grasp of fundamental techniques—including deep learning, regression, classification, and clustering algorithms—are essential. Familiarity with retrieval-augmented generation pipelines and designing LLM-supported use cases is also critical.
ResponsibilitiesLeading AI strategy by delivering integrated solutions that combine software engineering, statistics, and machine learning for complex clinical applications.Executing rigorous, analytical experiments to achieve incremental improvements using the most suitable techniques.Preparing comprehensive reports and presentations to convey hypotheses and insights that support organizational decision-making.Becoming the subject matter expert on available organizational data by identifying, collecting, transforming, and exploring datasets.Facilitating seamless collaboration between engineers, product analysts, and other teams within the organization.
Experience5 to 7 years of relevant experience.Leadership experience is advantageous.At least three years of experience in an AI Engineer role.Advanced degree (Ph.D. or MSc) in computer science, machine learning, AI, or related fields, or equivalent experience in designing, building, and evaluating machine learning systems.Proficiency with machine learning ecosystem tools such as PyTorch, TensorFlow, Scikit-learn, and XGBoost.Strong statistical analysis and machine learning knowledge, with practical programming skills (e.g., Python, R, SQL).Expertise in data visualization, model implementation, testing, and deployment.Experience evaluating and working with LLM-based pipelines, including retrieval-augmented generation and prompting techniques, is advantageous.Familiarity with tools like LangChain, LlamaIndex, Haystack, Azure AI Studio, vector databases, or equivalent LLM-enabled technologies.Proficiency in database access and management using SQL, Azure Data Factory, or similar technologies.Familiarity with big data frameworks (e.g., Hadoop, Spark) or equivalent systems.Understanding of MLOps principles, including orchestration tools, cloud computing, and observability platforms.
💰Up to $200,000 USD + Bonus📍Fully remote anywhere in the US
If you are interested in finding out more about this hire please reach out to jason@enigma-rec.ai for immediate consideration.
Senior AI Engineer | NLP | Large Language Models | Machine Learning | Remote, US",https://www.linkedin.com/jobs/view/4259582519,5
69,Jobot,https://www.linkedin.com/company/jobot/life,Machine Learning Engineer – Generative Visuals (Text-to-Image),https://www.linkedin.com/jobs/view/4257179910,4257179910,"Levittown, PA",Remote,$150K/yr - $300K/yr,2025-07-02 17:25:22,True,7 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Y Combinator–backed Bio-Tech company is looking for a Senior Data Engineer to join their growing team!

This Jobot Job is hosted by Sydney Weaver

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $150,000 - $300,000 per year

A Bit About Us

With backing from Sequoia Capital and Y Combinator, a growing startup is looking for Senior ML Engineers and Applied AI Software Engineers to play a pivotal role in developing AI models that generate scalable vector graphics. This is a high-impact opportunity at the frontier of generative AI and their platform helps researchers quickly create accurate scientific visuals.



Why join us?


 Excellent Benefits Remote Work (USA or Canada) Growth Opportunity  Pet Benefits Great Culture, PTO, 401K, and much more!

Job Details

Responsibilities

 Design and implement advanced ML architectures for vector graphics generation Own end-to-end development of generative AI systems, from prototyping to scalable deployment Leverage foundational model techniques to power scientific communication tools Collaborate with a world-class team of engineers with deep AI and product experience Deploy multimodal AI pipelines and ensure robust, production-level performance Drive technical decisions and long-term planning for the ML platform Evaluate and incorporate open-source tools where appropriate Participate in hackathons and in-person team events (remote-first culture)

Requirements

 Bachelor’s degree in Computer Science or related field and Master’s or PhD from a top-tier academic institution strongly preferred Text to image gen AI experience Demonstrated experience building and scaling generative models (e.g., foundational models, text-to-image, Adobe Firefly) Strong programming skills in Python and deep learning frameworks like PyTorch or TensorFlow Expertise with text-to-image, foundational model, or generative AI  History of deploying multimodal AI systems in production environments Industry experience in both startup and large tech environments preferred Ability to pragmatically choose between custom solutions and leveraging existing tools Strong ownership mindset and ability to lead model architecture without heavy oversight

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257179910,5
67,JPMorganChase,https://www.linkedin.com/company/jpmorganchase/life,Data Engineer III,https://www.linkedin.com/jobs/view/4260898833,4260898833,"Plano, TX",On-site,,2025-07-02 18:52:13,False,,"Job Description

Be part of a dynamic team where your distinctive skills will contribute to a winning culture and team. The Identity and Access Management (IAM) Data Platform is a comprehensive system that facilitates the collection, storage, processing, and analysis of IAM data within an organization.

As a Data Engineer III at JPMorgan Chase within the Corporate Identity & Acess Management Technology team, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm’s business objectives.

Job Responsibilities

Gather, analyze, synthesize, and create visualizations and reports from large, diverse data sets to continuously improve software applications and systems.Identify (proactively) hidden issues and patterns in data, using these insights to enhance coding practices and system architecture.Implement software solutions, design, development, and technical troubleshooting, thinking beyond conventional approaches to create solutions or resolve technical issues.Develop secure, high-quality production code and maintain algorithms that operate in sync with relevant systems.Produce architecture and design artifacts for complex Business Intelligences solutions, ensuring that design constraints are met during software code development.Contribute to software engineering communities of practice and events that explore new and emerging technologies.Adds to team culture of diversity, equity, inclusion, and respect

Required Qualifications, Capabilities, And Skills

Formal training or certification on Data Engineering concepts and 3+ years applied experienceExperience with SQL, data acquisition, and data analysis and working knowledge of relational and No-SQL databases (e.g., SQL Server, MySQL, Oracle, Redshift, DynamoDB)Experience with additional data visualization tools (e.g. Tableau, Qlik, Power BI, QuickSight)Proficiency in Macros and VBA (Visual Basic for Applications) scripting is essential.Understanding of how to manage and optimize queries for large volumes of data within different kinds of data storesCompetent data analysis in PythonUnderstanding of software development lifecycle (SDLC)Knowledge of data and software designing concepts, including Test Driven DevelopmentExperience with Quality Assurance processes and practicesExperience in end-to-end implementation of Business Intelligence (BI) reports & dashboardsProficiency in data processing, logical reasoning and problem-solving skills

Preferred Qualifications, Capabilities, And Skills

Understanding of AWS cloud technologiesIdentity & Access Management Domain knowledge a plusCertifications in AWS Cloud Certification is preferred

ABOUT US

JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set and location. Those in eligible roles may receive commission-based pay and/or discretionary incentive compensation, paid in the form of cash and/or forfeitable equity, awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.

JPMorgan Chase & Co. is an Equal Opportunity Employer, including Disability/Veterans

About The Team

Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we’re setting our businesses, clients, customers and employees up for success.",https://JPMorganChase.contacthr.com/148115833,5
37,Jobot,https://www.linkedin.com/company/jobot/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4257180678,4257180678,"Addison, TX",On-site,$125K/yr - $135K/yr,2025-07-02 17:25:21,True,26 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

A rapidly growing thousand person non-profit is currently hiring for a fully remote Senior Data Engineer in the Texas area!

This Jobot Job is hosted by Ryan Sullivan

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $125,000 - $135,000 per year

A Bit About Us

A rapidly growing thousand person non-profit is currently hiring for a fully remote Senior Data Engineer in the Texas area!



Why join us?


Excellent base salary of 150-170K403 B plan, similar to a 401KGenerous vacation time Phenomenal work life balance Robust benefits package Tuition reimbursement Opportunity for growth

Job Details

Job Details

Our company is seeking a highly skilled and experienced Permanent Senior Data Engineer to join our dynamic and innovative scientific team. This is an exciting opportunity for a seasoned professional to contribute to our cutting-edge projects, utilizing their expertise in Python, ETL, and Data Science. The successful candidate will be responsible for the development and maintenance of robust data pipelines, database architectures, and data processing systems that meet our high scientific standards. This role requires a minimum of 5 years of experience in the field.

Responsibilities

 Design, construct, install, test, and maintain highly scalable data management systems. Collaborate with data architects, modelers, and IT team members to identify opportunities for process improvements and system efficiencies. Utilize your expertise in Python, ETL, and Data Science to develop innovative data solutions. Ensure systems meet business requirements and industry practices for data quality, data security, and data privacy. Interact with data scientists and industry experts to understand data needs and deliver systems that aid in achieving company objectives. Develop set processes for data mining, data modeling, and data production. Troubleshoot data-related problems and authorize maintenance or modifications. Participate in the creation of data governance policies and manage metadata effectively. Continually strive for an understanding of emerging technologies, data issues, and the latest industry trends. Provide technical leadership and mentoring to junior team members.

Qualifications

 A bachelor's degree or higher in Computer Science, Data Science, Engineering, or a related field. A minimum of 5 years of experience in a Data Engineer role, with a proven track record of successful data platform setup and operation. Expert knowledge of Python, ETL, and Data Science. Experience with SQL and NoSQL databases, including Postgres, Cassandra, and MongoDB. Strong experience with data modeling, data extraction, and data mining. Demonstrated understanding of distributed computing principles. Proficient in the use of big data tools, such as Hadoop, Spark, or Kafka. Excellent problem-solving and analytical skills. Strong communication skills, with the ability to explain complex technical concepts to non-technical stakeholders. Ability to work both independently and as part of a team, on multiple projects simultaneously, under tight deadlines. Familiarity with the scientific industry is a plus.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257180678,5
49,Centraprise,https://www.linkedin.com/company/centraprise/life,Tableau Developer ,https://www.linkedin.com/jobs/view/4259569209,4259569209,"Plano, TX",On-site,,2025-07-02 18:54:52,True,22 applicants,"Job DescriptionSkill: Tableau Developer5+ years of Experience in Tableau development (creating and publishing medium to complex dashboards).3+ years of experience in Hive, Impala.Understand Business/user requirements to create and maintain tableau dashboards based on the requirements.Optimize SQL queries for improving performances as part of Tableau dashboard Development.Provide dynamic service in identifying and solving issues ranging from extracts, user access and Dashboard development in Tableau Manage Tableau servers including access control, publishing and maintaining dashboards.Manage Tableau licenses for developers, end users and other stake holders in the Bank.Work with Tableau partners and CoE to upgrade tableau servers as per regulatory standards.Coordinates and facilitates routines to support delivery of technology solutions – e.g. kick-offs, status reviews, stakeholders meetings, change controls, and tollgates.Plans and coordinates delivery and dependencies across multiple technology teams.Facilitates dependency management, risk managements, and impediment removal for the defined deliverables.Promotes and facilitates communication and collaboration across organizations to support the deliverable completion and timeline. Articulate clear updates and critical path.Gathers and facilitates project updates for the deliverables to stakeholders and leadership pertaining to deliver, risks/issues and schedule.Ensures that execution is aligned with deliverable requirements by working with the sponsor and stakeholders.Identify any emerging risks/issues, escalated as needed, and identify critical path to resolve.Execute appropriate due diligence and financial management routines to deliver against financial commitments.Ensures deliverables comply with Enterprise Change Management standards and maintain evidence and systems for record for change.Supports resource planning for delivery/execution.Strong MS Excel skills to ensure cost reconciled with other systems of records.",https://www.linkedin.com/jobs/view/4259569209,5
35,COVET IT INC,https://www.linkedin.com/company/covet-it-inc/life,Data Solution Engineer,https://www.linkedin.com/jobs/view/4259580366,4259580366,"Boston, MA",On-site,,2025-07-02 19:25:23,True,25 applicants,"Hi,
Please go through the below requirements and let me know your interest and forward your resume along with your contact information to raja@covetitinc.com
Role : Data Solution Engineer - Data EngineeringLocation : Boston, MA (Onsite)Duration : Long Term - Contract Experience: 15+JOB DESCRIPTION:Roles and responsibilitiesThe Data Solutions Engineer will build data solutions that are resilient and scalable for both the primary domain and cross-domain analytics. The Data Solutions Engineer implements data engineering best practices (e.g., Data Discovery, Naming Conventions, Operational Excellence, and Data Security) and implements standards at the DataMart level. The Data Solutions Engineer influences decisions made by other teams and builds consensus. The Data Solutions Engineers will be responsible for specific data domains and partner with the data steward and Data Platform Engineers, Data Analysts, and Technical Program Managers. The Data Solutions Engineer will lead projects across teams in, but is not limited to Advancement, HR, and Finance. Principle Duties and ResponsibilitiesOwns the design, development, and maintenance of scalable data models and leads projects to develop ongoing metrics, reports, analyses, dashboards, etc. to support analytical and business needs Interfaces with the Data Platform team to extract, transform, and load data from a wide variety of data sources using AWS services and internal tools Builds, optimizes and delivers high quality data sets to support administrative and academic data needs Leads continuous improvement projects for ongoing reporting and analysis processes, automating or simplifying self-service support for customers Translates business problem statements into data model requirements Uses analytical and statistical rigor to answer business questions and drive business decisions in areas including, but limited to Advancement, HR, and Finance Writes queries and output efficiently and has in-depth knowledge of the available in area of expertise. Pull the needed with standard query syntax; periodically identify more advanced methods of query optimization. Convert to make it analysis-ready Recognizes, adopts and documents best practices in the development and support of data solutionsTroubleshoots operational quality issues pertaining to data processing and orchestration code written in SQL/PythonReviews and audits existing jobs and queries Recommends improvements to back-end sources for increased accuracy and simplicity  QualificationsRequiredBachelor's degree At least 10 years of data engineering experience including experience with programming, data modeling, warehousing, and building data pipelines Experience with modern databases such as Snowflake and Redshift Experience with data orchestration tools such as airflow or DagsterExperience with AWS technologies such as S3, AWS Glue, Lambda, and IAM roles and permissions Experience in writing complex, highly-optimized SQL queries across large data sets Experience in a scripting language (e.g. Python, PySpark, Java, Scala) Experience developing, monitoring, and maintaining Extract Transform Load (ETL) and Extract Load Transform (ELT) data pipelines; experience ensuring data integrity Proven success in communicating with business users, technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Ability to communicate thoughts, ideas, and solutions logically both written and verbally Ability to build trusted relationships and collaborate across diverse teams and with leadership Ability to get up to speed quickly on complex issues; desire to work in a fast-paced, rapidly evolving environment Comfort with ambiguity in a complex matrixed environment PreferredProject management experience Experience providing technical leadership and educating other engineers for best practices on data engineering. Familiarity with BI tools and Data Science models",https://www.linkedin.com/jobs/view/4259580366,5
23,Wiraa,https://www.linkedin.com/company/wiraa/life,Machine Learning Engineer (ML),https://www.linkedin.com/jobs/view/4261012747,4261012747,United States,Remote,,2025-07-02 19:36:29,False,,"About The CompanyGreenbox Capital is a forward-thinking Fintech company committed to empowering small and mid-sized businesses globally. We specialize in providing fast, flexible, and stress-free funding solutions designed to help businesses grow and succeed in competitive markets.Our company values integrity, trust, and a people-first approach, fostering a collaborative environment where innovation and customer satisfaction are at the forefront. With a focus on transforming the alternative lending space, Greenbox Capital leverages cutting-edge technology and data-driven strategies to deliver exceptional financial products and services. Join us to be part of a dynamic team dedicated to making a meaningful impact in the financial industry while advancing your career in a supportive and innovative setting.About The RoleWe are seeking a highly skilled Machine Learning Engineer to join our Data Science team on a remote basis in the US, based out of Jacksonville, FL. In this pivotal role, you will be responsible for designing, developing, and deploying sophisticated machine learning models that influence our decision-making processes. Your expertise will help transform raw data into actionable insights, enhance our product offerings, and improve operational efficiencies. You will work closely with data scientists, software engineers, and business stakeholders to build scalable MLOps and LLMOps pipelines, ensuring seamless integration and deployment of machine learning solutions. This role offers an exciting opportunity to be at the forefront of innovation within the fintech industry, contributing to projects that have a direct impact on our company's growth and success.QualificationsMasters Degree in Machine Learning, Computer Science, Quantitative Finance, Statistics, or Industrial Engineering5+ years of experience in a Machine Learning Engineering roleHands-on experience with ML models in Natural Language Processing, computer vision, and statistical learning theoryProficiency with MLflow, Databricks, Delta Lake, and Azure cloud servicesStrong programming skills in Python and SQLExperience with CI/CD practices using Azure DevOps, GitHub Actions, or similar toolsKnowledge of version control systems, particularly GitExperience with data pipelines, ETL processes, and data integrationFamiliarity with data visualization tools such as Power BI or Databricks SQLExperience working in Agile environments with tools like Azure DevOps or JIRAAbility to communicate technical concepts effectively to non-technical stakeholdersResponsibilitiesDesign and implement MLOps CI/CD pipelines for new data science initiativesDevelop and manage LLMOps pipelines for advanced Data & Analytics projectsCollaborate with data scientists, engineers, and business teams to translate requirements into scalable solutionsLeverage Azure DevOps for project tracking, management, and prioritizationEnsure data quality, pipeline reliability, and system robustnessOptimize machine learning models for performance and scalabilityStay updated with the latest trends and advancements in MLOps, data engineering, and AI technologiesParticipate in code reviews, testing, and documentation to maintain high-quality standardsAssist in troubleshooting and resolving technical issues related to machine learning workflowsContribute to the continuous improvement of data science processes and infrastructureBenefitsCompetitive salary and performance-based incentivesFlexible remote work environmentHealth, dental, and vision insurance plansRetirement savings options with company matchingPaid time off and holidaysOpportunities for professional development and certificationsCollaborative and innovative company cultureAccess to cutting-edge tools and technologies in AI and data scienceEqual OpportunityGreenbox Capital is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based on race, ethnicity, gender, age, religion, sexual orientation, disability, or any other protected status. We believe that diverse teams foster innovation and drive better business outcomes.",https://www.wiraa.com/job-description/us853D36B43076FE4818DE6136653AE03C?source=Linkedin,5
20,Jobot,https://www.linkedin.com/company/jobot/life,Staff Data Engineer – AI Startup,https://www.linkedin.com/jobs/view/4257179916,4257179916,"Worcester, MA",Remote,$175K/yr - $215K/yr,2025-07-02 17:25:22,True,24 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Early-Stage AI startup is hiring for multiple data engineers ranging from senior to staff level to join the team

This Jobot Job is hosted by Sydney Weaver

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $175,000 - $215,000 per year

A Bit About Us

Join a breakout team building the data backbone of AI—help us scale secure, high-impact systems at the intersection of privacy, performance, and multimodal learning.

This early-stage company is building a platform that enables AI developers to access critical training data—legally, securely, and at scale. Already in use across healthcare and media, the system handles complex data types like medical imaging, clinical notes, and licensed video. With fewer than 30 employees and $10M in seed funding from CRV, SV Angel, and Bloomberg Beta, this is a rare opportunity to shape core infrastructure at a company solving one of AI’s biggest bottlenecks.



Why join us?


Great Benefits, Remote Work, Growth Opportunity, Great Culture, PTO, 401K, and much more!

Job Details

Requirements (Must Haves)

 Bachelor’s degree in Computer Science or a related field and a minimum of 6 years of data engineering experience Experience working at an early stage startup (required) Java (required) and Python (required) Comfortable building data infrastructure in AWS Hands-on with DBT for transformations, testing, and data ops (required) Prior work with Snowflake / Databricks Experience with at least 1 of these orchestration tools Airflow, Prefect, or Dagster  US Based Demonstrated ownership, grit, and interest in shaping the future of AI infrastructure.

Responsibilities

 Build and scale AWS-based data pipelines for ingest, transformation, validation, and orchestration. Develop and maintain orchestration systems using Airflow, Prefect, or Dagster. Create scalable workflows to handle complex, multimodal data including video, imaging, and structured documents. Use dbt to standardize transformation logic, testing, and data documentation. Optimize cloud compute and storage operations for performance and cost-efficiency. Contribute to architecture and development of core platform components. Drive ambiguous projects from ideation to production with full ownership. Help shape engineering practices and infrastructure from the earliest stages.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257179916,5
18,Jobot,https://www.linkedin.com/company/jobot/life,Senior AI Engineer - Remote,https://www.linkedin.com/jobs/view/4257179829,4257179829,"Chicago, IL",Remote,$170K/yr - $230K/yr,2025-07-02 17:25:22,True,6 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

About Us

Our client is a fast-growing, venture-backed startup transforming how professionals in law and finance extract insights from unstructured data. Their platform enables attorneys and financial experts to forecast, analyze, and negotiate deals with the power of their firm’s collective knowledge—turning static documents into real-time, actionable intelligence. Serving elite firms in M&A, debt finance, asset management, and real estate, this team is reimagining deal execution through AI and LLM technologies. They’re backed by leading investors in enterprise AI and are scaling rapidly across the U.S.

Job Details

Are you a good fit?

 Develop and deploy AI models for legal and finance sectors Collaborate with cross-functional teams to enhance platform capabilities Ensure scalability and performance of AI solutionsExperience with AI and machine learning technologiesStrong programming skills in PythonFamiliarity with legal and finance industries
Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257179829,5
17,Direct Client Requirements,https://www.linkedin.com/company/rahul-kumar/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4260879914,4260879914,"El Segundo, CA",On-site,$65/hr - $68/hr,2025-07-02 15:45:37,True,over 100 applicants,"Role: Senior Data Engineering & AnalyticsDuration: 1 YearLocation: El Segundo, CA - 90245
***Only W2 Contract: USC & GC***
Skills Required:Expert in SQL, PythonHighly prefer - Snowflake, Tableau or PowerBI, understanding of APIs
Key Responsibilities- Manage and maintain existing data ingestion pipelines to ensure data accuracy and reliability.- Review, troubleshoot, and update data ingestion processes, including schema migrations.- Develop new data ingestion workflows and transformations to support business reporting and visualization needs.- Identify opportunities to automate data input processes and build APIs for external data integration.",https://www.linkedin.com/jobs/view/4260879914,5
15,M1 Technology LLC,https://www.linkedin.com/company/m1-technology/life,Data Engineer,https://www.linkedin.com/jobs/view/4260893988,4260893988,"Tysons Corner, VA",On-site,,2025-07-02 17:47:26,True,27 applicants,"We are seeking an experienced Data Engineer to support data operations within the Mission Integration Data Science Environment (MIDSE). This role is responsible for maintaining and expanding the data pipeline that fuels critical national security missions. The ideal candidate will have a deep background in Python-based data workflows and experience working in classified IC environments.

Clearance Requirement : TS/SCI with Polygraph

Key Responsibilities:

Maintain and execute data extraction pipelines to transport data in and out of MIDSE. Enrich existing datasets using external and under-represented data sources. Integrate and manage data catalog capabilities for discoverability and governance. Troubleshoot and resolve issues related to data ingestion, integration, and processing. Develop and enhance scripts and tools using Python and PySpark to process structured (SQL) and unstructured (JSON, XML) data. Interface with cloud-based storage solutions (object repositories, RDBMS) to support analytics platforms. Manage data acquisition via secure HTTPS requests, direct DB connections, and standalone file transfers. 

Required Qualifications:

Bachelor’s degree in Computer Science, Engineering, or related field; or equivalent experience. 5+ years of software development experience with at least one of: Python, Java, C#, JavaScript, or Ruby (Python strongly preferred). 5+ years of experience working with databases and applications in classified IC environments. Extensive experience designing and maintaining SQL-based data systems and crafting complex queries. Proficiency in generating and interpreting technical documentation (e.g., ERDs, schemas, architecture diagrams). In-depth knowledge of IC cloud environments and C2E cloud services. Experience integrating data systems with visualization and analytics platforms (Tableau, PowerBI, Kibana, Grafana, etc.). 

Desired Skills:

Familiarity with IC metrics and assessments: alignment with intelligence needs, responsiveness, and collection discipline utilization. Experience building tools to assess and visualize large-scale datasets. Proven ability to design and implement secure APIs for data access and integration. 

Why Join Us?

Be part of a mission-focused team delivering critical insights to key decision-makers.

You’ll work in a dynamic, cloud-based environment with cutting-edge tools and the opportunity to shape how data fuels intelligence operations.

M1 Technology is an equal opportunity employer and values diversity. We do not discriminate in hiring on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by federal, state, or local law.",https://www.linkedin.com/jobs/view/4260893988,5
12,TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,Data Engineer,https://www.linkedin.com/jobs/view/4260887673,4260887673,"Richmond, VA",Hybrid,,2025-07-02 16:27:07,False,,"Senior Data Engineer

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do:Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:Bachelor's DegreeAt least 3 years of experience in application development (Internship experience does not apply)At least 1 year of experience in big data technologies
Preferred Qualifications:5+ years of experience in application development including Python, SQL, Scala, or Java2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)2+ year experience working on real-time data and streaming applications2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of data warehousing experience (Redshift or Snowflake)3+ years of experience with UNIX/Linux including basic commands and shell scripting2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
McLean, VA: $158,600 - $181,000 for Senior Data Engineer
Richmond, VA: $144,200 - $164,600 for Senior Data Engineer









Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).PDN-9f46bd69-98e6-48e2-833c-11be559b913f",https://talentally.com/job/senior-data-engineer-3673/redirect,5
11,SoTalent,https://www.linkedin.com/company/sotalentjobs/life,Big Data Engineer,https://www.linkedin.com/jobs/view/4257610320,4257610320,"Chicago, IL",Hybrid,$120K/yr - $150K/yr,2025-07-02 19:20:12,True,52 applicants,"Job Title: Big Data EngineerLocation: Chicago, Illinois (Onsite 4 days a week)Type: Full time
Join our fast-paced engineering team to design and build scalable, high-performance data pipelines that power critical business insights. We’re looking for a creative problem solver who thrives in collaborative environments and is passionate about data architecture, big data technologies, and continuous improvement.
What You’ll Do:Lead the end-to-end development of robust batch and streaming data pipelinesOptimize data workflows for performance, scalability, and cost-effectivenessMonitor, troubleshoot, and enhance data systems to ensure uptime and reliabilityMaintain internal monitoring tools to drive operational excellenceCollaborate with cross-functional teams to define and deliver impactful data solutionsTranslate technical concepts for non-technical stakeholders with clarityStay current with emerging technologies and introduce best practicesDocument systems and mentor junior team members to foster growth and collaboration
What You Bring:Degree in Computer Science, Engineering, or a related field2+ years of experience building and managing large-scale data systemsProficiency in Python, Java, Scala, and SQLHands-on experience with big data tools like Spark, Flink, or modern ETL frameworksFamiliarity with cloud-based storage and processing solutionsUnderstanding of data modeling, ETL design, and modern data architecturesKnowledge of workflow orchestration tools (e.g., Airflow) and cloud platformsStrong problem-solving skills and a detail-oriented mindsetExcellent communicator and team player who can juggle multiple priorities
Bonus Points:Experience optimizing complex, high-volume data pipelinesBackground working with structured and semi-structured data like Ad LogsAbility to lead planning and coordination efforts with product stakeholders",https://www.linkedin.com/jobs/view/4257610320,5
10,Akkodis,https://www.linkedin.com/company/akkodis/life,GCP Data Engineer,https://www.linkedin.com/jobs/view/4259555508,4259555508,"Dearborn, MI",Hybrid,$55/hr - $65/hr,2025-07-02 16:46:41,True,over 100 applicants,"Akkodis is seeking multiple of a GCP Data Engineer for a 12+ months contract position with one of our Automotive Industry leading Direct Client in Dearborn, MI (hybrid schedule). The ideal candidate would be someone with a minimum 10+ years of overall experience in Data Engineering with GCP and Python.
Pay Range: $55.00 - $65.00 hourly on W2(all inclusive), the pay range may be negotiable based on experience, education, geographic location, and other factors.
Position Description:Design data solutions in the cloud or on premises, using the latest data services, products, technology, and industry best practices.Experience migrating legacy data environments with a focus performance and reliability.Data Architecture contributions include assessing and understanding data sources, data models and schemas, and data workflows.Ability to assess, understand, and design ETL jobs, data pipelines, and workflows.BI and Data Visualization include assessing, understanding, and designing reports, creating dynamic dashboards, and setting up data pipelines in support of dashboards and reports.Data Science focus on designing machine learning, AI applications, MLOps pipelines.Addressing technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products.Experience in crafting data lake house solutions in GCP. This includes relational & vector databases, data warehouses, data lakes, and distributed data systems.Must have PySpark API processing knowledge utilizing resilient distributed datasets (RDSS) and data frames.
Skills Required:Design data solutions in the cloud or on premises, using the latest data services, products, technology, and industry best practices.Experience migrating legacy data environments with a focus performance and reliability.Data Architecture contributions include assessing and understanding data sources, data models and schemas, and data workflows.Ability to assess, understand, and design ETL jobs, data pipelines, and workflows.BI and Data Visualization include assessing, understanding, and designing reports, creating dynamic dashboards, and setting up data pipelines in support of dashboards and reports.Data Science focus on designing machine learning, AI applications, MLOps pipelines.Addressing technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products.Experience in crafting data lake house solutions in GCP. This includes relational & vector databases, data warehouses, data lakes, and distributed data systems.Must have PySpark API processing knowledge utilizing resilient distributed datasets (RDSS) and data frames.
Skills Preferred:Ability to write bash, python and groovy scripts to help configure and administer tools.Experience installing applications on VMs, monitoring performance, and tailing logs on Unix.PostgreSQL Database administration skills are preferred.Python experience and experience developing REST APIs
Experience Required:10 + Years
Education Required:Bachelor's Degree Computer Science, Computer Information Systems, or equivalent experience.
Education Preferred:Masters Data Science
Equal Opportunity Employer/Veterans/Disabled:
Benefit offerings available for our associates include medical, dental, vision, life insurance, short-term disability, additional voluntary benefits, an EAP program, commuter benefits, and a 401K plan. Our benefit offerings provide employees the flexibility to choose the type of coverage that meets their individual needs. In addition, our associates may be eligible for paid leave including Paid Sick Leave or any other paid leave required by Federal, State, or local law, as well as Holiday pay where applicable. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs that are direct hires to a client.To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy.The Company will consider qualified applicants with arrest and conviction records in accordance with federal, state, and local laws and/or security clearance requirements, including, as applicable:· The California Fair Chance Act· Los Angeles City Fair Chance Ordinance· Los Angeles County Fair Chance Ordinance for Employers· San Francisco Fair Chance Ordinance.
Thanks & RegardsAditya AgnihotriSr. Resource Development ManagerEmail: aditya.agnihotri@akkodisgroup.comDirect: +1(610)-472-0979LinkedIn: https://www.linkedin.com/in/aditya-agnihotri-7300722061/
(An Adecco Group Company)World Leader in IT and Engineering Workforce Solutionswww.akkodis.com
“Believe you can and you're halfway there.” — Theodore Roosevelt",https://www.linkedin.com/jobs/view/4259555508,5
9,TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,"Lead Data Engineer (Python, SQL, AWS)",https://www.linkedin.com/jobs/view/4260891478,4260891478,"Longview, TX",Hybrid,,2025-07-02 16:45:28,False,,"Lead Data Engineer (Python, SQL, AWS)

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Lead Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do:Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:Bachelor's DegreeAt least 4 years of experience in application development (Internship experience does not apply)At least 2 years of experience in big data technologiesAt least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)
Preferred Qualifications:7+ years of experience in application development including Python, SQL, Scala, or Java4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)4+ year experience working on real-time data and streaming applications4+ years of experience with NoSQL implementation (Mongo, Cassandra)4+ years of data warehousing experience (Redshift or Snowflake)4+ years of experience with UNIX/Linux including basic commands and shell scripting2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).


The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
McLean, VA: $193,400 - $220,700 for Lead Data Engineer
Plano, TX: $175,800 - $200,700 for Lead Data Engineer
Richmond, VA: $175,800 - $200,700 for Lead Data Engineer








Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).PDN-9f46bd5e-8375-4ed4-a735-63b0b4029d44",https://tracking.prodivnet.com/track/apply/9f46bd5e-8375-4ed4-a735-63b0b4029d44,5
8,Synechron,https://www.linkedin.com/company/synechron/life,Big Data Engineer,https://www.linkedin.com/jobs/view/4257186574,4257186574,"Charlotte, NC",Hybrid,$100K/yr - $110K/yr,2025-07-02 15:41:58,True,over 100 applicants,"We areAt Synechron, we believe in the power of digital to transform businesses for the better. Our global consulting firm combines creativity and innovative technology to deliver industry-leading digital solutions. Synechron’s progressive technologies and optimization strategies span end-to-end Artificial Intelligence, Consulting, Digital, Cloud & DevOps, Data, and Software Engineering, servicing an array of noteworthy financial services and technology firms. Through research and development initiatives in our FinLabs we develop solutions for modernization, from Artificial Intelligence and Blockchain to Data Science models, Digital Underwriting, mobile-first applications and more. Over the last 20+ years, our company has been honored with multiple employer awards, recognizing our commitment to our talented teams. With top clients to boast about, Synechron has a global workforce of 14,500+, and has 58 offices in 21 countries within key global markets.

Our challengeWe are looking for a highly skilled and experienced Senior Big Data Engineer with over a decade of expertise in building large-scale data solutions utilizing Hadoop and Spark ecosystems. The ideal candidate will be responsible for designing, developing, and optimizing complex data processing architectures to enable advanced analytics, machine learning, and reporting platforms.

Additional Information*The base salary for this position will vary based on geography and other factors. In accordance with law, the base salary for this role if filled within Charlotte, NC is $100k - $110k/year & benefits (see below).
The RoleResponsibilities:Design, develop, and optimize big data pipelines and processing frameworks using Hadoop (HDFS, MapReduce, YARN) and Apache Spark.Build scalable data ingestion processes and data lakes for diverse data sources.Develop and maintain ETL workflows that handle processing of structured and unstructured data.Collaborate with Data Scientists, Analysts, and Business Teams to translate requirements into technical solutions.Tune and troubleshoot Spark and Hadoop jobs for maximum efficiency and performance.Implement data security, privacy, and compliance best practices across all platforms.Mentor junior team members and foster best practices in big data development.Stay current with emerging trends and technologies related to Hadoop and Spark.Document architecture, workflows, and standards for maintainability and knowledge sharing.
Requirements:Bachelor’s, Master’s, or Ph.D. in Computer Science, Information Technology, or related field.10+ years of industry experience in big data engineering with proven expertise in Hadoop and Spark technologies.Extensive experience with Hadoop ecosystem components: HDFS, MapReduce, YARN, Hive, Pig, HBase, and Oozie.Strong proficiency in Apache Spark (Scala, Python, or Java) and Spark SQL.Experience with data ingestion tools such as Apache NiFi, Kafka, or Flume.Hands-on experience with cloud platforms (AWS, Azure, GCP) and their big data services integrated with Hadoop/Spark.Knowledge of data modeling, data warehousing, and database technologies (NoSQL, relational systems).Familiarity with containerization and orchestration tools like Docker and Kubernetes.Familiar with data governance, security, and compliance standards.Excellent problem-solving, system architecture, and communication skills.

Preferred, but not required:Experience with Spark Streaming and real-time data processing.Knowledge of advanced analytics, machine learning pipelines, and integration with Spark MLlib.Experience with automation and orchestration tools such as Apache Airflow.Familiarity with version control and CI/CD practices for big data platforms.
We offer:A highly competitive compensation and benefits package.A multinational organization with 58 offices in 21 countries and the possibility to work abroad.10 days of paid annual leave (plus sick leave and national holidays).Maternity & paternity leave plans.A comprehensive insurance plan including medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region).Retirement savings plans.A higher education certification policy.Commuter benefits (varies by region).Extensive training opportunities, focused on skills, substantive knowledge, and personal development.On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses.Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups.Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms.A flat and approachable organization.A truly diverse, fun-loving, and global work culture.
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.",https://www.linkedin.com/jobs/view/4257186574,5
7,"Market Street Talent, Inc.",https://www.linkedin.com/company/market-street-talent-inc-/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4260888399,4260888399,"Portland, Maine Metropolitan Area",Hybrid,,2025-07-02 16:26:09,True,over 100 applicants,"Could you be a good fit? We are looking for the best, highly skilled Senior Data Engineer to join the team of our exceptional client located in Portland, ME for a contract opportunity. 
Benefits:• Healthcare Medical (PPO or HMO)• Dental Insurance• 401k
What will your day look like? As a Senior Data Engineer, you will:Implement and design scalable serverless data solutions using AWS services such as Lambda and Glue.Maintain and develop reliable and scalable data solutions that support business and operational requirements. Collaborate with cross-functional teams to meet and understand data needs for near real-time processing.Document architecture decisions, data flows, and metadata to ensure maintainability and knowledge sharing.Implement and design fault-tolerant systems, ensuring resilience and high availability in data processing pipelines.
You will be a good fit for the Senior Data Engineer role if you have:5 plus years of related hands-on experience within a technology-driven environmentProven experience maintaining and building operational data pipelines, particularly with streaming technologies like AWS KinesisStrong proficiency in Apache Spark and ScalaExperience in AWS cloud services with a focus on serverless architectures for data processing (Glue, Lambda, etc.)SQL and NoSQL databases (Oracle, MySQL, PostgreSQL, MongoDB, DynamoDB)Experience building ETL pipelines and data services in the cloud using infrastructure as code and large-scale data processing engines, like SparkStrong experience with SQL and one or more programming languages, with Python preferredStrong communication and problem—solving skills, with the ability to prioritize and adapt to changing business needs
About Market Street Talent
Our Vision: To promote and foster the growth of information technology (IT) in our world…one candidate, one client, one community at a time.Our Goal: To coach clients and candidates through the entire placement process and cultivate long term healthy business relationships.Our Culture: At MST, we believe in pursuing excellence in everything we do, treating everyone with the utmost respect, and showing empathy for our community.",https://www.linkedin.com/jobs/view/4260888399,5
6,Dave & Buster's Inc.,https://www.linkedin.com/company/daveandbusters/life,Sr PowerBI Developer,https://www.linkedin.com/jobs/view/4259572213,4259572213,"Coppell, TX",Hybrid,,2025-07-02 19:11:08,False,,"Job Description

We are seeking a Senior Power BI Developer with administrative experience to support our retail business intelligence initiatives. This role will focus on building high-performance Power BI dashboards and reports to support sales, inventory, customer behavior, and operational analytics while managing and administering the Power BI Service environment.

The ideal candidate brings a deep understanding of retail metrics, technical expertise in Power BI development and administration, and a collaborative mindset to work across merchandising, marketing, finance, and operations teams.

Logistics

Candidate must physically work at Store Support Center, located in Coppell, TX; Monday – Thursday. Friday is WFH until further noticeCandidate must be US Citizen or Green Card HolderIf the candidate opts to relocate to the DFW area, relocation assistance will NOT be provided

The Essentials

Works across Technology, Vendor, and D&B operational and executive teams for data-related and insight needs.Familiarity with data structures, storage systems, cloud infrastructure, and other technical tools.Ability to work effectively in teams of technical and non-technical individuals.Ability to continuously learn, work independently, and make decisions with minimal supervisionDemonstrate accountability, prioritize tasks, and consistently meet deadlines.Familiarity with Agile/SCRUM practices

Scope

Design, develop, and maintain interactive dashboards and visualizations for retail-specific use cases (e.g., sales performance, inventory turnover, product margin, foot traffic, promotions).Translate retail business needs into scalable and insightful BI solutions.Build data models, measures, and KPIs using DAX and Power Query.Optimize report performance for large volumes of POS, ERP, and e-commerce data.Work with retail stakeholders to define requirements and deliver insights that drive decisions.Administer Power BI Service including workspace management, dataset refreshes, gateway configuration, and capacity monitoring.Implement and manage row-level security (RLS) and user access across departments and store locations.Maintain compliance with corporate data governance and privacy policies (e.g., customer data protection).Monitor usage metrics and audit logs to ensure efficient and secure BI operations.Study, analyze and understand business requirements in context to business intelligence.Work closely with data engineering teams to integrate data from POS systems, inventory management platforms, CRM, and e-commerce channels.Partner with merchandising, marketing, finance, and store operations to understand analytics needs.Develop strong data documentation about algorithms, parameters, modelsServe as a mentor and technical lead to Power BI developers and analysts.Establish and promote BI development standards, best practices and documentation processes.

Credentials

Bachelor’s Degree in Computer Science or Mathematics or Software Engineering7+ years of experience in Power BI dashboard development.2+ years administering Power BI in production environment.Advanced skills in DAX, Power Query and data modeling.MS Power BI CertificationExperience in restaurant / gaming industry

The Goods

Minimum 5 years of experience of working with BI toolsKnowledge about database management, SQL querying, data modeling, and Online Analytical Processing.Must be able to build rich dashboards, write DAX expressions, and implement security.Additional consideration given for experience scripting and programming language such as PythonIn depth knowledge of Microsoft Business Intelligence stack like Power Pivot, SSRS, SSIS, and SSAS

WHAT’S IN IT FOR ME?:

Dave & Buster’s is an imaginative and dynamic company dedicated to creating innovative entertainment experiences that bring joy, laughter, and excitement to people of all ages. Our mission is to foster a culture of fun and creativity, and we take pride in our commitment to delivering memorable and unique entertainment solutions.

Exclusive discounts on food and games at D&B & Main Event.Paid Time Off (PTO) that increases with tenure.10 Company Holidays (Including your Birthday) & 2 Floating Holidays per year.Medical, dental, vision and voluntary benefits Part Time/Full Time benefits availableSub Benefits:SurgeryPlus, and Telehealth benefits401k with company match following 6 months of employment.Buster’s Legacy Fund (Support Team Members during difficult Times)Employee Assistance Program (EAP) Offerings.Work out facility on-site.Employee Power Card | Free Video Games.

We work hard, play hard and have FUN!

Salary Range

102000

136000

We are an equal opportunity employer and participate in E-Verify in states where required.",https://daveandbusters.wd1.myworkdayjobs.com/Dave_and_Busters_Careers/job/Coppell-TX-Corporate/Sr-PowerBI-Developer_R-1008851?source=LinkedIn,5
4,Jobot,https://www.linkedin.com/company/jobot/life,Data Engineer,https://www.linkedin.com/jobs/view/4257178926,4257178926,"Harrisonburg, VA",Remote,$80K/yr - $100K/yr,2025-07-02 17:25:23,True,31 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Fully remote Data Engineer opportunity // Data Modeling experience required!

This Jobot Job is hosted by Craig Rosecrans

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $80,000 - $100,000 per year

A Bit About Us

We are currently seeking a seasoned Data Engineer to join our dynamic Tech Services team. The ideal candidate will be a data enthusiast with a solid understanding of the latest data technologies and trends coupled with a strong desire to deliver top-notch solutions to our clients. The successful candidate will be responsible for designing, developing, and maintaining data architectures, databases, and processing systems. With a focus on ETL, Data Modeling, SSIS, Tableau, and PowerBI, this role will be integral in transforming data into readable, goal-driven reports for continued innovation and growth.



Why join us?


 Competitive Base Salary Company paid health plan for employees Flexible Hours Very generous PTO Dental and Vision, FSA, HSA Small team, autonomy Many more great perks!

Job Details

Responsibilities

 Design, develop, automate, and maintain productivity tools using programming, database or scripting languages to improve software modeling and development. Design and implement ETL procedures for intake of data from both internal and outside sources; as well as ensure data is verified and quality is checked. Collaborate with data architects, modelers and IT team members on project goals. Develop and maintain databases by acquiring data from primary and secondary sources, and build scripts that will make our data evaluation process more flexible or scalable across data sets. Employ a variety of languages and tools to marry systems together or try to hunt down opportunities to acquire new data from other systems. Develop and implement data standards, ensuring data quality and consistency. Perform data profiling to identify and understand anomalies. Present information using data visualization techniques through Tableau and PowerBI. Work closely with team members, clients, project managers, and other stakeholders to ensure solutions are delivered timely and accurately.

Qualifications

 Bachelor's degree in Computer Science, Information Systems, or a related field. A minimum of 5+ years of experience in a data engineering role with a proven record of successful data manipulation. Proficiency with ETL tools, SSIS, and experience with SQL/NoSQL databases. Knowledge and experience in data modeling. Experience with business intelligence tools like Tableau, PowerBI or similar. Strong problem-solving skills, attention to detail, and ability to think critically. Excellent written and verbal communication skills, with the ability to present complex data in a clear and concise manner. Ability to work independently and with team members from different backgrounds. Excellent organizational skills and the ability to manage multiple tasks concurrently. A strong desire to learn and develop new skills, staying up to date with the latest data best practices and technologies. Ability to work in a fast-paced, deadline-driven work environment.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/jobs/view/4257178926,5
3,Largeton Group,https://www.linkedin.com/company/largeton-inc/life,Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4260899485,4260899485,"Miami, FL",On-site,,2025-07-02 18:17:12,True,62 applicants,"Lead the design, architecture, and development of scalable data solutions on the Snowflake platform. Provide technical leadership and mentorship to junior data engineers, ensuring adherence to best practices. Develop, implement, and optimize efficient ETL/ELT processes using Snowflake and third-party tools. Optimize and tune Snowflake warehouses, queries, and data ingestion processes for high performance. Establish and enforce robust data governance, security, and compliance standards. Automate and orchestrate data pipelines, leveraging features of Snowflake and Azure. Collaborate with cross-functional teams (data architects, scientists, analysts) to align infrastructure with business needs. Stay current with Snowflake features, best practices, and introduce innovative technologies or methodologies. Lead cloud integration and management efforts for Snowflake deployments on Azure. Maintain comprehensive technical documentation for data architecture, pipelines, and processes. Leverage deep expertise in Snowflake features (Snowpipe, Streams, Tasks, Materialized Views, Data Sharing, etc.). Extract and ingest data from APIs, ensuring high data quality and adherence to best practices. Apply advanced data modeling and warehousing concepts (star/snowflake schemas). Demonstrate advanced SQL proficiency for query optimization and troubleshooting. Utilize ETL/ELT tools (FiveTran, Matillion, Talend, Informatica) for data integration. Implement scripting and automation (Python, Bash) for data processes and CI/CD integration. Develop and tune machine learning models within Azure and Snowflake environments. Ensure compliance with data security standards (GDPR, HIPAA, SOC 2) and practices. Work within Agile development environments and adapt to fast-paced, iterative workflows.",https://www.linkedin.com/jobs/view/4260899485,5
1,Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Analytics Engineer, New Grad & Entry Level",https://www.linkedin.com/jobs/view/4259550903,4259550903,United States,Remote,,2025-07-02 16:25:16,False,,"Who we areAdelaide is the leader in one of the fastest-growing areas of digital advertising: attention metrics. Since 2020, we’ve been a trusted measurement partner for 40% of Fortune 50 companies. They rely on our metric, AU, to maximize the effectiveness of media spend. AU is “the attention economy's most widely recognized metric,” according to Adweek, and we swept the measurement category in the 2024 Adexchanger awards.
Position OverviewThis position reports to the Senior Data Analytics Engineer; it will behave cross functionally across Data teams with an emphasis on supporting the Analytics team and their workflows.In this role, you will be joining a team of data scientists and engineers. You'll help build and maintain the semantic layer of our data pipeline, ensuring clean, consistent, and reusable data models. Day-to-day activities range from developing and testing data models, editing LookML, managing a data catalog, automating data analysis workflows, and working closely with stakeholders to understand and support their data needs.
What you'll learnAn important part of our culture is continuing education and the sharing of ideas. We offer:A large network of investors and advisors for you to access that will help your team succeedMentorship from executives with decades of experience in adtech and mediaRegular internal knowledge-sharing sessionsEducation budget to accelerate your team’s development
Core responsibilities:Assist in building, testing, and maintaining transformational data models (dbt, Redshift)Help create and manage a clean, reliable reporting layer in Looker using LookMLWork cross-functionally with emphasized support to the Analytics TeamHelp identify and automate manual data analysis processes (Python, dbt)Contribute to maintaining a well-organized data catalog with accurate, accessible metadata for both technical and non-technical audiencesEnsure metric consistency across dashboards and data toolsCollaborate with analysts and PMs to document key metrics and definitionsMonitor adoption and identify opportunities for improved data usability
What you'll bring:SQL Proficiency – Strong ability to write, optimize, and debug SQL queries. You also understand data modeling and warehouse best practices, and you’re committed to writing clean, readable, and well-documented code.BI & Visualization Tools – Hands-on experience with BI/dashboarding platforms (e.g., Looker, Tableau).Data Modeling Tools – Familiarity with semantic modeling tools (e.g., LookML) and dbt; understanding of ETL concepts. Experience with orchestration tools (e.g., Airflow) is a plus.Data Quality & Testing – Strong attention to detail in building data validation, profiling routines, and root cause analysis workflows.Programming Skills – Experience with Python for data transformation, scripting, and automation; experience with associated libraries (e.g., pandas, openpyxl) is a plus.Communication & Collaboration – Excellent interpersonal skills with experience working cross-functionally; ability to translate technical concepts for non-technical audiences and deliver training/support.Educational Background & Experience – Bachelor’s degree in a quantitative, technical, or analytical field (e.g., Computer Science, Math, Physics, Engineering) or a rigorous coding bootcamp with a portfolio demonstrating the above skills.",https://jobright.ai/jobs/info/6859f854d3b885d69bbfe235?utm_source=1124&utm_campaign=6859f854d3b885d69bbfe235&tob=true,5
24,Centraprise,https://www.linkedin.com/company/centraprise/life,Data Engineer (Full time role),https://www.linkedin.com/jobs/view/4259547968,4259547968,"Jersey City, NJ",Hybrid,,2025-07-02 15:57:21,True,over 100 applicants,"Advanced Python programming skills with APIs. Expertise in App design and solutioning. Exp in designing and Implementing Kafka producers and consumers.Experience with Mongo collections, caching technologies like Hazel cast and performance tuning.Deep Development experience in backend data engineering and custom data warehouse applications.Experience in building ETL Pipelines using PySpark or Data Factory. DW experience in MS SQL Server/Oracle/Sybase/Snowflake etc.Knowledge on CI/CD, AKS, Cloud Services,Azure/DBT experience is a plus",https://www.linkedin.com/jobs/view/4259547968,5
25,Gensyn,https://www.linkedin.com/company/gensynai/life,Systems Engineer,https://www.linkedin.com/jobs/view/4259582332,4259582332,United States,Remote,,2025-07-02 19:45:30,False,,"Machine intelligence will soon take over humanity’s role in knowledge-keeping and creation. What started in the mid-1990s as the gradual off-loading of knowledge and decision making to search engines will be rapidly replaced by vast neural networks - with all knowledge compressed into their artificial neurons. Unlike organic life, machine intelligence, built within silicon, needs protocols to coordinate and grow. And, like nature, these protocols should be open, permissionless, and neutral. Starting with compute hardware, the Gensyn protocol networks together the core resources required for machine intelligence to flourish alongside human intelligence.

The Role

Build the software components that make the Gensyn network solve complex distributed machine learning problems

Responsibilities

Contribute to and maintain the Gensyn executor, mesh network, and SDK, adding new features, optimizing existing solutions, fixing bugs, and participating in the architectural evolution of the systemContribute to system architecture and design decisions for large-scale deploymentsConduct code and design reviews to uphold high standards of code quality across the teamMeasure and optimize existing components to ensure they meet end-user requirements effectively

Competencies

Must Have

Rock solid computer science fundamentalsDemonstrable production experience shipping codeProficient in one system-level language (e.g. C/C++, Rust, Go, Zig, …)Deep understanding of distributed systems or systems at scaleOperating system fundamentals - preferably Linux/macOSHighly self-motivated with excellent verbal and written communication skills

Preferred

Experience with common networking protocolsExperience working in high-growth start/scale-up environmentsExperience working with observability tools, such as metrics and tracesExperience with building distributed databases, file systems, or queuesExperience with UNIX/POSIX APIs

Nice to have

Experience with AI systems in productionExperience contributing to or maintaining open-source distributed systemsPerformance optimization tooling and tactics experienceProficient understanding of distributed systems concepts (Consensus, CAP, Fault Tolerance)Proficient in Rust

Compensation / Benefits

Competitive salary + share of equity and token poolFully remote work - we currently hire between the West Coast (PT) and Central Europe (CET) time zonesVisa sponsorship - available for those who would like to relocate to the US after being hired3-4x all expenses paid company retreats around the world, per yearWhatever equipment you needPaid sick leave and flexible vacationCompany-sponsored health, vision, and dental insurance - including spouse/dependents [🇺🇸 only]

Our Principles

Autonomy & Independence

Don’t ask for permission - we have a constraint culture, not a permission culture.Claim ownership of any work stream and set its goals/deadlines, rather than waiting to be assigned work or relying on job specs.Push & pull context on your work rather than waiting for information from others and assuming people know what you’re doing.Communicate to be understood rather than pushing out information and expecting others to work to understand it.Stay a small team - misalignment and politics scale super-linearly with team size. Small protocol teams rival much larger traditional teams.

Rejection of mediocrity & high performance

Give direct feedback to everyone immediately - rather than avoiding unpopularity, expecting things to improve naturally, or trading short-term pain for extreme long-term pain.Embrace an extreme learning rate - rather than assuming limits to your ability / knowledge.Don’t quit - push to the final outcome, despite any barriers.Be anti-fragile - balance short-term risk for long-term outcomes.Reject waste - guard the company’s time, rather than wasting it in meetings without clear purpose/focus, or bikeshedding.Build and design thinly.",https://job-boards.eu.greenhouse.io/gensyn/jobs/4465952101?gh_src=2b6c1912teu,5
16,TekDoors Inc.,https://www.linkedin.com/company/tekdoors/life,GCP Data Engineer,https://www.linkedin.com/jobs/view/4259564507,4259564507,"Phoenix, AZ",On-site,,2025-07-02 18:28:44,True,76 applicants,"Job Title: GCP Data EngineerLocation: Phoenix, AZContract Type: 12 MonthsContract, W2 Must Have:• 6–8 years of hands-on experience in data engineering roles.
Strong expertise with GCP services, specifically:• Dataproc• BigQuery• SQL• Pub/Sub• Airflow• Advanced proficiency in Python programming.• Experience designing and maintaining scalable data pipelines on cloud platforms.• Excellent problem-solving and analytical skills.",https://www.linkedin.com/jobs/view/4259564507,5
99,TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,Data Engineer,https://www.linkedin.com/jobs/view/4260889367,4260889367,"Salisbury, MD",Hybrid,,2025-07-02 16:24:02,False,,"Senior Data Engineer

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do:Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:Bachelor's DegreeAt least 3 years of experience in application development (Internship experience does not apply)At least 1 year of experience in big data technologies
Preferred Qualifications:5+ years of experience in application development including Python, SQL, Scala, or Java2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)2+ year experience working on real-time data and streaming applications2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of data warehousing experience (Redshift or Snowflake)3+ years of experience with UNIX/Linux including basic commands and shell scripting2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
McLean, VA: $158,600 - $181,000 for Senior Data Engineer
Richmond, VA: $144,200 - $164,600 for Senior Data Engineer









Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).PDN-9f46bd6a-6f81-46ac-839a-3f809a899d7b",https://tracking.prodivnet.com/track/apply/9f46bd6a-6f81-46ac-839a-3f809a899d7b,5
31,Rose International,https://www.linkedin.com/company/rose-international/life,Database Architect- Center for Data Analytics,https://www.linkedin.com/jobs/view/4261007902,4261007902,"Austin, TX",Hybrid,$95/hr - $107.80/hr,2025-07-02 19:11:37,True,3 applicants,"Date Posted: 07/02/2025Hiring Organization: Rose InternationalPosition Number: 484967Industry: GovernmentJob Title: Database Architect- Center for Data AnalyticsJob Location: Austin, TX, USA, 78751Work Model: HybridWork Model Details: 3 days on-site, 2 days remoteShift: M-f, 8-5Employment Type: TemporaryFT/PT: Full-TimeEstimated Duration (In months): 13Min Hourly Rate($): 95.00Max Hourly Rate($): 107.80Must Have Skills/Attributes: AWS, Azure, Big Data, CCDA, Data Architecture, Data Modeling, Data Warehouse, Java, Python, SQLExperience Desired: Data architecture, data modeling, and data warehousing (10+ yrs); Data architecture, data modeling, and data warehousing (10+ yrs); Ccloud platforms (Azure, AWS, or GCP) (10+ yrs); Cloud-based applications and data storage. (10+ yrs)Preferred Certifications/Licenses: TOGAF, Azure/AWS/GCP Cloud Solution Architect Job Description***Candidates must already reside in the Austin, TX area***
Minimum Requirements:• 10+ years of experience in data architecture, data modeling, and data warehousing.• 10+ years of experience with business intelligence and big data solutions.• 10+ years of experience with cloud platforms (Azure, AWS, or GCP).• 10+ years of experience in developing, deploying, and managing cloud-based applications and data storage.• Strong proficiency in SQL and other database technologies.• Deep understanding of data architecture principles and best practices.• Proven leadership in managing advanced analytics projects.• Experience with healthcare data standards (FHIR, CCDA, HL-7).• Proficiency in ETL tools, data integration platforms, and APIs.• Strong communication skills for both technical and non-technical audiences.• Experience with EHR integration and real-time data streaming.• Knowledge of accessibility standards for user interfaces and reporting tools.
Preferred Experience:• 8+ years of experience implementing and deploying big data applications.• Hands-on expertise in Python and/or Java/Scala.• Experience with hybrid cloud deployments and on-premise-to-cloud migration strategies.• Healthcare industry experience.• Understanding of Information Management principles, IT processes, and SDLC.• Strong consulting and facilitation skills.• Customer-focused communication across all organizational levels.• Proactive leadership style with strong attention to detail.• Certifications such as TOGAF, Azure/AWS/GCP Cloud Solution Architect.
The Senior Data Architect & Analytics Lead will play a critical role in shaping the data infrastructure and analytics strategy for the Texas Health and Human Services Commission (HHSC) Center for Data Analytics (CDA). This role is ideal for a seasoned professional with deep expertise in data architecture, cloud platforms, and advanced analytics, particularly within the healthcare domain. You will be responsible for designing and implementing scalable data structures, leading analytics initiatives, and ensuring data governance and accessibility standards are met. This position requires strong leadership, technical proficiency, and the ability to collaborate across multidisciplinary teams to drive data-driven decision-making.
Key Responsibilities:• Design and build relational databases, data warehouses, and multidimensional databases.• Develop strategies for data acquisition, archival, recovery, and implementation.• Clean and maintain databases, ensuring data integrity and performance.• Lead the design and modeling of cloud-based data architecture solutions using platforms like Snowflake.• Architect and manage ETL pipelines using tools such as Informatica.• Integrate and evaluate emerging technologies including AI, machine learning, and big data analytics.• Ensure compliance with EIR Accessibility standards and assistive technologies.• Maintain centralized metadata registries and enforce enterprise data standards.• Collaborate with cross-functional teams to align data strategies with organizational goals.• Lead analytics projects and provide insights to support strategic decision-making.• Support EHR integration, real-time streaming, and API-based data exchange.
**Only those lawfully authorized to work in the designated country associated with the position will be considered.**
**Please note that all Position start dates and duration are estimates and may be reduced or lengthened based upon a client’s business needs and requirements.**
 Benefits:For information and details on employment benefits offered with this position, please visit here. Should you have any questions/concerns, please contact our HR Department via our secure website.California Pay Equity:For information and details on pay equity laws in California, please visit the State of California Department of Industrial Relations' website here.Rose International is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender (expression or identity), national origin, arrest and conviction records, disability, veteran status or any other characteristic protected by law. Positions located in San Francisco and Los Angeles, California will be administered in accordance with their respective Fair Chance Ordinances.If you need assistance in completing this application, or during any phase of the application, interview, hiring, or employment process, whether due to a disability or otherwise, please contact our HR Department.Rose International has an official agreement (ID #132522), effective June 30, 2008, with the U.S. Department of Homeland Security, U.S. Citizenship and Immigration Services, Employment Verification Program (E-Verify). (Posting required by OCGA 13/10-91.).",https://www.linkedin.com/jobs/view/4261007902,5
33,IntePros,https://www.linkedin.com/company/intepros/life,Data Engineer,https://www.linkedin.com/jobs/view/4257612045,4257612045,"Austin, TX",Remote,$50/hr - $55/hr,2025-07-02 19:08:37,True,91 applicants,"Job Title: Data Engineer

Location: Austin, TX (Hybrid Onsite – Local Candidates Only)

Duration: 6 Months (Potential for Extension or Conversion)

Schedule: Monday–Friday, 9:00 AM – 5:00 PM CST

Rate: Up to $50-55/hr

Start Date: ASAP

About The Role

We are seeking a Data Engineer to join a fast-paced and innovative technology team focused on delivering high-impact data solutions. In this role, you will work on building and optimizing data pipelines, creating scalable BI applications, and enabling data-driven decision-making across global operations.

You’ll collaborate with software developers, data engineers, and business leaders to develop analytical solutions that improve program performance and support business strategy. This is a great opportunity for someone who enjoys working with large datasets, building automation, and translating complex data into insights.

Responsibilities

Build, scale, and optimize data pipelines and BI applications using modern cloud-based technologies.Design, build, and maintain dashboards and reports to monitor key business metrics.Perform statistical analysis, A/B testing, and exploratory data analysis.Collaborate with stakeholders to understand business needs and deliver analytical solutions.Respond to ad hoc data requests and deep dive into usage trends and unusual patterns.Support data architecture, automation, and optimization for ETL, data warehouses, and reporting tools.Participate in quarterly in-person team summits (occasional travel required).

Required Qualifications

3–6 years of experience working with large, complex datasets.Advanced proficiency in SQL and Python.Strong analytical and modeling skills.Experience with building and maintaining scalable data pipelines.Background in statistics or computer science.Strong communication skills across technical and business stakeholders.Bachelor’s degree in a related field (e.g., Computer Science, Mathematics, Engineering).

Preferred Qualifications

Experience with AWS big data services (e.g., Redshift, S3, Glue).Familiarity with BI tools such as QuickSight or similar.Experience with SAS is a plus.Graduate degree in statistics, mathematics, or related field.

",,5
29,Sikich,https://www.linkedin.com/company/sikich/life,Data Engineer,https://www.linkedin.com/jobs/view/4259565789,4259565789,United States,Remote,$123.2K/yr,2025-07-02 18:57:18,False,,"Description

Data Engineer

What to expect when you join the Sikich family

Team members at Sikich have a lot in common while also being part of a rich and diverse group of contributors, creating a distinct and thriving culture. Chief among our commonalities is a desire for growth and a shared unity of purpose in our professional lives. We believe that through diverse perspectives, challenging the status quo and rewarding action, we accelerate innovation and drive growth – for our clients, for ourselves and for our communities.

The professional services landscape continues to evolve. For Sikich, this means we have an opportunity to further cement our leadership position in this industry and continue to grow our organization in increasingly exciting ways. This growth is meaningful for every team member at our company because larger companies simply see more interesting client opportunities and can attract impressively talented individuals like you. Through a dedicated focus on key business priorities and intentionally creating a rewarding employee experience, Sikich has developed into a highly regarded provider of professional services and a sought-after employer of choice.

Do you want to work with other skilled practitioners and serve clients in a way that makes a difference? Are you seeking a supportive environment backed by a deep and extensive set of skillsets? Are you ready to make an impact and be acknowledged for your contributions? If you answered yes to these questions, we see a mutually beneficial and gratifying relationship on the horizon!

Are you ready to grow with us?

Position Summary

Sikich is looking for a highly motivated Data and Analytics professional to provide strategic and operational consulting services to our clients. The Data Engineer will be responsible for planning, designing, and delivering data engineering services to our Data & Analytics practice clients. You will work closely with our clients to understand and document their business requirements and translate them into effective data engineering solutions. You will design, develop, and maintain data pipelines, databases, and data warehouses. You will also collaborate with other teams to ensure successful project delivery.

Successful candidates will have 3 or more years or more experience working with large Fortune 1000 clients to develop data requirements, architectures, roadmaps, data lakes/warehouses, master and reference data programs, ERP & CRM implementations and migrations, as well as data catalogs, lineage, and governance. Additionally, Sikich is looking for data engineers with the communication and organizational skills necessary to provide input and guidance to both our clients and their peers. Experience with Generative AI, Predictive and Descriptive Analytics, machine learning, and GRC (governance, risk, and compliance) is also highly desired.

What will you do in this role?

Work with clients to identify requirements, design architectures & infrastructures, and implement solutions in support of their goals and objectives. Review and assess the maturity of various aspects of the information landscape including, but not limited to, Data Strategy, Governance, Stewardship, Warehousing/Cloud Lakes, Data Quality, and technical architecture as well as AI readiness and adoption. Design and document processes which ensure the availability, usability, integrity, security, and privacy of our client’s data. Collaborate with clients to design and implement operating models for data management, governance, analytics and AI, security, and privacy in including roles, responsibilities, and RACI matrices. Assist in identifying opportunities for instituting intelligent automation opportunities (e.g., AI – Machine Learning/Deep Learning), leveraging the Sikich use case framework. Develop processes, procedures, and plans to ensure high quality data, fit for the specific purposes and objectives of our clients. Create/document process maps, SOP’s, methodologies, training, and other materials to support all aspects of our client’s information management. Review and Assess client data management practices, processes, and technologies in general and in situ for integrity, efficiency, and effectiveness. Create, document and present recommendations for improvements to client data management practices, processes, and technologies. Collaborate with business and technology leaders to identify Key Data Elements, Assets, Features, and Models and create ontologies, taxonomies, and other standards necessary to manage each as an asset. Review, assess, and make recommendations to our clients on the use of metadata, master data, reference data in addition to operational and external data based on each client’s specific needs, objectives, and goals. Design and oversee the selection, implementation, and configuration of various technologies including, but not limited to cloud migration, data lakes, warehouses, ETL/ELT processes, metadata repositories, data catalogs, data lineage tools, data science workbenches, etc.. Review, assess, and make recommendations regarding the state of or need for MLOps, DataOps. Collaborate with clients to troubleshoot, identify, and rectify root causes of critical data management issues. Provide other Information and Data Management-related Services as required by our clients. 

What do you need to succeed in this role?

Bachelor's degree in computer science, information technology, or related field, or equivalent practical experience. Minimum of 3 years of experience in a similar role, preferably in a consulting or professional services environment. Proven track record of successfully delivering data engineering projects. Experience working with large-scale data sets and distributed computing frameworks.  Strong experience with T-SQL, stored procedures, PL/SQL, Oracle, MS SQL, and SQL Server Analysis Services. Proficiency in data integration and ETL tools, such as Azure Data Factory. Familiarity with cloud platforms, such as Azure, and related services, such as Azure ADF. Experience with SAS scripts for data manipulation and analysis. Strong practical experience with contemporary Information Management paradigms, technologies, and methodologies and the ability to clearly and effectively communicate with both technical and non-technical audiences. Proven track record of cloud migration, warehouse/lake/mart implementation, ETL/ELT development, Power BI, and other data management technologiesSolid understanding of emerging trends and new technologies in Artificial Intelligence, Machine Learning, MLOps, and Digital Transformation. Excellent problem-solving and analytical skills. Strong communication and interpersonal skills. Ability to work independently and as part of a team. Client-focused mindset and ability to build strong relationships. Exceptional organization skills with the ability to manage multiple priorities simultaneously and follow through to ensure timely completion. Travel required. 

In addition, specific skills/experience required are as follows: …..

Servant Leader – You are hyper focused on engaging employees, fostering their development, and building a positive culture. Solutions Focused – You see opportunities in every business problem and can develop, articulate, and implement solutions. Collaboration – You are a relationship builder across all levels of the organization and across all business units. Instills Trust - You do what you say, and you follow through on commitments, you act with integrity, you are consistent and are perceived as credible. Impact & Influence Thinking – You gain support for ideas, proposals, and solutions, and get others to act, with or without formal authority, to advance initiatives/objectives. 

About Sikich

Sikich is a global company specializing in Accounting, Advisory, and Technical professional services. With employees across the globe, Sikich ranks as one of the largest professional services companies in the United States. Our comprehensive skillsets, obtained over decades of experience as entrepreneurs, business owners and industry innovators, allow us to provide insights and transformative strategies to help strengthen every dimension of our clients’ businesses.

Sikich Total Rewards

Our team members enjoy expansive benefits ranging from competitive compensation and insurance options to wellness programs and a flexible time off policy, to name only a few. Sikich also takes pride in prioritizing team members’ health, total wellbeing and time spent with family, friends and in the pursuit of personal goals, hobbies, and endeavors.

In compliance with this state’s pay transparency laws, the midpoint of the base salary range for this role is $123,150. This is not a guarantee of compensation or salary, as final offer amount may vary based on factors including but not limited to experience, commission, and geographic location.

Some examples of our many benefits:

Sikich maintains a Flexible Time Off (FTO) Policy. We encourage every full-time employee, as your role permits, to utilize paid time off (personal time, mental/physical health care, vacation, sick leave, etc.). Waiting for time off to accrue is common at other companies. At Sikich, you do not have to wait for this benefit to kick in. FTO is activated on your first day with our organization. Sikich will also recognize paid holidays during the year and strives to permit employees to have time off the last week of the calendar year when client and project work permits. Sikich offers a comprehensive wellness program to engage, challenge and empower team members to take responsibility for their wellbeing. Activities can be tracked through our wellness provider to obtain gift cards and other rewards. 

We also offer:

Flexible work arrangementsHealth, dental, vision, life, and accident/death/disability insurance optionsHSA employer contributionNine (9) paid holidays annually. A robust paid Parental Bonding Leave program covering birth, adoption, and foster children. 401(k) with employer contributionsCPA bonus with four (4) paid exam days & four (4) paid study days. Tuition reimbursementGenerous employee referral bonus programClient referral bonus programPet insuranceFORCE – Sikich community volunteer program enabling each team member to use up to four hours of paid time annually to volunteer and make a difference in their local communities. 

Want to learn more? Visit our Careers website or Glassdoor profile.

Sikich is an Equal Opportunity Employer M/F/D/V

Sikich currently practices in an alternative practice structure in accordance with the AICPA Professional Code of Conduct and applicable law, regulations, and professional standards. Sikich CPA LLC is a licensed CPA firm that provides audit and attest services to its clients. Sikich LLC has a contractual arrangement with Sikich CPA LLC under which Sikich LLC provides Sikich CPA LLC with professional and support personnel and other services to support Sikich CPA LLC’s performance of its professional services, and Sikich CPA LLC shares certain client information with Sikich LLC with respect to the provision of such services.

",https://jobs.jobvite.com/sikichcareers/job/om3rwfw6?__jvst=Job+Board&__jvsd=LinkedIn,5
27,Hunter Bond,https://www.linkedin.com/company/hunter-bond/life,"Data Centre Engineer – Elite Quant - Up to $150,000 Starting base + Exceptional benefits/bonus package Fund – New York",https://www.linkedin.com/jobs/view/4257609328,4257609328,"New York, United States",Hybrid,$150K/yr,2025-07-02 18:54:55,True,11 applicants,"Title: Data Center EngineerClient: Quant Fund – Global collaborative firm run by passionate Computer ScientistsSalary: up to $150,000 + bonus + package/perksLocation: New Jersey (Hybrid/Remote)
My client is a dynamic global quant fund that is building cutting-edge systems. In this position you will be responsible for managing the company's cutting-edge Trading Infrastructure, which spans several colocated data centers and thousands of servers. This role focuses on enhancing the efficiency, speed, and reliability of the infrastructure.
The successful candidate will have the following skills/experience;✔️ Managing Infrastructure space, power, electrical systems, security, cabling, facilities – all things Datacentre✔️ Liaising with multiple teams to design and deploy Datacentre’s and associated facilities✔️ Managing build and construction Datacentre projects as per agreement with key stakeholders✔️ Capacity management✔️ Bachelors/Associates Degree

Please apply with your updated CV or email at hrubin@hunterbond.com.",,5
28,Xyant Services,https://www.linkedin.com/company/xyantservices/life,Sr. Data Engineer (MDM),https://www.linkedin.com/jobs/view/4259564238,4259564238,"San Jose, CA",Hybrid,,2025-07-02 18:07:59,True,over 100 applicants,"Role : Senior Data Engineer (MDM)Location : San Jose, CA (Hybrid)Duration : Contract
Job Description:  Looking for experience with MDM platforms and funneling data through RTP (formerly MDM). AI-based feature experience is a plus but not required; the team can train. Five Transference experiences are nice-to-have.Generative AI experience is a plus. Databricks experience is required. Must have a data engineering background in enterprise environments; architects are not a fit. Should be hands-on, capable of designing and improving data and pipelines. MongoDB, Databricks experience required. Neo4j and HANA experience are pluses due to migration into Databricks. Need to write and execute code.
Job Summary:We are seeking a skilled and proactive Senior Data Engineer to join our Master Data Management (MDM) team. This role is pivotal in designing and maintaining scalable data pipelines that support enterprise-wide master data initiatives. The ideal candidate will have deep expertise in cloud data platforms, and MDM principles, and will collaborate across teams to ensure high-quality, governed data is available for analytics and operational use.
Key Responsibilities:Develop and maintain automated ELT pipelines to ingest master data from systems such as SAP, Salesforce, and Oracle.Integrate Fivetran pipelines with cloud data platforms like Snowflake, BigQuery, and Azure Synapse.Collaborate with data stewards and business teams to enforce data quality rules and support golden record creation.Implement monitoring and observability solutions to ensure pipeline health and data accuracy.Optimize transformation logic using DBT and SQL for performance and scalability.Document data flows, mappings, and transformation logic for compliance and audit purposes.Support reporting and analytics needs through integration with BI tools like Power BI and Tableau.Work closely with product managers, architects, and analysts to align technical solutions with business goals.
Required Skills:Experience configuring and managing connectors and transformations.SQL Mastery: Advanced SQL skills for data extraction, transformation, and validation.Cloud Data Platforms: Hands-on experience with Snowflake, BigQuery, Redshift, or Azure Synapse.MDM Concepts: Understanding of master data domains and governance workflows.Python & DBT: Proficiency in Python scripting and DBT for modular pipeline design.Data Quality & Governance: Familiarity with tools like Informatica MDM, Monte Carlo, or Collibra.Metadata & Cataloging: Experience with metadata repositories and lineage tracking.BI Tools: Ability to support dashboards and reports using Power BI, Tableau, or Google Data Studio.DevOps & Automation: Knowledge of CI/CD pipelines, Git, and infrastructure-as-code tools.Collaboration: Strong communication and documentation skills; ability to work cross-functionally.Preferred Qualifications:Bachelor’s or Master’s degree in Computer Science, Information Systems, or related field.5+ years of experience in data engineering or analytics roles.Prior experience implementing MDM solutions and working with enterprise data architectures.Experience configuring and managing Fivetran connectors and transformations.",https://www.linkedin.com/jobs/view/4259564238,5
34,Brooksource,https://www.linkedin.com/company/brooksource/life,Data Access Engineer ,https://www.linkedin.com/jobs/view/4259558697,4259558697,"Franklin, TN",Hybrid,,2025-07-02 17:38:28,True,over 100 applicants,"As a Data Access Engineer, you will be responsible for providing expert content and professional leadership on complex assignments and projects. You will contribute to data oversight, data usage, system analysis, and data retention to assure data quality standards. Collaborating with other departments, you will ensure data consistency and quality throughout the organization. This role requires considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiatives. You will use your deep professional knowledge and acumen to advise functional leaders. This position is specifically put in place to complete the migration of usage across legacy systems (predominantly MS SQL Server but other RDBMS may be involved) to the Lakehouse (Azure Databricks). You will work with other analysts and development teams comprised of onshore and offshore developers, exploring data processes and usage of systems to determine next steps for migration and foster adoption from users to our modern platforms. This will require heavy analysis of usage against the legacy system as well as platforms within our Azure environment. You will interact with business stakeholders and SMEs. Healthcare data knowledge is a major plus. This is a very fast-paced environment that will require a lot of self-guidance and motivation.
Minimum Qualifications:In-depth knowledge in SQL, MS SQL Server stack (SSIS, SSRS, Information Schema), Azure Databricks, Azure Data Factory, Azure Storage Services, Python, SparkSQL, DevOps & GITFluent understanding of data models, schemas, and databases to support analytical and reporting needs, and use SQL for data manipulation and queryingAdvanced proficiency in data processingAdvanced capabilities with data profiling, validation, and analyticsAdvanced analytics skillsets with MS Excel, making use of Excel's data retrieval capabilities, pivot tables, data comparison operations, etc.Strong ability to explore, discover, and map out large data pipelines that support the processing of structured and semi-structured dataWorking knowledge of CICD pipelines for Azure resourcesHighly organized and able to track multiple fast-moving projects at a timeKnowledge and experience in an enterprise-level data warehouse environment with knowledge of data architecture patterns and best practicesIn-depth knowledge of the US Healthcare industry to address healthcare-related challenges through data engineering solutions with particular emphasis in medical claims dataExcellent written and verbal communication skills, with the ability to effectively convey complex analytical findings and tell a compelling story with dataAbility to work in a client-facing project environment independentlyResponsibilities:Provide expert content and professional leadership on complex Data Management Strategy & Governance assignments/projectsContribute to data oversight, data usage, system analysis, and data retention to assure data quality standardsCollaborate with other departments to ensure data consistency and quality throughout the organizationExercise considerable creativity, foresight, and judgment in conceiving, planning, and delivering initiativesUse deep professional knowledge and acumen to advise functional leadersWhat’s In It For You:Weekly PaychecksMedical, Dental & Vision BenefitsEducation:Bachelor’s degree preferred but not required3-5+ years of experience in an enterprise data warehousing environment with similar duties as described above, preferably using both MS SQL Server and Azure data platformsHS Diploma or GED is minimum requirement",https://www.linkedin.com/jobs/view/4259558697,5
26,Catapult Solutions Group,https://www.linkedin.com/company/catapultsolutionsgroup/life,Data Engineer II,https://www.linkedin.com/jobs/view/4259558875,4259558875,"Mountain View, CA",Hybrid,$70/hr - $75/hr,2025-07-02 18:04:03,True,over 100 applicants,"Data Engineer IIEngineering Department
Mountain View, California Hybrid 3X/week on-site ***(Must reside a commutable distance to be considered)***C2C will not be considered 
About Our ClientOur client is a well-established technology company operating in the Silicon Valley area with a focus on data-driven solutions and analytics. They maintain a collaborative engineering culture that emphasizes innovation and technical excellence. The company has built a mature data infrastructure and continues to invest in cutting-edge technologies to support their growing data needs. Their team values both independent problem-solving and cross-functional collaboration, working closely with data scientists and architects to deliver scalable solutions that drive business impact.
Job DescriptionWe are seeking an experienced Data Engineer to join our client's dynamic engineering team, where you'll play a crucial role in building and maintaining the data infrastructure that powers critical business decisions. In this position, you'll be responsible for developing scalable data pipelines, optimizing database systems, and translating complex algorithms into production-ready code that processes large-scale datasets.
Your day-to-day work will involve designing ETL processes using modern tools like Databricks and Spark, collaborating with cross-functional teams to understand data requirements, and implementing solutions that improve data accessibility and reliability. You'll have the opportunity to make a significant impact by identifying process improvements, automating manual workflows, and ensuring high-performance data systems that support advanced analytics and business intelligence initiatives.
The ideal candidate is passionate about solving complex data challenges, enjoys working with cutting-edge technologies, and thrives in a collaborative environment where your technical expertise directly contributes to the company's data-driven decision making capabilities.
Duties and ResponsibilitiesDesign, build, and maintain scalable data pipelines using Databricks and Spark for optimal data extraction, transformation, and loading from diverse sourcesDevelop and translate computer algorithms into prototype code for large-scale data processing systemsEmploy robust data modeling techniques to optimize databases and processing systemsMaintain, organize, and identify trends in large datasets to support business intelligence initiativesUse GitHub for version control and collaborative codebase management within development teamsEnsure high-quality data reliability and streamline data flow to support advanced querying and analyticsCollaborate with data scientists and architects to enhance system performance and data integrityOversee real-time business metric aggregation, data warehousing, querying, and schema managementIdentify, design, and implement internal process improvements including automation and infrastructure optimizationDevelop reports, dashboards, and tools for business users to access and interpret data insightsTroubleshoot and debug issues in data models to guarantee high performance and system accessibilityCreate comprehensive process documentation and provide technical guidance on data solutionsAggregate and analyze various datasets to provide actionable insights for stakeholders
Required Experience/SkillsMinimum of 3-4 years of experience as a Data EngineerProficiency in Databricks, Spark, SQL, and Python with solid experience in scripting and data manipulationStrong experience in building and maintaining data pipeline architecturesProficiency in SQL database design and optimizationKnowledge of algorithms, data structures, and performance optimizationExperience with processing and interpreting large datasetsFamiliarity with GitHub for version control and collaborative developmentStrong written and verbal communication skillsExcellent problem-solving and analytical abilitiesAbility to work independently and effectively in team environmentsExperience creating technical documentation and process workflows
Nice-to-HavesExperience with cloud services like AWS, Azure, or Google Cloud PlatformFamiliarity with computer coding languages such as Java, Kafka, Hive, or StormExperience with real-time data processing and streaming technologiesBackground in data warehousing and business intelligence toolsExperience working in fast-paced technology environmentsInterest in emerging data technologies and industry best practices
EducationBachelor's degree in Computer Science or similar degree required
Call-to-ActionReady to take your data engineering career to the next level? Apply today to join an innovative team where your expertise will drive meaningful business impact!
Keywords: Data Engineer | Databricks | Spark | SQL | Python | ETL | Data Pipeline | GitHub | Big Data | Analytics | Mountain View | Hybrid",https://www.linkedin.com/jobs/view/4259558875,5
93,Fracsys Inc,https://www.linkedin.com/company/fracsys-inc/life,Data Engineer AWS - US Citizens,https://www.linkedin.com/jobs/view/4257181750,4257181750,"Leesburg, VA",On-site,,2025-07-02 15:09:55,False,,"Fracsys Inc is hiring a AWS Data Engineer position. The ideal candidate must have at least 8+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects. This positions requires US Citizenship due to Clearance requirements.

Responsibilities

Data extraction, Data cleaning, Data Loading, Statistical Data Analysis, Exploratory Data Analysis, Data Wrangling,Write complex SQL queries and stored procedures to extract, manipulate, and analyze data from relational databasesConduct diagnostic, troubleshooting steps and present conclusions to a varied audiencecomfortable slinging some code to solve a problemBuild Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of dataDesign and implement data patterns on cloud using cloud Data warehouse and data virtualization toolsEvaluate new tools and technologies for target state on cloudLearn and implement new ETL, BI tools and data warehousing technologies


Qualifications

Senior Data professional with over 8 years of expertise in Data Engineering, Data Analysis, and Data Science.Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworksExperience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Serviceextensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligenceExperience building self-service data consumption patterns and knowledge of cloud-based data Lake platformsExperience wrangling data (structured and unstructured), in-depth knowledge of database architectureExperience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, RedshiftExperience in snowflake is an added bonus.Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, SparkAbility to drive, contribute to, and communicate solutions to technical product challengesAbility to roll-up your sleeves and work in process or resource gaps and fill it in yourselfExcellent written and oral communication skills in English


Education

Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.",https://fracsys-inc.careerplug.com/j/02olg13,6
41,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4260844551,4260844551,"St Paul, MN",On-site,$224K/yr - $279.4K/yr,2025-07-02 14:08:43,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, build, and launch data pipelines to move data across systems and build the next generation of data tools that generate business insights for a product. Analyze user needs and software requirements to determine workability and to offer support for end users on data usage. Design, architect, and develop software and data solutions that help product and business teams make data-driven decisions. Rethink and influence strategy and roadmap for building efficient data solutions and scalable data warehouse plans. Design, develop, test, and launch new data models and processes into production, and provide support. Leverage homegrown extract, transform, and load (ETL) framework as well as off-the-shelf ETL tools, as appropriate. Interface closely with data infrastructure, product, and engineering teams to build and extend cross platform ETL and reports generation framework. Identify data infrastructure issues and drive to resolution. Telecommuting is permitted anywhere in the U.S. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign equivalent) in Computer Science, Data Analytics, Business Analytics, or a related field and 24 months of experience in the job offered or in a related occupation. Experience must include 24 of experience in the following skills and technologies: Data ETL (Extract, Transform, Load) design, implementation, and maintenance on a large scaleProgramming in Python, Perl, Java, or PHPWriting SQL statementsAnalyzing large volumes of data to provide data driven insights, gaps, and inconsistenciesData warehousing architecture and plansInformatica, Talend, Pentaho, dimensional data modeling, or schema designMap Reduce or MPP systemHadoop, HBase, or Hive
Public Compensation

$224,028/year to $279,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/a93ab86aa78741bb9475d152496b06aatjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
91,Rackspace Technology,https://www.linkedin.com/company/rackspace-technology/life,Machine Learning Architect (AWS),https://www.linkedin.com/jobs/view/4259518289,4259518289,"Irvine, CA",Remote,$153K/yr - $244.7K/yr,2025-07-02 15:12:03,True,13 applicants,"We are expanding our team of motivated technologists with a proven track record of delivering results in technology consulting. We are looking for a Machine Learning Architect with experience in cloud (AWS preferred) who is passionate about helping customers build AI/ML solutions at scale. Being an experienced technologist with technical depth and breadth, aided with strong interpersonal skills, you will work directly with customers as part of a delivery team, helping to enable innovation by creating state of the art Machine Learning solutions that align to business goals. 

This role includes responsibilities both as a Professional Services Machine Learning Architect and as a hands-on Machine Learning engineer on customer engagements. 

The qualified Machine Learning Architect will have demonstrated the ability to think strategically about businesses, create technical definitions around customer objectives in complex situations, develop solution strategies, motivate & mobilize resources, and deliver results. The ability to connect technology with measurable business value is a critical component to be successful in this role. We seek team members who are self-motivated, driven, collaborative, passionate about machine learning, and want to have a direct positive impact on our customer's business. Strong communication skills and emotional intelligence are also needed to help develop a team that works with you. 

Work Location: Remote 

Key Responsibilities:

Design machine learning solutions and execute machine learning projects end to end from proof-of-concept stage to deployment in production using cloud native technologies and state of the art machine learning models. Be technically focused but work directly with the business representatives/customers to understand the requirements driving the need for a solution to be developed. Be responsible for all phases of the project from problem definition, data annotation, model development, model deployment to end user documentation/training. Design the architecture of ML solutions on cloud platforms (AWS, Azure, GCP) including MLOPs. Stay abreast of the latest developments. Read the latest published machine learning research and adapt the models to solve customer’s problems. Establish credibility by demonstrating technical excellence and delivering value through solutions you build. Develop strong relationships with our customers. 


Qualifications:

Masters with 10+ years of experience or PhD with 6+ years of experience in Machine Learning, Natural Language Processing (NLP) and Deep Learning. Minimum 5+ years of experience architecting and building Machine Learning solutions. Minimum 5+ years of experience with cloud platforms (AWS, GCP, Azure). Experience building ML models and strong knowledge of ML techniques is required. Experience with hugging face, TensorFlow/pytorch, transformer architectures, prompt engineering, agentic systems, LLMs. Strong coding experience in Python and architectural patterns like microservices. Solid understanding of agile methodologies and experience in planning machine learning projects from inception to production deployment. Strong problem-solving skills and the ability to lead a team on “what’s next” when encountering a technical issue in a machine learning project. Excellent communication and presentation skills, with the ability to explain complex technical concepts to both technical and non-technical audiences. 


Travel:

As per business requirements


#rackspace

The following information is required by pay transparency legislation in the following states: CA, CO, HI, NY and WA. This information applies only to individuals working in these states. 

The anticipated starting pay range for Colorado is: $153,000 - $204,000

The anticipated starting pay range for Hawaii and New York (not including NYC) is: $167,400 - $223,200

The anticipated starting pay range for California, New York City and Washington is: $183,500 - $244,700

Based on eligibility, compensation for the role may include variable compensation in the form of bonus, commissions, or other discretionary payments.

These discretionary payments are based on company and/or individual performance, and may change at any time.

Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, licenses and certifications, and specific work location. 

Information on benefits offered is here.

",https://www.linkedin.com/jobs/view/4259518289,6
90,Akumin®,https://www.linkedin.com/company/akumin/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4260594769,4260594769,"Dallas, TX",Remote,,2025-07-02 08:10:25,False,,"The Senior Data Engineer will play a crucial role in building out the company’s enterprise data platform to support analytics and AI. As part of the Enterprise Data team, you will be tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation. The role will be responsible for the infrastructure and operations of the organizations data storage solutions, third-party integrations data provisioning services and overall ETL/ELT code-based solutions and tooling that process data.

Specific duties include, but are not limited to:

Development of complex SQL queries for data extraction, manipulation, and reporting and transformation technologiesDesign and implement robust ETL/ELT pipelines using custom-tooling (Python/Google) and off-the shelf tooling with focus on monitoring, supportability, and resource stewardship. Contribute to and leverage coding standards and best practices to ensure efficient and re-usable services and components. Architect, implement and deploy new data models and data processes in productionBuilds data pipelines which acquire, cleanse, transform and publish data from a wide variety of sources. Assembles large, complex data sets which meets functional and non-functional business requirements. Partners with data asset managers, architects, and development leads to ensure technical solution provides data which is fit for use and in line with architecture blueprints. Identify, document, design, and implement internal process improvements. Other duties as assigned by management. 

Position Requirements:

Bachelor’s degree required in Data Science, Computer Science or MIS, Mathematics, Engineering, or related field. 5+ years of prior experience in Data Management / ETL / ELT / Data WarehousingExperience in writing Data Quality routines for cleansing of data and capturing confidence score and master data management (MDM). Hands-on experience with designing and implementing data pipelines and ELT/ETL processes (ex. Fivetran, DBT)Hands-on experience with cloud platforms (ex.GCP, AWS, Azure) and related services (ex. BigQuery, S3, Snowflake, etc.)Strong understanding of data modeling, data integration, and data governance priciples (ex. DBT)Experience working in a highly regulated domain (ex. Healthcare, Banking, etc. Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)Experience using scripting languages such as JavaScript or PythonExperience with agile delivery methodologiesStrong organizational, administrative, and analytical skills required. Travel may be required between 5 to 10%

Preferred Requirements: .

Master's degree in Data Science, Computer Science or MIS, Mathematics, Engineering, or related fieldDBT CertificationData Modeling experience preferred. Experience working with Clinical Data and Regulatory Governance in a Healthcare settingExperience Healthcare data models, datasets, and source systems (e.g. EHR, claims, DICOM images, labs, etc.)Experience with healthcare reference data (ICD, CPT etc.)Experience with big data technologies (ex.Kafka, Hadoop, Spark) and containerization (ex. Docker, Kubernates)Experience with FHIR and FHIR store formats and data repositories. Hands-on design and configuration experience with cloud services Google (related to data storage and processing)Data Governance Principles and enterprise frameworks (Warehousing, Data-as-a-Product, MESH, MDM). Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations

Physical Requirements:

Standard office environment.

More than 50% of the time:

Sit, stand, and walk. Repetitive movement of hands, arms and legs. See, speak and hear to be able to communicate with patients. 

Less than 50% of the time:

Stoop, kneel or crawl. Climb and balance. Carry and lift 10-20 pounds. 

Residents living in CA, HI, WA, CO, IL, NY, and Jersey City, NJ, click here  to view pay range information.

Akumin Operating Corp. and its divisions are an equal opportunity employer and we believe in strength through diversity. All qualified applicants will receive consideration for employment without regard to, among other things, age, race, religion, color, national origin, sex, sexual orientation, gender identity & expression, status as a protected veteran, or disability.",https://akumincorp.wd5.myworkdayjobs.com/akumincareers/job/Dallas-TX/Senior-Data-Engineer_2025-4865?source=LinkedIn,6
79,MANTECH,https://www.linkedin.com/company/mantech/life,Data Engineer,https://www.linkedin.com/jobs/view/4257144246,4257144246,"Ashburn, VA",On-site,,2025-07-02 08:45:58,False,,"ManTech seeks a motivated, career- and customer-oriented Data Engineer SME to join our innovative team in Ashburn, VA. This is a hybrid position with 2 days onsite and 3 days remote.

Each day U.S. Customs and Border Protection (CBP) oversees the massive flow of people, capital, and products that enter and depart the United States via air, land, sea, and cyberspace. The volume and complexity of both physical and virtual border crossings require the application of “big data” solutions to promote efficient trade and travel. Further, effective solutions help CBP ensure the legal, safe, and secure movement of people, capital, and products. In response to this challenge, ManTech, as a trusted mission partner of CBP, seeks capable, qualified, and versatile Data Engineer subject matter experts (SMEs) to facilitate data-driven decision making in response to national security threats.

Responsibilities Include But Are Not Limited To

Responsible for data analysis of large database tables to understand the data structures, definitions, and patterns which will be used to support various predictive models.Work closely with the client to assist in managing data needs and supporting collection of data needed for various operational needs.Data analysis, problem solving, investigation and creative thinking with massive amount of data supporting variety of operational scenarios and responsible for source code control using GitLab.Implement cloud techniques and workflows (on-prem to cloud platforms).Respond to data queries/analysis requests from various groups within the organization. Create and publish regularly scheduled and/or ad hoc reports as needed.Research and document data definitions for all subject areas and primary datasets supporting the core business applications.Demonstrate a strong practical understanding of application-relevant cargo and passenger data and databases used to support analytic application development, functionality, and targeting end user (officer) operations. 

Minimum Qualifications

HS Diploma/GED and 20+ years or AS/AA and 18+ years or BS/BA and 12+ years or MS/MA/MBA and 9+ years or PhD/Doctorate and 7+ yearsExperience in application development/full life cycle on Data Warehouse engagements and at least 5 years’ experience in large (80TB+) and complex data warehousing architecture, design and implementation/migrationExperience with one or more relational database systems such as Oracle, MySQL, Postgres, SQL server, etc. and experience in Extract-Transform-Load (ETL) development and knowledge of ETL concepts, tools, and data structuresExperience with cloud platforms like Amazon Web Services (AWS), Microsoft Azure, etc. and migrating customers/projects to the cloud and experience with one of the modern Cloud DW like Redshift, Databricks or BigQuery.Experience working in Unix/Linux environment and with shell scripting and scheduling CRON jobsExperience with Database query tuning and other performance enhancement methodology is a plus.Experience with NoSQL database (MongoDB or DynamoDB or DocumentDB). 

Preferred Qualifications

Knowledge of Continuous Integration & Continuous Development tools (CI/CD).Ability to multitask efficiently and progressively and work comfortably in an ever-changing data environment.Ability to work well in a team environment as well as independently.Have excellent verbal/written communication and problem-solving skills and ability to communicate information to a variety of groups at different technical skill levels.Experience with relational databases and knowledge of query tools and/or BI tools like Power BI or Business Objects (BO) and data analysis tools.Experience with the Hadoop ecosystem, including HDFS, YARN, Hive, Pig, and batch-oriented and streaming distributed processing methods such as Spark, Kafka, or Storm.Experience with Atlassian suite of tools such as Jira and Confluence. 

Clearance Requirements

Must be a U.S. citizen with the ability to obtain DHS CBP suitability. 

Physical Requirements

The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, or to communicate with co-workers, management, and customers, which may involve delivering presentations.",https://click.appcast.io/t/4drvDBnG7A4XBT0ofgTQ3FhnEji2nvrONsgGbh2aNDYlvuSa-CwJdE5Am46bZNNd?source=LinkedIn,6
5,Capax RM,https://www.linkedin.com/company/capaxrm/life,Power BI Developer ,https://www.linkedin.com/jobs/view/4259181010,4259181010,United States,Remote,$90K/yr - $115K/yr,2025-07-02 11:58:01,True,over 100 applicants,"Job Summary:We are looking for an experienced Power BI Developer with strong expertise in building Paginated Reports, integrating REST APIs as data sources, and working with modern web technologies. This role requires deep technical skills in Power BI, strong understanding of API authentication mechanisms (like OAuth), and hands-on experience with tools like Postman and JSON. You'll collaborate with business stakeholders and data engineers to deliver actionable insights and scalable BI solutions.Key Responsibilities:Design, develop, and deploy Power BI Paginated Reports for operational and regulatory reporting.Connect to and integrate REST APIs as Power BI data sources, including handling pagination, authentication, and transformation of API responses.Utilize Postman to test and explore APIs, document request/response cycles, and troubleshoot connectivity issues.Work with JSON data formats for transforming and loading semi-structured data into Power BI models.Configure and manage OAuth 2.0 flows to securely access data from authenticated APIs.Develop robust data models and DAX calculations to support dynamic and interactive dashboards.Collaborate with cross-functional teams to understand business requirements and translate them into data solutions.Monitor, troubleshoot, and optimize report performance and data refresh schedules.Maintain documentation for report logic, data sources, and security configurations.Required Qualifications:Bachelor’s degree in Computer Science, Information Systems, or a related field.3+ years of experience with Power BI, including strong hands-on experience with Paginated Reports (Report Builder / SSRS).Proven experience integrating REST APIs with Power BI, including authentication and data transformation.Skilled in using Postman for API testing, debugging, and workflow validation.Strong understanding of JSON, including nested structures and schema interpretation.Solid grasp of OAuth 2.0 flows, including client credentials, authorization code, and token management.Proficient in DAX and Power Query (M).Strong SQL skills and experience working with relational databases.Excellent problem-solving and analytical skills.",https://www.linkedin.com/jobs/view/4259181010,6
40,Adaptive Technology Insights,https://www.linkedin.com/company/atiglobalservices/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4259508188,4259508188,"St Louis, MO",On-site,,2025-07-02 14:39:08,True,over 100 applicants,"About the Role:We are looking for an experienced Data Engineer with strong expertise in Google Cloud Platform (GCP) and BigQuery to join our growing data team. You will be responsible for designing, building, and optimizing data pipelines and architecture to support analytics, reporting, and data-driven decision-making across the organization.
Key Responsibilities:Design and implement scalable, reliable, and high-performance data pipelines on GCP.Build and maintain data warehouse solutions using BigQuery.Advanced skills in SQL, Python, Kafka and possibly Scala or Java.Develop ETL/ELT processes to ingest structured and unstructured data from multiple sources.Ability to make architectural decisions, optimize performance, and manage data security and governance.Collaborate with data analysts, data scientists, and business stakeholders to define data needs.Optimize query performance and manage data partitioning and clustering in BigQuery.Ensure data quality, governance, and security best practices.Monitor and troubleshoot pipeline performance and data flow issues.Work with tools such as Dataflow, Cloud Composer (Apache Airflow), Pub/Sub, and Cloud Storage.
Required Qualifications:7+ years of experience in Data Engineering or a related field.Strong hands-on experience with Google Cloud Platform (GCP) services, especially BigQuery and Kafka.Proficiency in SQL and at least one programming language like Python.Experience with data modeling, ETL/ELT processes, and building data pipelines.Experience with GCP tools like Cloud Dataflow, Pub/Sub, Cloud Composer, and Cloud Storage.Understanding of CI/CD practices and version control systems like Git.Strong problem-solving and communication skills.
Preferred Qualifications:GCP Professional Data Engineer Certification.Experience with data lakes, streaming data, and real-time analytics.Exposure to other cloud platforms (AWS, Azure) is a plus.",https://www.linkedin.com/jobs/view/4259508188,6
42,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer (Skillbridge Intern),https://www.linkedin.com/jobs/view/4260846376,4260846376,"Augusta, GA",On-site,,2025-07-02 14:08:33,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Huntington Ingalls Industries.

Enlighten, honored as a Top Workplace from the Baltimore Sun, is a leader in big data solution development and deployment, with expertise in cloud-based services, software and systems engineering, cyber capabilities, and data science. Enlighten provides continued innovation and proactivity in meeting our customers’ greatest challenges.

We recognize that the most effective environment for your projects doesn’t always look the same. Our hybrid work approach ensures that you can make lasting relationships with your team and collaborate in-person to get the job done—while having the flexibility to work from home when needed to achieve focused results.

Job Description

This is an UNPAID internship through the DoD SkillBridge Program for transitioning active-duty US military personnel. DoD SkillBridge Internships are available to help transitioning active-duty military personnel gain real-world experience in the work force sometime during their final 180 days of active-duty service. The intern will actively train on meaningful projects and work closely with a mentor and with senior company leadership. Enlighten Internship programs are focused on placing transitioning military into internships that require KSAs, Education & Military Training similar to their current or previous military jobs; positions that could easily transition over to a full-time regular and permanent job with Enlighten.

Essential Job Responsibilities

The SkillBridge intern will train as a Data Engineer with Enlighten, reporting to a designated Enlighten Supervisor, with the goal of learning Enlighten’s approach to design, build, and optimize Big Data Platform (BDP) systems for data collection, storage, access, and analytics at scale. The intern will be assigned special projects as needed.

Desired End State (3-4 month target)

At the end of four months, the intern will be able to understand basic data engineer concepts. Data engineers, as part of enterprise data analytics teams, manage, optimize, oversee, and monitor data retrieval, storage, and distribution throughout the BDP enclaves. The intern will develop a good understanding of all critical tasks to successfully conduct data engineer objectives: Data acquisition Develop data set processes (Extract, transform, load (ETL)) Use programming language and tools (Java, NiFi, maven, etc) Identify ways to improve data reliability, efficiency, and quality Prepare data for predictive and prescriptive modeling Use data to discover tasks that can be automated Enlighten will benefit from the military background of the SkillBridge intern and considers the SkillBridge internship an overall positive experience. 

Assumptions/Restrictions

If candidate is selected and approved, SkillBridge Intern can travel in conjunction with this internship; Enlighten will fund all travel costs. SkillBridge Intern will be available during core hours for critical meetings and training. 

Training Plan

Phase 1 Basics

Week 1: In processing, Introductions/office familiarization Week 2: Intro to the Big Data Platform (BDP) technologies. Intro to BDP analytics and applications Week 3: Intro to Army and Air Force Cyber data ETL process 

Phase 2 Parser Overview

Week 4: Data Acquisition, Taxonomy/Schema. Review existing parsers and deployment flows; understand existing transforms available for use in parsers. Weeks 5-6: BDP technologies - Kafka and Nifi workflows; Establish a development environment for BDP parser development 

Phase 3 Parser Development, Deployment, and Testing

Weeks 7-8: Modify Kafka and Nifi workflows; modify existing parser schema and transforms to alter ingest pipeline. Week 9: Understand how to access logs to identify errors; understand common parser development errors; troubleshoot errors. Weeks 10-end: Implement data parsing best practices, building, deploying, and troubleshooting workflows. 

Additional Goals

Project management

Minimum Qualifications

Active-Duty Military ONLY within final 180 days of active-duty service. Minimum of 5 years relevant data engineering (including experience with Python or Java) experience with Bachelors in related field; 3 years relevant experience with Masters in related field; or High School Diploma or equivalent and 9 years relevant experience. Additional experience dependent on Program of Instruction. 

Physical Requirements

Physical Requirements will vary and is dependent on the Program of Instruction.

We have many more additional great benefits/perks that you can find on our website at www.eitccorp.com [eitccorp.com].

If you have questions about this posting, please contact support@lensa.com

",https://lensa.com/cgw/e4c646d1a1754b85a82d9ebe4e054d05tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
2,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4260848349,4260848349,"Salem, OR",On-site,$224K/yr - $279.4K/yr,2025-07-02 14:08:26,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, build, and launch data pipelines to move data across systems and build the next generation of data tools that generate business insights for a product. Analyze user needs and software requirements to determine workability and to offer support for end users on data usage. Design, architect, and develop software and data solutions that help product and business teams make data-driven decisions. Rethink and influence strategy and roadmap for building efficient data solutions and scalable data warehouse plans. Design, develop, test, and launch new data models and processes into production, and provide support. Leverage homegrown extract, transform, and load (ETL) framework as well as off-the-shelf ETL tools, as appropriate. Interface closely with data infrastructure, product, and engineering teams to build and extend cross platform ETL and reports generation framework. Identify data infrastructure issues and drive to resolution. Telecommuting is permitted anywhere in the U.S. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign equivalent) in Computer Science, Data Analytics, Business Analytics, or a related field and 24 months of experience in the job offered or in a related occupation. Experience must include 24 of experience in the following skills and technologies: Data ETL (Extract, Transform, Load) design, implementation, and maintenance on a large scaleProgramming in Python, Perl, Java, or PHPWriting SQL statementsAnalyzing large volumes of data to provide data driven insights, gaps, and inconsistenciesData warehousing architecture and plansInformatica, Talend, Pentaho, dimensional data modeling, or schema designMap Reduce or MPP systemHadoop, HBase, or Hive
Public Compensation

$224,028/year to $279,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/e7c619fe439d41b08d53ea1d73f7569ctjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
32,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer I - AWS Cloud/ETL,https://www.linkedin.com/jobs/view/4260846465,4260846465,"Atlanta, GA",On-site,$106.3K/yr - $175.4K/yr,2025-07-02 14:08:26,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Travelers Insurance.

Who Are We?

Taking care of our customers, our communities and each other. That’s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.

Job Category

Technology

Compensation Overview

The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.

Salary Range

$106,300.00 - $175,400.00

Target Openings

1

What Is the Opportunity?

Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights.

What Will You Do?

Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions. Design data solutions. Analyze sources to determine value and recommend data to include in analytical processes. Incorporate core data management competencies including data governance, data security and data quality. Collaborate within and across teams to support delivery and educate end users on data products/analytic environment. Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate. Test data movement, transformation code, and data components. Perform other duties as assigned. 

What Will Our Ideal Candidate Have?

Bachelor’s Degree in STEM related field or equivalent Six years of related experience Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions. Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on. Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems. Strong verbal and written communication skills with the ability to interact with team members and business partners. Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities. Experience with some of the following tools & platforms (or similar): Ab Initio, Autosys, AWS (s3, Lambda, Kinesis, API Gateway, IAM, Glue, SNS, SQS, EventBridge, EKS, VPC, Step Functions, ECS/EKS, DynamoDB, etc.), Databricks, Python, Kafka, dbt, Terraform, Snowflake, SQL, Jenkins, Github,, Talend, Alation, Hashicorp Vault / AWS Secrets Manager, Docker, BlackDuck, SonarQube Knowledge and experience with the some of the following concepts: Real-time & Batch Data Processing, Workload Orchestration, Cloud, Datalakes, Data Security, Serverless, Testing/Test Automation (Unit, Integration, Performance, etc.), DevOps, Logging, Monitoring, and Alerting, Encryption / Decryption, Data Masking, Cost & Performance Optimization. 

What is a Must Have?

Bachelor’s degree or equivalent training with data tools, techniques, and manipulation. Four years of data engineering or equivalent experience. 

What Is in It for You?

Health Insurance : Employees and their eligible family members – including spouses, domestic partners, and children – are eligible for coverage from the first day of employment. Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers. Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays. Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs. Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice. 

Employment Practices

Travelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences.

In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email (4-ESU@travelers.com) so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.

To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/ .

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/9c2eb74d105c43919fbe80ceefa26e7ftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
30,Burtch Works,https://www.linkedin.com/company/burtch-works/life,Data Engineer,https://www.linkedin.com/jobs/view/4260596903,4260596903,"Phoenix, AZ",,$120K/yr - $140K/yr,2025-07-02 08:28:11,False,,"Job Title:  Data Engineer

Location: Remote (U.S.)

About The Company

We are a fast-growing telecommunications company committed to empowering customers through accessible, affordable, and data-informed service experiences. Our team is focused on building scalable and intelligent data infrastructure that supports marketing, operations, and customer engagement efforts across all business lines.

Job Summary

We’re seeking a Data Engineer with strong analytical instincts and hands-on experience in Azure, Apache Spark, PySpark, and data lake architecture . In this role, you’ll help optimize our data workflows for performance, cost-efficiency, and scalability. You’ll collaborate with analysts and data scientists to design robust pipelines that power exploratory analytics, advanced attribution models, and operational dashboards. If you enjoy solving real-world data problems and have a passion for performance tuning and automation, we’d love to talk.

Key Responsibilities

 Collaborate with data scientists and analysts to translate data needs into scalable, production-ready pipelines.  Design and optimize ETL processes across Azure Synapse, Data Factory, and ADLS Gen2 environments.  Use PySpark, SQL, and Delta Lake to ingest, transform, and serve structured and semi-structured data.  Implement incremental loading strategies to reduce compute time and lower cloud costs.  Secure and monitor data ingestion from third-party systems (e.g., SFTP, APIs), building logging and alerting systems using Azure Logic Apps.  Architect data structures for reuse, version control, and traceability—supporting data rollback when needed.  Design and document data flows that protect PII and comply with privacy standards .  Independently integrate and stage new data sources, including high-volume API connections (e.g., NICE call center data).  Collaborate on cost optimization initiatives , identifying performance bottlenecks and query inefficiencies.  Build solutions that accelerate marketing analytics, decision science, and campaign performance tracking. 

What You’ll Bring

Education: Bachelor’s degree in Computer Science, Engineering, or a related field (Advanced degree a plus). Experience: 5+ years of experience building and maintaining large-scale data pipelines.  Expertise with Apache Spark , PySpark , SQL , and Delta Tables in cloud environments.  Proven ability to optimize runtime, storage, and cloud cost—especially in Azure Synapse . Technical Skills: Strong Python and SQL development for data transformation.  Azure services: Synapse, Data Factory, Storage Accounts, Key Vault, Logic Apps.  Familiarity with Data Lake best practices, including schema design, partitioning, and performance tuning.  Experience designing logging frameworks, monitoring systems, and alert workflows.  Hands-on with API ingestion , SFTP automation, and secured external data integration.  Working knowledge of Power BI is a plus. Soft Skills: Proactive mindset with a bias for automation and performance.  Ability to collaborate across engineering, data science, and marketing teams.  Excellent documentation and communication skills.  Strong problem-solving capabilities, with an ability to learn new tools and techniques quickly. 
Preferred Qualifications

 Experience implementing version-controlled data architecture using Delta Tables.  Familiarity with Splink or probabilistic matching techniques for customer attribution.  Prior experience reducing cloud infrastructure costs through intelligent pipeline redesign.  Understanding of data governance, access control, and PII compliance best practices. 

Why Join Us?

Be part of a collaborative, impact-driven team working on high-visibility initiatives. You’ll get to:

 Tackle real-world problems in attribution modeling, marketing analytics, and campaign intelligence.  Use the latest in Azure, PySpark, and data lake technologies.  Design systems that scale and perform—while reducing operational costs.  Have the freedom to research, prototype, and deploy new solutions independently. 

If you're a data engineer who’s passionate about performance, ownership, and continuous improvement—this is the role for you.",https://jobs.burtchworks.com/s/?jobId=a09Vv000002PHvmIAG,6
81,PwC,https://www.linkedin.com/company/pwc/life,Data Engineer - Senior Manager,https://www.linkedin.com/jobs/view/4259199246,4259199246,"Texas, United States",Hybrid,$124K/yr - $280K/yr,2025-07-02 13:53:26,False,,"Specialty/Competency: Data, Analytics & AI

Industry/Sector: Not Applicable

Time Type: Full time

At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth. In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.

Growing as a strategic advisor, you leverage your influence, expertise, and network to deliver quality results. You motivate and coach others, coming together to solve complex problems. As you increase in autonomy, you apply sound judgment, recognising when to take action and when to escalate. You are expected to solve through complexity, ask thoughtful questions, and clearly communicate how things fit together. Your ability to develop and sustain high performing, diverse, and inclusive teams, and your commitment to excellence, contributes to the success of our Firm.

Skills

Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:

Craft and convey clear, impactful and engaging messages that tell a holistic story.Apply systems thinking to identify underlying problems and/or opportunities.Validate outcomes with clients, share alternative perspectives, and act on client feedback.Direct the team through complexity, demonstrating composure through ambiguous, challenging and uncertain situations.Deepen and evolve your expertise with a focus on staying relevant.Initiate open and honest coaching conversations at all levels.Make difficult decisions and take action to resolve issues hindering team effectiveness.Model and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.

As part of the Data and Analytics Engineering team you design and implement thorough data architecture strategies that meet current and future business needs. As a Senior Manager you lead large projects, innovate processes, and maintain operational excellence while interacting with clients at a strategic level to drive project success. You also be responsible for developing and documenting data models, data flow diagrams, and data architecture guidelines, maintaining compliance with data governance and data security policies, and collaborating with business stakeholders to translate their data requirements into technical solutions.

Responsibilities

 Design and implement thorough data architecture strategies Lead large-scale projects and innovate processes Maintain operational excellence and client interactions Develop and document data models and data flow diagrams Adhere to data governance and security policies Collaborate with business stakeholders to translate data requirements into technical solutions Drive project success through strategic advising and problem-solving Foster a diverse and inclusive team environment

What You Must Have

 Bachelor's Degree 8 years of experience

What Sets You Apart

 Certification in Cloud Platforms [e.g., AWS Solutions Architect, AWS Data Engineer, Google Professional Cloud Architect, GCP Data Engineer Microsoft Azure Solutions Architect, Azure Data Engineer Associate, Snowflake Core, Snowflake Architect, Databricks Data Engineer Associate] is a plus Designing and implementing thorough data architecture strategies Developing and documenting data models and data flow diagrams Maintaining data architecture compliance with governance and security policies Collaborating with stakeholders to translate data requirements into solutions Evaluating and recommending new data technologies and tools Leading data strategy engagements providing thought leadership Developing leading practices for Data Engineering, Data Science, and Data Governance Architecting and implementing cloud-based solutions meeting industry standards

Learn more about how we work: https://pwc.to/how-we-work

PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.

As PwC is an equal opportunity employer, all qualified applicants will receive consideration for employment at PwC without regard to race; color; religion; national origin; sex (including pregnancy, sexual orientation, and gender identity); age; disability; genetic information (including family medical history); veteran, marital, or citizenship status; or, any other status protected by law. 

For only those qualified applicants that are impacted by the Los Angeles County Fair Chance Ordinance for Employers, the Los Angeles' Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, San Diego County Fair Chance Ordinance, and the California Fair Chance Act, where applicable, arrest or conviction records will be considered for Employment in accordance with these laws. At PwC, we recognize that conviction records may have a direct, adverse, and negative relationship to responsibilities such as accessing sensitive company or customer information, handling proprietary assets, or collaborating closely with team members. We evaluate these factors thoughtfully to establish a secure and trusted workplace for all.

Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines

The salary range for this position is: $124,000 - $280,000, plus individuals may be eligible for an annual discretionary bonus. For roles that are based in Maryland, this is the listed salary range for this position. Actual compensation within the range will be dependent upon the individual's skills, experience, qualifications and location, and applicable employment laws. PwC offers a wide range of benefits, including medical, dental, vision, 401k, holiday pay, vacation, personal and family sick leave, and more. To view our benefits at a glance, please visit the following link: https://pwc.to/benefits-at-a-glance

",https://ad.doubleclick.net/ddm/clk/438405794;254302458;m?https://jobs.us.pwc.com/job/-/-/932/81216676768?utm_source=linkedin.com&utm_campaign=core_media&utm_medium=social_media&utm_content=job_posting&ss=paid,6
94,Lensa,https://www.linkedin.com/company/lensa/life,Associate Data Engineer,https://www.linkedin.com/jobs/view/4260846567,4260846567,United States,Remote,$40K/yr - $45K/yr,2025-07-02 14:08:19,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Lumerate.

Lumerate is growing rapidly, and we're searching for a UK-based Associate Data Engineer to join our team for the journey!

Who is Lumerate?

Lumerate is a Toronto-based SaaS company that has built game-changing technology to help sales teams accelerate revenue growth. We help our customers achieve the full picture of their industries. We also strive to achieve our own personal full pictures from a career fulfillment and learning perspective. We're in the business of gathering intelligence about industries and delivering it to the right people within those industries through innovative software interfaces. Our vision is to be the world's most useful and trusted source of information for professionals seeking to understand what's happening in their industry.

Our mission: To deliver industry awareness to an ever-increasing number of people, in whatever way helps them to make the most informed decisions, take the most immediate action and be the most awesome at their unique jobs.

What The Role Looks Like

Working closely with the Data Engineer, you will work on a variety of tasks and projects mainly to support the appropriate data team. Some of these projects may include: Automation of manual tasks; extracting data from various sources such as databases and web scraping; evaluating and setting up new data pipelines; data point retrieval experiments and analysis; working with LLMs for inclusion in current and future workflows. This role will really suit someone who enjoys building processes. Due to the data you will be working with, a background in life sciences would be helpful. Python will be the main programming language used in this role.

What You'll Be Responsible For

Collaborating effectively with data team members and business stakeholders to ensure alignment and clear communication. Supporting definition of detailed requirements for projects with appropriate stakeholders. Supporting solution design based on requirements. Developing and maintaining processes primarily using Python. Ensuring appropriate testing has been conducted. Creating detailed documentation to ensure processes can be easily followed (e.g. User guides) Participating in regular meetings. 

Who will be successful in this role?‍

Someone who holds a minimum of a Bachelor's degree in a scientific or technological field, or possesses equivalent industry experience. Experience with at least Python and associated libraries such as Pandas, requests, and selenium. Experience with spreadsheets and databases. A person with strong analytical and problem-solving skills that genuinely enjoys tackling complex challenges. Someone who demonstrates an ability to quickly learn complex concepts and understand systems. Someone who is able to communicate complex data/ processes to a non technical audience. A person who is able to work well remotely, previous experience of this would be preferable. Someone who is detail oriented. You'll see this request to add the name of the establishment you find at these coordinates in your cover letter 43.66484814697388, -79.45482568477735 

What We'd Love To See In Your Work History

Work experience at a software/ technology company AND/OR background in a life sciences data related role. Experience in automating manual processes. Experience using APIs, LLMs, scraping web content and/or version control software would be helpful. Experience writing Standard Operating Procedures (SOPs) and/or user/resource documentation. Experience managing multiple projects or competing priorities simultaneously. 

Reasons You Might Want To Apply

You'll work with a team who encourages continuous learning and supports your growth. You'll gain exposure to many different projects and work collaboratively with colleagues. You'll build foundational skills essential for a successful career path in Data Engineering. 

Why Lumerate? Fancy perks etc.

Grow with an experienced team with skills in machine learning, development, business and organizational culture Earn yourself some equity (employee options make up 20% of the value of the company at all times) Join us for our annual all-company retreat when we reach our goals (past destinations include Bermuda, Iceland, Costa Rica, Portugal and Dominican Republic) 4 weeks paid annual leave + all local Bank Holidays (paid) Earn additional paid vacation days with continued learning ($1000 annual stipend for courses and classes) Take part in our Employee Giving Program (you choose the causes and the company provides the funds) Basic and extended health and dental benefits Paid and topped-up maternal and parental leave 

Salary: 40,000-45,000 GBP

Start Date: July 2025 (flexible based on the successful candidate)

Location: This is a remote position. Our new Associate Data Engineer can live and work anywhere in the United Kingdom, provided they are legally entitled to do so.

Already picturing your first day as Lumerate's first Associate Data Engineer? Apply today by submitting your cover letter and resume. While we thank all candidates for their interest. Please note that applications without a cover letter will not be considered.

At Lumerate we celebrate diverse backgrounds, experiences, and perspectives. We are passionate about fostering an inclusive environment where everyone feels empowered to bring their authentic selves to work. Lumerate is an equal-opportunity employer, and we are committed to working with applicants requesting accommodation at any stage of the hiring process.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/abed0aea90b2402ba4785dd61f37625atjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
54,PwC,https://www.linkedin.com/company/pwc/life,Data Engineer- Manager,https://www.linkedin.com/jobs/view/4259198292,4259198292,"California, United States",Hybrid,$99K/yr - $232K/yr,2025-07-02 13:53:26,False,,"Specialty/Competency: Data, Analytics & AI

Industry/Sector: EUR X-Sector

Time Type: Full time

Travel Requirements: Up to 80%

At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth. In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.

Enhancing your leadership style, you motivate, develop and inspire others to deliver quality. You are responsible for coaching, leveraging team member’s unique strengths, and managing performance to deliver on client expectations. With your growing knowledge of how business works, you play an important role in identifying opportunities that contribute to the success of our Firm. You are expected to lead with integrity and authenticity, articulating our purpose and values in a meaningful way. You embrace technology and innovation to enhance your delivery and encourage others to do the same.

Skills

Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:

Analyse and identify the linkages and interactions between the component parts of an entire system.Take ownership of projects, ensuring their successful planning, budgeting, execution, and completion.Partner with team leadership to ensure collective ownership of quality, timelines, and deliverables.Develop skills outside your comfort zone, and encourage others to do the same.Effectively mentor others.Use the review of work as an opportunity to deepen the expertise of team members.Address conflicts or issues, engaging in difficult conversations with clients, team members and other stakeholders, escalating where appropriate.Uphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.

Minimum Degree Required

Bachelor's Degree

Required Field(s) Of Study

Management Information Systems, Computer and Information Science, Systems Engineering,Electrical Engineering,Chemical Engineering,Industrial Engineering,Mathematics,Statistics,Mathematical Statistics

Minimum Year(s) Of Experience

5 year(s)

Preferred Qualifications

Demonstrates abilities and/or success in one or many of the following areas:

Design and implement comprehensive data architecture strategies that meet the current and future business needs; Develop and document data models, data flow diagrams, and data architecture guidelines; Ensure data architecture is compliant with data governance and data security policies; Collaborate with business stakeholders to understand their data requirements and translate them into technical solutions; Evaluate and recommend new data technologies and tools to enhance data architecture; Build, maintain, and optimize ETL/ELT pipelines for data ingestion, processing, and storage across batch and real-time data processing; Build, maintain, and optimize Data Quality rules leveraging DQ tools and/or other ETL/ELT tools; Develop and deploy scalable data storage solutions using AWS, Azure and GCP services such as S3, Redshift, RDS, DynamoDB, Azure Data Lake Storage, Azure Cosmos DB, Azure SQL DB, GCP Cloud Storage etc.; Implement data integration solutions using AWS Glue, AWS Lambda, Azure Data Factory, Azure Functions, GCP Functions, GCP Dataproc, Dataflow and other relevant services; Design and manage data warehouses and data lakes, ensuring data is organized and accessible; Monitor and troubleshoot data pipelines, data warehouses and workflows to ensure data quality, system reliability, performance and cost management; Implement IAM roles and policies to manage access and permissions within AWS, Azure, GCP; Use AWS CloudFormation, Azure Resource Manager templates, Terraform for infrastructure as code (IaC) deployments; Use AWS, Azure and GCP DevOps services to build and deploy DevOps pipelines; Implement data security best practices using AWS, Azure, GCP, Snowflake or Databricks; Optimize Cloud resources for cost, performance, and scalability; Strong proficiency in SQL and experience with relational databases; Proficient in programming languages such as Python, Java, or Scala; Familiarity with big data technologies like Hadoop, Spark, or Kafka is a plus; Experience with machine learning and data science workflows is a plus; Knowledge of data governance and data security best practices; Strong analytical, problem-solving, and communication skill; and, Ability to work independently and as part of a team in a fast-paced environment. 

Demonstrates thought leader-level abilities with, and/or a proven record of success directing efforts in the following areas:

Applying modern, cloud-based technology skills, ability to research emerging trends, analyst publications, and adoption of modern technologies in solution architectures; Collaborating and contributing as a team member: understanding personal and team roles, contributing to a positive working environment by building proven relationships with team members, proactively seeking guidance, clarification and feedback; Prioritizing and handling multiple tasks, researching and analyzing pertinent client, industry and technical matters, utilizing problem-solving skills, and communicating effectively in written and verbal formats to various audiences (including various levels of management and external clients) in a professional business environment; and, Coaching and collaborating with associates who assist with this work, including providing coaching, feedback and guidance on work performance. 

Learn more about how we work: https://pwc.to/how-we-work

PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.

As PwC is an equal opportunity employer, all qualified applicants will receive consideration for employment at PwC without regard to race; color; religion; national origin; sex (including pregnancy, sexual orientation, and gender identity); age; disability; genetic information (including family medical history); veteran, marital, or citizenship status; or, any other status protected by law. 

For only those qualified applicants that are impacted by the Los Angeles County Fair Chance Ordinance for Employers, the Los Angeles' Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, San Diego County Fair Chance Ordinance, and the California Fair Chance Act, where applicable, arrest or conviction records will be considered for Employment in accordance with these laws. At PwC, we recognize that conviction records may have a direct, adverse, and negative relationship to responsibilities such as accessing sensitive company or customer information, handling proprietary assets, or collaborating closely with team members. We evaluate these factors thoughtfully to establish a secure and trusted workplace for all.

Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines

The salary range for this position is: $99,000 - $232,000, plus individuals may be eligible for an annual discretionary bonus. For roles that are based in Maryland, this is the listed salary range for this position. Actual compensation within the range will be dependent upon the individual's skills, experience, qualifications and location, and applicable employment laws. PwC offers a wide range of benefits, including medical, dental, vision, 401k, holiday pay, vacation, personal and family sick leave, and more. To view our benefits at a glance, please visit the following link: https://pwc.to/benefits-at-a-glance

",https://ad.doubleclick.net/ddm/clk/438405794;254302458;m?https://jobs.us.pwc.com/job/-/-/932/81216676944?utm_source=linkedin.com&utm_campaign=core_media&utm_medium=social_media&utm_content=job_posting&ss=paid,6
13,iHire,https://www.linkedin.com/company/ihire-llc/life,Machine Learning Engineer,https://www.linkedin.com/jobs/view/4259189813,4259189813,"Dallas, TX",Remote,,2025-07-02 13:28:19,False,,"Capital One has partnered with iHire to reach top talent for their opening below. Check it out and apply via iHireTechnology today!

Lead Machine Learning Engineer, Shopping - Feed (Remote)

Interested in joining a dynamic remote first engineering team in a fast-paced environment full of greenfield problem-solving? Then Capital One Shopping might be the place for you. Join us in supporting a growth-stage line of business with a startup mindset as we build technology to save our customers money.

As a Capital One Machine Learning Engineer (MLE), you'll be part of a fast moving, highly collaborative Agile team dedicated to productionizing machine learning applications and systems at scale. Youll drive and deliver the detailed technical designs, development, and implementation of machine learning applications using existing and emerging technology platforms. Youll be a leader of machine learning architectural design, develop and review model and application code, and ensure high availability and performance of our machine learning applications. You'll contribute to researching our next generation of models and recommendation systems to deliver value to our customers. Youll mentor junior developers and serve as a technical bridge between product partners. You will use tools like Docker, Nomad, SQL, Python, Pytorch, Transformers, language models, and other statistical tools.

This is more than just a job; it's an opportunity to be part of a collaborative and forward-thinking community, where your contributions will make a significant impact in an ever-dynamic tech landscape. Join us as we push boundaries and redefine the future of our industry.

What Youll Do In The Role

 The MLE role overlaps with many disciplines, such as Ops, Modeling, and Data Engineering. In this role, you'll be expected to perform many ML engineering activities, including one or more of the following:  Design, build, and/or deliver ML models and components that solve real-world business problems, while working in collaboration with the Product and Data Science teams.  Inform your ML infrastructure decisions using your understanding of ML modeling techniques and issues, including choice of model, data, and feature selection, model training, hyperparameter tuning, dimensionality, bias/variance, and validation).  Solve complex problems by writing and testing application code, developing and validating ML models, and automating tests and deployment.  Collaborate as part of a cross-functional Agile team to create and enhance software that enables state-of-the-art big data and ML applications.  Retrain, maintain, and monitor models in production.  Leverage or build cloud-based architectures, technologies, and/or platforms to deliver optimized ML models at scale.  Construct optimized data pipelines to feed ML models.  Leverage continuous integration and continuous deployment best practices, including test automation and monitoring, to ensure successful deployment of ML models and application code.  Ensure all code is well-managed to reduce vulnerabilities, models are well-governed from a risk perspective, and the ML follows best practices in Responsible and Explainable AI.  Use programming languages like Python, Scala, or Java.  Design and research new models using data scientist experience/expertise 

Basic Qualifications

 Bachelors degree  At least 6 years of experience designing and building data-intensive solutions using distributed computing (Internship experience does not apply)  At least 4 years of experience programming with Python, Scala, or Java  At least 2 years of experience building, scaling, and optimizing ML systems 

Preferred Qualifications

 Master's or doctoral degree in computer science, electrical engineering, mathematics, or a similar field  3+ years of experience building production-ready data pipelines that feed ML models  3+ years of on-the-job experience with an industry recognized ML framework such as scikit-learn, PyTorch, Dask, Spark, or TensorFlow  2+ years of experience developing performant, resilient, and maintainable code  2+ years of experience with data gathering and preparation for ML models 

At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).

The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.

Remote (Regardless of Location): $175,800 - $200,700 for Lead Machine Learning EngineerRichmond, VA: $175,800 - $200,700 for Lead Machine Learning Engineer

Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidates offer letter.

This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.

Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.

This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York Citys Fair Chance Act; Philadelphias Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at [phone removed] or via email at [email removed] . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.

For technical support or questions about Capital One's recruiting process, please send an email to [email removed]

Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.

Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).

",https://www.ihiretechnology.com/ppc/dp/24/483539033?utm_campaign=programmatic&utm_medium=downpost&utm_term=2025-07-02&utm_content=&utm_source=linkedin&ih_linkedinjobtype=slot&ih_date=2025-07-02,6
53,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Amazon Pharmacy",https://www.linkedin.com/jobs/view/4260849088,4260849088,"Seattle, WA",On-site,$118.9K/yr - $205.6K/yr,2025-07-02 14:08:35,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Amazon.

Description

Innovation is at the core of what we do. We believe that by removing and reducing the barriers that prevent people from taking their medications, we can help customers conveniently get the medications they need, when they need them and take them as prescribed.

We have provided customers with the ability to find transparent and simple pricing, receive 24/7 customer service support, and have their meds delivered to their doorsteps while creating programs and products that embody our mission and position ourselves in becoming the world’s safest and fastest online pharmacy.

We are looking for a Data Engineer to join us on our journey to make it drastically easier for customers to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy!

As a Data Engineer in Pharmacy team, you will partner with Software Engineers, Business Intelligence Engineers and product managers. You will design, implement, and maintain next generation BI solutions for large scale, highly secure and complex data structures to ensure the data is auditable, available and accessible. You will gain a deep understanding of our services and the data they produce, and become our resident expert in how to transform that data into a format that is useful for analytics and business intelligence. You will proactively help to identify new data for integration with our platform, and propose and implement new technologies to help us better understand our data.

Key job responsibilities

 Design, implement, maintain and support a secure and robust data lake in native AWS with 300+ datasets which contain both PII and PHI data. Implement ingestion routines both real time and batch using best practices in modeling, ETL/ELT processes by leveraging AWS technologies and big data tools. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL, Python and AWS big data technologies. Gather business and functional requirements and translate into secure, robust, scalable,

operable solutions with a flexible and adaptable architecture.

 Collaborate with engineers to help adopt best practices in system creation, integrity, test design, analysis, validation, and documentation. Help continually improve ongoing reporting and analysis processes, automating or simplifying

self-service tools for customers.

 Explore and learn the latest AWS technologies to provide new capabilities and increase

efficiency

Basic Qualifications

3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Bachelor's degree 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience working on and delivering end to end projects independently 

Preferred Qualifications

Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience building large-scale, high-throughput, 24x7 data systems Experience providing technical leadership and mentoring other engineers for best practices on data engineering Experience working with secure medical data, pipeline, services and HIPAA compliant environment 

Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.

Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits . This position will remain posted until filled. Applicants should apply via our internal or external career site.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/4394ea22965d4bb9976072a12fb5060ftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
22,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer I,https://www.linkedin.com/jobs/view/4260844616,4260844616,"San Antonio, TX",On-site,,2025-07-02 14:08:36,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for USAA.

Why USAA?

At USAA, our mission is to empower our members to achieve financial security through highly competitive products, exceptional service and trusted advice. We seek to be the #1 choice for the military community and their families.

Embrace a fulfilling career at USAA, where our core values – honesty, integrity, loyalty and service – define how we treat each other and our members. Be part of what truly makes us special and impactful.

The Opportunity

Employer: United Services Automobile Association

Tasks: Identifies and manages existing and emerging risks that stem from business activities and the job role. Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. Follows written risk and compliance policies and procedures for business activities. Design and implement technical solutions. Identify and solve significant technical problems and architecture deficiencies. Participate in daily standups and design reviews. Breakdown business features and into technical stories and approaches. Analyze data and enable machine learning. Create proof of concepts and prototypes. Help on-board entry level engineers. Collaborate with the team and other engineers to plan and execute assignments and tasks. Mentor junior engineers. May telecommute.

Requirements: Will accept a Bachelor’s degree in Computer Science, Computer Software, Computer Engineering, Applied Sciences, Mathematics, Physics, or related field and 4 years of experience in the job offered or in an engineering-related occupation. Alternatively, will accept a Master’s degree in Computer Science, Computer Software, Computer Engineering, Applied Sciences, Mathematics, Physics, or related field and 1 year of experience in the job offered or in an engineering-related occupation. Position requires experience in the following:

Cloud Computing Machine Learning, Deep Learning - Computer Vision Large Scale Distributed Systems Machine Learning infrastructure as code SQL Unix or Linux Shell Scripting Programming Languages: Python, Java, C, and C++ Cuda programming Agile Methodologies Airflow Docker and Containerization Git (CI/CD Automation) Data Extract Transform Load (ETL) Amazon Web Services (AWS) Technologies: Sagemaker, EC2 (Elastic Compute Cloud), S3 (Simple storage Service), Lambda, Step Functions, CloudFormation, DynamoDB, Athena, Relational Databases (RDS), Cloudwatch, SNS (Simple Notification Service), and SQS (Simple Queuing Service) 

Worksite: 9800 Fredericksburg Road, San Antonio, TX 78288

Relocation assistance is Not Available for this position.

This position is eligible for the Employee Referral Program.

#DNP

Compensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location.

Employees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.

The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.

Benefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.

For more details on our outstanding benefits, visit our benefits page on USAAjobs.com.

Applications for this position are accepted on an ongoing basis, this posting will remain open until the position is filled. Thus, interested candidates are encouraged to apply the same day they view this posting.

USAA is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

If you are an existing USAA employee, please use the internal career site in OneSource to apply.

Please do not type your first and last name in all caps.

Find your purpose. Join our mission.

USAA is unlike any other financial services organization. The mission of the association is to facilitate the financial security of its members, associates and their families through provision of a full range of highly competitive financial products and services; in so doing, USAA seeks to be the provider of choice for the military community. We do this by upholding the highest standards and ensuring that our corporate business activities and individual employee conduct reflect good judgment and common sense, and are consistent with our core values of service, loyalty, honesty and integrity.

USAA attributes its long-standing success to its most valuable resource: our 35,000 employees. They are the heart and soul of our member-service culture. When you join us, you'll become part of a thriving community committed to going above for those who have gone beyond: the men and women of the U.S. military, their associates and their families. In order to play a role on our team, you don't have to be connected to the military yourself – you just need to share our passion for serving our more than 13 million members.

USAA is an EEO/AA Employer - applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetic information, sexual orientation, gender identity or expression, pregnancy, protected veteran status or other status protected by law.

California applicants, please review our HR CCPA - Notice at Collection (https://statmcstg.usaa.com/mcontent/static_assets/Media/enterprise_hr_cpra_notice_at_collection.pdf) here.

USAA is an EEO/AA Employer - applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetic information, sexual orientation, gender identity or expression, pregnancy, protected veteran status or other status protected by law.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/ec4247e74c254c5fb738b61be4805278tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
60,Startup,https://www.linkedin.com/company/startup_5/life,Senior Data Operations Engineer [32296],https://www.linkedin.com/jobs/view/4259178175,4259178175,United States,Remote,,2025-07-02 11:09:35,True,over 100 applicants,"Position SummaryThe Senior Data Operations Engineer plays a key role in ensuring the reliability, accuracy, and responsiveness of the company's Pharmacy’s data ingestion ecosystem. This role focuses on monitoring and managing ingestion pipelines, validating vendor data, creating operational runbooks, and leading incident response efforts. You will partner closely with Data Engineers to proactively surface pipeline issues and improve operational efficiency across the data platform.Duties and ResponsibilitiesMonitors daily ingestion jobs, vendor data feeds, and API/file-based pipelines for delays, failures, and anomalies.Serves as the first line of response for data operations incidents, escalating issues according to defined paths.Performs validation of inbound vendor files to identify schema mismatches, volume anomalies, or missing data.Designs and maintains ingestion quality checks to ensure freshness, completeness, and correctness.Collaborates with the Data Engineering team to identify and address root causes of ingestion failures.
While performing the responsibilities of the job, the employee is required to talk and hear. The employee is often required to remain in a stationary position for a significant amount of the workday and frequently use their hands and fingers to handle or feel in order to access, input, and retrieve information from the computer and other office productivity devices. The employee is regularly required to move about the office and around the corporate campus. The employee is regularly required to stand, walk, reach with arms and hands, climb or balance, and to stoop, kneel, crouch or crawl.Knowledge and SkillsProficient in scripting and automation (Python, Bash, or similar) for data quality checks and alerting required.Solid understanding of data formats (CSV, JSON, Parquet), file validation, and API-based ingestion required.Hands-on experience with Snowflake or similar cloud data warehouse technologies preferred.Familiarity with incident response best practices, including playbooks, escalation paths, and postmortems preferred.
Key CompetenciesCustomer Focus: Ability to build strong customer relationships and deliver customer centric solutions.Optimizes Work Processes: Know the most effective and efficient processes to get things done, with a focus on continuous improvement.Collaborates: Builds partnerships and works collaboratively with others to meet shared objectives.Resourcefulness: Secures and deploys resources effectively and efficiently.Manages Complexity: Makes sense of complex, high quality, and sometimes contradictory information to effectively solve problems.Ensures Accountability: Holds self and others accountable to meet commitments and objectives.Situational Adaptability: Adapts approach and demeanor in real time to match shifting demands of different situations.Communicates Effectively: Develops and delivers multi-mode communications that convey a clear understanding of the unique needs of different audiences.
ValuesPeople: Our people define who we are as a company, and we believe that understanding and addressing the needs of our team, clients, and community is fundamental to fostering a culture of support and growth.Quality: Quality stands at the core of our mission, reflecting our commitment to excellence in every medication we produce.Service: We are here to serve others. Every interaction with our patients, providers, employees and other stakeholders comes from a place of service.Innovation: By continuously exploring new methodologies and embracing technology, we ensure that every solution we offer is at the forefront of pharmaceutical care.Experience and QualificationsBachelor’s or Master’s degree in Computer Science, Information Systems, or a related technical field required.8+ years of experience in data operations, pipeline monitoring, or production support for data systems required.Deep experience monitoring ingestion tools such as Airflow, Fivetran, or custom ingestion frameworks required.AWS Certified SysOps Administrator; Certified Kubernetes Administrator or Docker Certified Associate; Apache Airflow Certification.BenefitsWe offer comprehensive benefits to support your health, well-being, and future, including medical, dental, and vision coverage, paid time off, 401(k) matching, wellness perks, IV therapy, and compounded medications",https://www.linkedin.com/jobs/view/4259178175,6
19,EPITEC,https://www.linkedin.com/company/epitec/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4259503985,4259503985,"Dearborn, MI",Hybrid,$69/hr - $72/hr,2025-07-02 14:52:18,True,over 100 applicants,"Position Description:We’re seeking an experienced GCP Data Engineer who can build cloud analytics platform to meet ever expanding business requirements with speed and quality using lean Agile practices. You will work on analyzing and manipulating large datasets supporting the enterprise by activating data assets to support Enabling Platforms and Analytics in the Google Cloud Platform (GCP). You will be responsible for designing the transformation and modernization on GCP, as well as landing data from source applications to GCP. Experience with large scale solution and operationalization of data warehouses, data lakes and analytics platforms on Google Cloud Platform or other cloud environment is a must. We are looking for candidates who have a broad set of technology skills across these areas and who can demonstrate an ability to design right solutions with appropriate combination of GCP and 3rd party technologies for deploying on Google Cloud Platform. You will:Work in collaborative environment including pairing and mobbing with other cross-functional engineersWork on a small agile team to deliver working, tested softwareWork effectively with fellow data engineers, product owners, data champions and other technical expertsDemonstrate technical knowledge/leadership skills and advocate for technical excellenceDevelop exceptional Analytics data products using streaming, batch ingestion patterns in the Google Cloud Platform with solid Data Warehouse principlesBe the Subject Matter Expert in Data Engineering and GCP tool technologiesSkills Required:Experience in working in an implementation team from concept to operations, providing deep technical subject matter expertise for successful deployment. Implement methods for automation of all parts of the pipeline to minimize labor in development and productionExperience in analyzing complex data, organizing raw data and integrating massive datasets from multiple data sources to build subject areas and reusable data productsExperience in working with architects to evaluate and productionalize appropriate GCP tools for data ingestion, integration, presentation, and reportingExperience in working with all stakeholders to formulate business problems as technical data requirement, identify and implement technical solutions while ensuring key business drivers are captured in collaboration with product management This includes designing and deploying a pipeline with automated data lineage. Identify, develop, evaluate and summarize Proof of Concepts to prove out solutions. Test and compare competing solutions and report out a point of view on the best solution. Design and build production data engineering solutions to deliver pipeline patterns using Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, DataProc, Cloud Composer, Cloud SQL, Compute Engine, Cloud Functions, and App Engine.Skills Preferred:Strong drive for results and ability to multi-task and work independentlySelf-starter with proven innovation skillsAbility to communicate and work with cross-functional teams and all levels of managementDemonstrated commitment to quality and project timingDemonstrated ability to document complex systemsExperience in creating and executing detailed test plansExperience Required:In-depth understanding of Google’s product technology (or other cloud platform) and underlying architectures5+ years of analytics application development experience required5+ years of SQL development experience3+ years of Cloud experience (GCP preferred) with solution designed and implemented at production scaleExperience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Terraform, Big Query, Google Cloud Storage, PubSub, Dataflow, Dataproc, Airflow, etc.2 + years professional development experience in Java or Python, and Apache BeamExtracting, Loading, Transforming, cleaning, and validating dataDesigning pipelines and architectures for data processing1+ year of designing and building CI/CD pipelinesExperience Preferred:Experience building Machine Learning solutions using TensorFlow, BigQueryML, AutoML, Vertex AIExperience in building solution architecture, provision infrastructure, secure and reliable data-centric services and application in GCPExperience with DataPlex or Informatica EDC is preferredExperience with development eco-system such as Git, Jenkins and CICDExceptional problem solving and communication skillsExperience in working with DBT/DataformExperience in working with Agile and Lean methodologiesTeam player and attention to detailPerformance tuning experienceEducation Required:Bachelor’s degree in computer science or related scientific field • IT or related Associated topics: data architect, data center, data integrity, data manager, data management, data scientist, data warehousing, sql, sybase, TeradataEducation Preferred:GCP Professional Data Engineer CertifiedMaster’s degree in computer science or related field2+ years mentoring engineersIn-depth software engineering knowledge Additional Information :***POSITION IS HYBRID BUT WILL CONSIDER FULLY REMOTE FOR A VERY STRONG CANDIDATE***",https://www.linkedin.com/jobs/view/4259503985,6
64,Varsity Brands,https://www.linkedin.com/company/varsitybrands/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4259506642,4259506642,United States,Remote,$110K/yr - $130K/yr,2025-07-02 15:09:10,False,,"JOIN THE BEST TEAM IN SPORT & SPIRIT

At Varsity Brands, we believe every student deserves the opportunity to succeed and every educator wants to make a difference. It takes a team to make a real impact, and through our two divisions – BSN SPORTS and Varsity Spirit – and our network of 6,000+ employees and independent representatives, we are proud to partner with a wide range of educational institutions and club and professional sports to transform the student journey in SPORT and SPIRIT.

WORK TYPE: Remote

LOCATION DETAILS: 14460 Varsity Brands Way, Farmers Branch, TX 75244

WORK HOURS: Monday – Friday; 8am-5pm CST

TRAVEL REQUIREMENT: Less than 5%

BASE PAY RATE: $110,000 - $130,000

The base salary will vary based on criteria such as education, experience and qualifications of the applicant, location, internal equity, and alignment with the market. 

How You Will Make An Impact

The Senior Data Engineer (Sourcing & Pipeline Management) to the HR team will play an integral role within the growing Varsity Brands Data Center of Excellence team.

The Role’s Key Elements Are

Architect + Implement: Design, build and launch efficient and reliable data pipelines to move data from source platforms, including front-end applications, back-end systems, and third-party analytics and data services, to our enterprise data hub. In addition, design and build pipelines to supply downstream enterprise applications with prepared reference data from our enterprise data hub.Orchestrate + Monitor: Manage data pipelines as an interdependent network, with proactive visibility into pipeline errors as well as costs over time.Partner + Educate: Partner with stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions. Use your data and analytics experience to identify gaps and propose improvements in existing systems and processes, as well as making your source data pipelines easily accessible to data stakeholders

What You Will Do

Working with data modelers and analysts to identify and prioritize data sourcing gaps.Assessing best fit tool for any given data source.Establishing pipeline cadences and timing based on analytics needs and use cases while being cost conscious.Providing downstream data stakeholders visibility to pipeline scheduling and status.Responsively troubleshooting errors or alerts in existing pipelines.Tracking and summarizing current period pipeline costs and trends for business and IT stakeholders.

Qualifications

KNOWLEDGE/ SKILLS/ ABILITIES

Familiarity with modern data stack tools and services employed to replicate data from source systems to cloud data warehouses or lakes, and a solid understanding of when and where a given tool is appropriate. Experience utilizing data replication tools and services like HVR, Fivetran, Airbyte, Meltano, Matillion, as well as proficiency writing custom code to source data from APIs when needed.Ability to work collaboratively with product or application owners to tease out relevant raw data is available to source as well as to identify source system data capture opportunities to unlock analytics capabilities.Strong knowledge in data architecture, data modeling, schema design, and software development principles.

Education/ Experience

3+ years of experience in the data engineering/warehousing space, custom ELT/ETL design, implementation, and maintenance.3+ years of experience writing SQL in an analytics or data pipeline context.2+ years of experience in at least one language (Python, Scala, Java, etc) in a data engineering or analytics context.1+ year of experience using an orchestration tool or service to coordinate ELT and downstream analytics pipelines.Experience using REST APIs to acquire and flow data from source to target systems.Experience working with cloud data analytics platforms and tools, particularly Snowflake, dbt, Tableau and Power BI.Experience standing up data pipelines from SAP ERP is a plus.Experience standing up data pipelines from Google Analytics 4 data is a plus.

Physical Requirements

This job operates in a professional office environment. Largely a sedentary role with some filing requiring the ability to lift files, open filing cabinets and bending or standing on a stool as necessary. The ability to sit or stand for long periods through meetings and while operating office equipment, PC’s, laptop, telephone will be required.

While performing the duties of this Job, the employee is regularly required to sit; use hands to finger, handle, or feel; reach with hands and arms and talk or hear. The employee is occasionally required to stand; walk; drive, climb, balance, stoop, kneel, crouch, or crawl.

The ability to stand and walk for long periods of time (5 hours plus) is required. The ability to communicate with customers and colleagues using the following but not limited to radio, walkie talkie, text message and email is required. The ability to lift, bend, push, pull and manipulate equipment that is a minimum of 30 lbs. is required.

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities required of the employee for this job.

OUR VALUES

Service - We lead with heart. We champion community.

Passion - We love what we do. It fuels our purpose.

Integrity - We do what we promise. We own our actions and decisions.

Respect - We earn it by giving it. Because everyone deserves it.

Innovation - We never stop striving to be better. For ourselves and our community.

Transparency - We are committed to openness and honesty in everything we do.

Our Benefits

We are committed to putting you and your families first. For benefits eligible roles, we offer a variety of choices and costs as well as program enhancements that align with our responsibility to elevate the employee experience. Some of our offerings include:

Comprehensive Health Care BenefitsHSA Employer Contribution/ FSA OpportunitiesWellbeing Program401(k) plan with company matchingCompany paid Life, AD&D, and Short-Term DisabilityGenerous My Time Off & Paid HolidaysVarsity Brands Ownership ProgramEmployee Resource GroupsSt. Jude Partnership & Volunteer OpportunitiesEmployee Perks including discounts on personal apparel and equipment!

Varsity Brands companies are equal opportunity employers. Qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, citizenship, gender, sexual orientation, gender identity, veteran’s status, age or disability.

",https://careers.varsitybrands.com/us/en/job/VABRUSJR109383EXTERNALENUS/Senior-Data-Engineer?utm_source=linkedin&utm_medium=phenom-feeds,6
65,Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer - Remote,https://www.linkedin.com/jobs/view/4260843794,4260843794,"Decatur, GA",Remote,,2025-07-02 14:08:23,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for TWO95 International.

Job Title: Power BI Developer

Location: Remote

Type: Long Term Contract

Rate: $Open /hr.

Expertise as a PowerBI developer with experience as a sole contributor in the design and develop of reports and dashboards Strong proficiency in data modeling, report building, and data visualization and reporting Ability to organize and structure data to be easier to work with and understand; design interactive Power BI dashboards Expertise in DAX functions including complex formulas 

Note: If interested please send your updated resume and include your rate requirement along with your contact details with a suitable time when we can reach you. If you know of anyone in your sphere of contacts, who would be a perfect match for this job then, we would appreciate if you can forward this posting to them with a copy to us.

We look forward to hearing from you at the earliest!

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/3d8ac05aa20448eab27affeea45cba13tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
51,SAIC,https://www.linkedin.com/company/saicinc/life,Big Data Engineer Principal,https://www.linkedin.com/jobs/view/4260829874,4260829874,"Washington, United States",Remote,$120K/yr - $160K/yr,2025-07-02 13:07:49,False,,"Job ID 2507187

Location REMOTE WORK, WA, US

Date Posted 2025-07-01

Category Information Technology

Subcategory Metrics and Data

Schedule Full-time

Shift Day Job

Travel No

Minimum Clearance Required None

Clearance Level Must Be Able to Obtain None

Potential for Remote Work Yes

Description

Big Data Engineer Principal to work in Reston VA

Roles And Responsibilities

Develop and design the application data. Develop and build optimal databases using an agile-like environment to store data coming from legacy systems and provide bi-directional dataflow. Interact with the business and Data Engineers and collect the requirements for data model for the target system and from the current system. Translate business requirements into conceptual, logical, and physical data models. Research, design, develop and implement detailed statistical models, and advanced analytics to support key strategic initiatives. Assess the effectiveness and accuracy of new data sources and data gathering techniques with expertise delivery. Develop processes and tools to monitor and analyze analytical model performance and data accuracy, providing ad-hoc analysis and visualizations. Focus on developing financial, healthcare and supply chain dashboards. Use ETL pipelines, pyspark, SSMS, data preprocessing, and data ingestion.

Qualifications

Requires a Bachelor’s degree in Enterprise Analytics (or equivalent based on evaluation of academic credentials, training and/or experience) as well as twelve (12) months in job or job related experience to include assess the effectiveness and accuracy of new data sources and data gathering techniques with expertise delivery; interact with the business and Data Engineers and collect the requirements for data model for the target system and from the current system; assess the effectiveness and accuracy of new data sources and data gathering techniques with expertise delivery; use ETL pipelines, pyspark, SSMS, data preprocessing, and data ingestion. Opportunity to work from home. Full-time position at 40 hours/week, Monday through Friday 800 a.m. to 500 p.m. Apply www.saic.com, Science Applications International Corporation. Job code# 2507187. EOE.

Target salary range $120,001 - $160,000. The estimate displayed represents the typical salary range for this position based on experience and other factors.",https://jsv3.recruitics.com/redirect?rx_cid=3284&rx_jobId=2507187&rx_url=https%3A%2F%2Fjobs.saic.com%2Fjobs%2F16370157-big-data-engineer-principal%3Fbid%3D370%26rx_campaign%3DLinkedin1%26rx_ch%3Djobpost%26rx_group%3D128565%26rx_id%3Db2ad5a3d-56c0-11f0-ba6f-d93f5fc9c8bf%26rx_job%3D2507187%26rx_medium%3Dpost%26rx_paid%3D1%26rx_r%3Dnone%26rx_source%3Dlinkedin%26rx_ts%3D20250702T121211Z%26rx_vp%3Djobslots%26tm_company%3D2520%26tm_event%3Dview%26tm_job%3D2507187,6
77,CareSignal® – Lightbeam's Deviceless Remote Patient Monitoring,https://www.linkedin.com/company/caresignal/life,Data Transformation Engineer,https://www.linkedin.com/jobs/view/4260822784,4260822784,"Horsham, PA",On-site,,2025-07-02 11:54:28,False,,"Job Summary

As a leader in end-to-end population health management solutions and services, we are continuing to grow our talented software development team. We are currently looking for full-time Data Transformation Engineer to join our experienced team. The Data Transformation Engineer will design, configure, implement, operate, and maintain technology systems. The Data Transformation Engineer will be capable of addressing new technology, compliance, and industry standards. They will be fluent in HL7 data management and normalization. Ability to work with teammates to accept support tickets from client partners around specific data transformation requests. This position will review, troubleshoot, analyze, and triage tickets appropriately. Strong personal skills in collaborating with partner resources Ideal candidate will have experience in:

Healthcare ITJavaJavaScriptGroovyXMLHL7/CCD knowledgeXSLTJSONSQL/Postgres

Job Responsibilities

Provide expertise on receiving and transforming data from disparate sourcesWork cross functionally within the company and externally with partners to meet data transformation, interface mapping, configuration, and validation needsPerform risk management pertaining to interfaces and data transformation to maximize client implementation successTrack any/all open interface issues through resolutionActively work with internal and external project team members to resolve open interface implementation itemsMonitor and report on ongoing data transmission for errors or issues at all stages of data ingestion and transformation, including communication to CustomerReport and escalate support issues as appropriateCommunicate and collaborate with development engineers for knowledge and assistanceEnsure interfaces and data transformation work are completed within agreed upon project schedule timelines provided by OperationsReview validation reports and address any discrepancies with the Customer

Qualifying Job Knowledge, Skills & Abilities

Proven working experience in interface implementation and/or working with claims/clinical data within the Healthcare IndustryPayor/Provider Background preferredKnowledge of value-based care, population health a plusAccustomed to Customer/Partner-facing rolesAbility to speak to all levels within an organizationOutstanding communication skillsSolid organizational skills, including attention to detail and multi-tasking skillsStrong working knowledge of Microsoft Office SuiteStrong working knowledge of SQLAbility to embrace change and company growthAnalytical and problem-solving skillsProven experience managing relationships with Customers technical and analytical team members

Key Competencies

Critical thinking with problem solving skillsDecision-makingCommunication skillsInfluencing and leadingTeamwork and positivityConflict managementAdaptabilityStress toleranceStrong computer skills are required

Education

Required: BS Degree (or higher) or equivalent work experience

Lightbeam Internal Use Only

Position Security Classification

CRITERIA: POSITION RISK LEVEL

High Risk (if any of these are true)

Access to Restricted information, including PHI and PII

Elevated access privileges

Access to the Security Infrastructure

Medium Risk

Access to Confidential, but not to Restricted information

Low Risk

Access to Unclassified information only

Data Transformation Engineer: High

CRITERIA: SECURITY ROLES & RESPONIBILITIES

Security Roles & Responsibilities

 Approve security-related changes Execute security-related changes Not applicable

Policy/procedures for Lightbeam

Cyber Security Incident Response (CSIRT) team

Data Transformation Engineer: Not Applicable

SECURITY ROLE-SPECIFIC TRAINING

Annual Incident Response Exercises",https://www.ziprecruiter.com/k/l/AAK6d-l67vvU7bXALlJLN46fvtBy1ydSZ_XopWtEiM_k9mMctN1iYfjzHNq8NG5Fdo5uvlQ5oeQFk-URyqR47XyVsJMilQCNkNApYMaz55gWhBQ-CJUGdWYdQzUB0V4JOoJXR5F22hQRaji3NVx3sIxOOEM7bexfFzDxU-1HlQLSFrFPHHINITRk6M1es_zB2lvSs5aMfMIcGLGslNr0Qif0pRxbFV_0xt9jjVKhqBFBKobpk8az5R0xCgeoxVdqRUBvUTbwFU90JkG8FE7YUMTqGSSRpIexgnE6YYYzoah2SyP84e7azSwT46ZxxImTpTWv4pUMi8gWTpYd,6
36,EnergyCAP,https://www.linkedin.com/company/energycap-llc/life,Data Engineer,https://www.linkedin.com/jobs/view/4260811725,4260811725,"Greenwood Village, CO",On-site,$105K/yr - $130K/yr,2025-07-02 09:52:05,False,,"For over 40 years, more than 10,000 energy, sustainability, and finance leaders have been using EnergyCAP to streamline accounting processes, reduce resource consumption, and identify opportunities for sustainable operations. EnergyCAP helps customers who are drowning in paper bills, manual processes, and cumbersome spreadsheets and enables them to execute, analyze, and report on the energy projects needed to build a more sustainable world.

EnergyCAP is the only Energy & Sustainability ERP empowering customers with full control and understanding of their energy & sustainability data to reduce their carbon footprint and drive savings.

Our values are Accountability, Impact, Innovation, Integrity, and Teamwork.

About The Role

At EnergyCAP, data isn't just a byproduct of our work—it is the work. We’re looking for our first in-house Data Engineer to lay the groundwork for a best-in-class data operation. This is a wide-ranging role with lots of visibility and a clear path toward future leadership.

If you enjoy the challenge of bringing order to chaos, solving hard problems at the system level, and translating data into business insight, this is your moment! You'll spend half your time focused on engineering scalable, reliable data infrastructure. The other half you’ll bring clarity to the business through reporting, dashboarding, and analysis.

Key Responsibilities

As a member of our Product team, you’ll own data pipelines end-to-end, implement new infrastructure, and bring disparate systems into alignment. You’ll work across the company delivering clean and actionable data.

You'll play a key role in equipping internal teams with insights they can act on by creating dashboards and reports that drive clarity and smarter decisions. In this high visibility role, you’ll work closely with executive leaders and decision makers. Your work will help determine data strategy and shape the future of data engineering and analytics at EnergyCAP.

Responsibilities

Design, implement, optimize, maintain, and own data pipelines for downstream usage.Configure and manage data infrastructure including databases, data warehouses and data lakes.Design, create, and schedule organizational dashboard and operational reporting to empower critical business processes .Ensure data quality and integrity throughout the data lifecycle.Troubleshoot and resolve data-related issues in a timely manner.Contribute to long-term data strategy and roadmap.

Preferred Experience & Skills

2–5 years of experience in data engineering, business intelligence, or analytics roles.Proficiency in Python and SQL + familiarity with platforms like Kafka, DuckDB, Kestra, N8N, Parquet, PostgreSQL, or Looker.Experience with Power BI, Tableau, or similar BI tools.A track record of building or maintaining ETL pipelines, ensuring data quality, and supporting operational reporting.Strong communication skills and a willingness to collaborate across teams. A builder’s mindset - you’re not just here to plug holes, you’re here to create a winning foundation. 

Pay Range: $105,000 - $130,000, 3% annual bonus

Pay is commensurate on a variety of factors, including experience, knowledge, skills and abilities.

Benefits (US)

100% company-paid medical (employee only level), competitive cost sharing when covering dependents100% company-paid dental, vision, life, and LTD insurance for yourselfCharitable contributions and matched employee giving programRemote Work (with occasional travel to the office)Emergenetics Development ProgramCounseling and adoption grants401(k) with 3% company matchProfessional coachingConnectivity stipendTuition assistance

EnergyCAP is a proud equal opportunity employer, committed to hiring and developing individuals from diverse background and experiences. We value and consider applications for all qualified candidates without regard to actual or perceived race, color, religion, national origin, sex, gender, age, marital status, personal appearance, sexual orientation, gender identity or expression, family responsibilities, disability, medical condition, enrollment in college or vocational school, political affiliation, military or veteran status, citizenship status, genetic information, or any other basis protected by federal, state or local law. 

EnergyCAP will reasonably accommodate qualified individuals with disabilities in accordance with applicable law. If you are in need of an accommodation in order to submit your application, please email HR@energycap.com. 

Please click HERE to view the mandatory Federal Applicant Laws that apply to your employment rights.

Note: We are currently not sponsoring VISAs.

#hybrid #remote",https://ats.rippling.com/energycap-external-career-page/jobs/f36f815b-9276-488e-8bca-68ad7b5fda29?jobSite=LinkedIn,6
71,Lensa,https://www.linkedin.com/company/lensa/life,Senior Data Engineer (Contract),https://www.linkedin.com/jobs/view/4260847157,4260847157,"Los Angeles, CA",On-site,$102.5K/yr - $227.7K/yr,2025-07-02 14:08:43,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for UCLA Health.

Description

As a Data Engineer on the Data Architecture team, you will play a pivotal role in advancing health informatics and analytics in the health sciences. Your efforts will focus on enhancing the usability, performance, and architecture of the Data Infrastructure. You will design and develop reliable, large-scale data processing pipelines, foundational architectural components, reusable frameworks, and data models to support enterprise-level data ecosystems such as Data Warehouses, Data Lakes, Feature Stores, and Machine Learning Platforms.

This role demands strong technical expertise in planning, designing, developing, implementing, and managing data-based systems to acquire, prepare, store, and make data and metadata accessible. You will optimize and maintain these systems, migrate data as needed, and ensure the integrity and completeness of both data and workflows.

As an information architect and data steward, you will design systems, data products, and data production processes, with a strong emphasis on data curation, exchange, security, and integrity. You will also evaluate and refine frameworks, strategies, and standards. This work may include contributions to project-level repositories, centers, or archives.

Key Responsibilities

Design and implement foundational data infrastructure to support the reporting, analytics, and data science needs of UCLA Health. Build data pipelines, training datasets, and inference datasets to enable AI, Machine Learning (ML), and Generative AI use cases. Develop efficient, scalable, and resilient pipelines to ingest data from multiple external sources. Create Data Architecture artifacts, including documentation, data quality metrics, logs, and observability tools for pipelines. Collaborate with cross-functional teams, including product management, analysts, and business stakeholders, to deliver scalable and usable datasets. Lead high-priority projects by formulating strategies and overseeing policies, processes, and resources. Elevate the technical expertise of the team through training, workshops, and presentations, fostering a culture of continuous improvement. Represent the Data Architecture team in cross-functional meetings to assess data needs and propose innovative design solutions. Work closely with the Manager of Data Architecture to develop and execute the technical and growth roadmap for the team. Establish and maintain information documentation standards, such as data naming conventions and value domain descriptions. 

This flexible hybrid role allows for a blend of remote and on-site work, requiring presence on-site at least once a month or as needed based on operational requirements. Please note, travel to the 'home office' location is not reimbursed. Each employee will complete a FlexWork Agreement with their manager to outline expectations and ensure mutual understanding. These arrangements are periodically reviewed and may be adjusted or terminated as necessary.

Salary offers are based on a variety of factors including qualifications, experience, and internal equity. The full salary range for this position is $102,500 - 227,700 annually. The University anticipates offering a salary between the minimum and midpoint of this range.

This is a three year contract. Contract appointments may convert to Career appointments.

Qualifications

Minimum three years of software development experience. 5+ years' experience on the data or backend systems side of the software development. Strong industry experience in programming languages such as Python with the ability to pick up new languages and technologies quickly. Experience with any code-based Orchestration tools like Airflow or Luigi is required. Strong experience with Relational databases like SQL Server or Oracle is required. Strong background in Data warehousing and ETL principles, architecture, and its implementation in large environments. Experience working with Machine Learning Systems like Databricks, Feature Stores, MLOps is strongly preferred. Working knowledge on leading cloud platforms like Azure, AWS, GCP; Microsoft Azure experience is preferred. Bachelor's degree in computer science, Computer Engineering, or related field from an accredited college or university; Master's Degree preferred. 

UCLA Health welcomes all individuals, without regard to race, sex, sexual orientation, gender identity, religion, national origin or disabilities, and we proudly look to each person’s unique achievements and experiences to further set us apart.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/cf4f14b27b2b4e33b53132c437935f43tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
72,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer I, AGI-DS Customer Analytics",https://www.linkedin.com/jobs/view/4260847426,4260847426,"Bellevue, WA",On-site,$91.2K/yr - $185K/yr,2025-07-02 14:08:19,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Amazon.

Description

Data engineer will develop end-to-end analytics pipelines and build a world-class data platform, deploying scalable business intelligence tools for the AGI team. The ideal candidate should demonstrate expertise in handling large-scale data operations and complex technical environments while maintaining strong business partnerships to drive strategic impact. The candidate will manage complete lifecycle from requirements gathering through implementation of interactive dashboards and reports. They will architect and implement large-scale data warehouse solutions utilizing SQL, Distributed/MPP data storage, Hadoop ecosystem, and AWS services. The ideal candidate will design robust ETL processes and data models to support business intelligence initiatives, while partnering with stakeholders to identify strategic opportunities where enhanced data infrastructure creates significant business value. They will support complex technical projects while maintaining attention to detail in fast-paced environments. The candidate will develop and maintain scalable BI solutions and interactive dashboards for data-driven decision making, leveraging expertise in SQL, data modeling, ETL design, AWS services, Hadoop, and dashboard tools like QuickSight.""

Key job responsibilities

 Design, implement, and support a platform providing ad hoc access to large datasets Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, Oracle, Redshift, and OLAP technologies Model data and metadata for ad hoc and pre-built reporting Interface with business customers, gathering requirements and delivering complete reporting solutions Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark. Build and deliver high quality datasets to support business analyst and customer reporting needs. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers Participate in strategic & tactical planning discussions, including annual budget processes

About The Team

AGI-DS holds a key position in the Generative intelligence space by being the provider of ground truth data for training the models that generate AI models.

Basic Qualifications

1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) 

Preferred Qualifications

Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. 

Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.

Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits . This position will remain posted until filled. Applicants should apply via our internal or external career site.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/c634e08330044c50b2f140bb50e51c2dtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
73,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Amazon Smart Vehicles, Amazon Smart Vehicles",https://www.linkedin.com/jobs/view/4260846478,4260846478,"Sunnyvale, CA",On-site,$91.2K/yr - $185K/yr,2025-07-02 14:08:25,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Amazon.

Description

Are you excited by the opportunity to design tools and infrastructure needed to analyze large volumes of data? Do you love bringing data together from diverse systems and working on innovative analytics problems for creating deep understanding of customer behavior and generating actionable business insights?

The Amazon Smart Vehicles organization is seeking a self-driven Data Engineer to join our team. In this role, you will be building complex data engineering and business intelligence applications using AWS technologies to support our growing smart vehicle business.

As a Data Engineer, you will be working in one of the world's largest and most complex data warehouse environments. You will design, implement and support scalable data infrastructure solutions to integrate with multi-heterogeneous data sources, aggregate and retrieve data in a fast and safe mode, curate data that can be used in reporting, analysis, machine learning models and ad-hoc data requests. You will be exposed to cutting-edge AWS big data technologies.

Key job responsibilities

Design and develop the pipelines required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Python, and AWS big data technologies. Oversee and continually improve production operations, including optimizing data delivery, re-designing infrastructure for greater scalability, code deployments, bug fixes and overall release management and coordination. Establish and maintain best practices for the design, development and support of data integration solutions, including documentation. Work closely with Product teams, Data Scientists, Software developers and Business Intelligence Engineers to explore new data sources and deliver the data. Read, write, and debug data processing and orchestration code written in Python/Scala following best coding standards. 

Basic Qualifications

1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets Bachelor's degree 

Preferred Qualifications

Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions 

Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.

Los Angeles County applicants: Job duties for this position include: work safely and cooperatively with other employees, supervisors, and staff; adhere to standards of excellence despite stressful conditions; communicate effectively and respectfully with employees, supervisors, and staff to ensure exceptional customer service; and follow all federal, state, and local laws and Company policies. Criminal history may have a direct, adverse, and negative relationship with some of the material job duties of this position. These include the duties and responsibilities listed above, as well as the abilities to adhere to company policies, exercise sound judgment, effectively manage stress and work safely and respectfully with others, exhibit trustworthiness and professionalism, and safeguard business operations and the Company’s reputation. Pursuant to the Los Angeles County Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits . This position will remain posted until filled. Applicants should apply via our internal or external career site.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/34058fc904a34b0db7c7edc02238f8a8tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
74,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4260840942,4260840942,"Santa Fe, NM",On-site,$225.6K/yr - $235.4K/yr,2025-07-02 14:08:49,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Bachelor's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and 3 years of work experience in the job offered or a computer-related occupation. Requires 3 years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SOL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesMapReduce or MPP systemPython
Public Compensation

$225,590/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/6f143c925e034cbb9600c75c63c15f5etjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,6
14,Joinrs US,https://www.linkedin.com/company/joinrs-us/life,[Full Remote] Data Engineer ,https://www.linkedin.com/jobs/view/4260824233,4260824233,United States,Remote,,2025-07-02 11:21:34,False,,"This position is in the team of Comcast, leading company in platforms for publishers, advertisers, and media buyers.
The summary of the opportunity from Joinrs AI: Comcast Advertising seeks a full-time Data Engineer II with a bachelor’s in a technical field and experience in SQL, Spark SQL, Databricks, Jupyter, Pyspark, and AWS. You will transform and analyze large data sets, develop ETL jobs, and optimize data systems. They offer comprehensive benefits for your physical, financial, and emotional well-being.
The selection process will be fully managed by Comcast. --
Job Description
DUTIESTransform large and complex data into consumable business databases and applications for self-service analytics and reportingPerform data analysis using SQL and Spark SQL queriesDevelop ETL jobs using Databricks, Jupyter, and PySparkAutomate ETL jobs using AWS services, including S3, Athena, Lambda, RDS, Glue, and EMRPerform data quality checks and exploratory data analysis on large datasets using PySparkCreate system architecture, design, and specifications using in-depth engineering skills and knowledge to solve complex development problems and achieve engineering goalsDetermine and source appropriate data for a given analysisWork with data modelers and analysts to understand business problems and create or augment data assets to support analysisPerform engineering tasks including database design, data manipulation, ETL, implementation, information storage and retrieval, data flow, and analysisResolve technical issues utilizing knowledge of policies and processesIdentify and react to system notifications and logs to ensure quality standards for databases and applicationsSolve abstract problems beyond a single development language or situation by reusing existing data files and flagsSolve critical issues and present trends, aggregate data, and quantify volume regarding specific data sourcesParticipate in the implementation of solutions via data architecture, data engineering, or data manipulation within big data systems, including Hadoop and SQLCollaborate with business owners and technical associates to ensure data collected and processed is actionable and relevant to end goalsDetermine solutions for data storage, optimization, and organizationDetermine table relationships and how fields interact within tables to develop relational modelsCollaborate with technology and platform management partners to optimize data sourcing and processing rules to ensure appropriate data quality
Note: Position is eligible for 100% remote work.

REQUIREMENTSBachelor’s degree (or foreign equivalent) in Computer Science, Engineering, or a related technical fieldOne (1) year of experience in:Performing data analysis using SQL and Spark SQL queriesDeveloping ETL jobs using Databricks, Jupyter, and PySparkAutomating ETL jobs using AWS services, including S3, Athena, Lambda, RDS, Glue, and EMRPerforming data quality checks and exploratory data analysis on large datasets using PySpark
SkillsAWS LambdaData QualityPySparkSpark SQL

Disclaimer:This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not intended to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this position.",https://www.joinrs.com/job-offers/437329/?utm_source=linkedin&utm_medium=job-offer-us&utm_campaign=437329-scraped,6
70,Insight Global,https://www.linkedin.com/company/insight-global/life,Power BI Developer,https://www.linkedin.com/jobs/view/4259198785,4259198785,"Washington, United States",Remote,Up to $71/hr,2025-07-02 13:59:45,True,over 100 applicants,"Contract Power BI Developer – Healthcare Data Transformation Project
 Location: Remote (U.S. based) Duration: Through December 2025 (potential extension into 2026) Industry: Healthcare IT
We’re seeking experienced Power BI Developers to join a strategic initiative with a large healthcare system on the West Coast. This high-impact project involves retiring Tableau and migrating nearly 1,000 reports to Power BI by the end of 2025.
Key Responsibilities:Support a major Tableau-to-Power BI migration effort, including analysis, restructuring, development, and dashboard creationAnalyze current clinical, financial, and operational reports to identify opportunities for consolidation or subdivisionBuild, test, and validate Power BI reports for business units including clinical leadership, finance, and operationsCollaborate with cross-functional teams in an Agile project environmentWrite and optimize queries using SQL and SnowSQL
Must-Have Qualifications:7+ years of experience with Power BI development and report migrationsHands-on experience migrating from Tableau to Power BI (within the past 3 years)Strong skills in Power BI dataset and data model designProfessional background working in healthcare analytics environmentsProficient in SQL and SnowSQL scripting
Nice-to-Have Qualifications:Bachelor’s degree in Computer Science or related fieldSnowflake SQL CertificationPower BI Developer or Designer CertificationExperience working in Agile delivery teams
This is an excellent opportunity to contribute to a meaningful transformation in healthcare reporting and analytics. If you're passionate about Power BI and data-driven decision-making in the healthcare space, we’d love to connect.",https://www.linkedin.com/jobs/view/4259198785,6
78,MANTECH,https://www.linkedin.com/company/mantech/life,Data Engineer,https://www.linkedin.com/jobs/view/4257140746,4257140746,"Ashburn, VA",On-site,,2025-07-02 08:45:58,False,,"ManTech seeks a motivated, career- and customer-oriented Subject Matter Expert Data Engineer to join our innovative team in Ashburn, VA. This is a hybrid position with 2 days onsite and 3 days remote.

Each day U.S. Customs and Border Protection (CBP) oversees the massive flow of people, capital, and products that enter and depart the United States via air, land, sea, and cyberspace. The volume and complexity of both physical and virtual border crossings require the application of “big data” solutions to promote efficient trade and travel. Further, effective solutions help CBP ensure the legal, safe, and secure movement of people, capital, and products. In response to this challenge, ManTech, as a trusted mission partner of CBP, seeks capable, qualified, and versatile Data Engineer subject matter experts (SMEs) to facilitate data-driven decision making in response to national security threats.

Responsibilities Include But Are Not Limited To

Responsible for data analysis of large database tables to understand the data structures, definitions, and patterns which will be used to support various predictive models.Work closely with the client to assist in managing data needs and supporting collection of data needed for various operational needs.Data analysis, problem solving, investigation and creative thinking with massive amount of data supporting variety of operational scenarios and responsible for source code control using GitLab.Implement cloud techniques and workflows (on-prem to cloud platforms).Respond to data queries/analysis requests from various groups within the organization. Create and publish regularly scheduled and/or ad hoc reports as needed.Research and document data definitions for all subject areas and primary datasets supporting the core business applications.Demonstrate a strong practical understanding of application-relevant cargo and passenger data and databases used to support analytic application development, functionality, and targeting end user (officer) operations.

Minimum Qualifications

HS Diploma/GED and 20+ years or AS/AA and 18+ years or BS/BA and 12+ years or MS/MA/MBA and 9+ years or PhD/Doctorate and 7+ yearsExperience in application development/full life cycle on Data Warehouse engagements and at least 5 years’ experience in large (80TB+) and complex data warehousing architecture, design and implementation/migrationExperience with one or more relational database systems such as Oracle, MySQL, Postgres, SQL server, etc. and experience in Extract-Transform-Load (ETL) development and knowledge of ETL concepts, tools, and data structuresExperience with cloud platforms like Amazon Web Services (AWS), Microsoft Azure, etc. and migrating customers/projects to the cloud and experience with one of the modern Cloud DW like Redshift, Databricks or BigQuery.Experience working in Unix/Linux environment and with shell scripting and scheduling CRON jobsExperience with Database query tuning and other performance enhancement methodology is a plus.Experience with NoSQL database (MongoDB or DynamoDB or DocumentDB).

Preferred Qualifications

Knowledge of Continuous Integration & Continuous Development tools (CI/CD).Ability to multitask efficiently and progressively and work comfortably in an ever-changing data environment.Ability to work well in a team environment as well as independently.Have excellent verbal/written communication and problem-solving skills and ability to communicate information to a variety of groups at different technical skill levels.Experience with relational databases and knowledge of query tools and/or BI tools like Power BI or Business Objects (BO) and data analysis tools.Experience with the Hadoop ecosystem, including HDFS, YARN, Hive, Pig, and batch-oriented and streaming distributed processing methods such as Spark, Kafka, or Storm.Experience with Atlassian suite of tools such as Jira and Confluence.

Clearance Requirements

Must be a U.S. citizen with the ability to obtain DHS CBP suitability prior to starting this position.

Physical Requirements

The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, or to communicate with co-workers, management, and customers, which may involve delivering presentations.",https://click.appcast.io/t/9C3T8PLSbx4jjsOyK1ytST_CRzEPaklH8EfySHNpiWwPn6h1EGfefG0KUiI2JCR8?source=LinkedIn,6
48,Foodsmart,https://www.linkedin.com/company/hellofoodsmart/life,Data Engineer,https://www.linkedin.com/jobs/view/4226046067,4226046067,United States,Remote,$130K/yr - $155K/yr,2025-06-25 14:45:23,False,,"About Us

Foodsmart is the leading telenutrition and foodcare solution, backed by a robust network of Registered Dietitians. Our platform is designed to foster healthier food choices, drive lasting behavior change, and deliver long-term health outcomes. Through our highly personalized, digital platform, we guide our 2.2 million members—including those in employer-sponsored health plans, regional and national Medicaid managed care organizations, Medicare Advantage plans, and commercial insurers—on a tailored journey to eating well while saving time and money.

Foodsmart seamlessly integrates dietary assessments and nutrition counseling with online food ordering and cost-effective meal planning for the entire family, optimizing ingredients both at home and on the go. We partner with national and regional retailers across the U.S., many of whom accept SNAP/EBT, making healthier food more accessible. Additionally, we assist members with SNAP enrollment and management, providing tangible access to nutritious food.In 2024, Foodsmart secured a $200 million investment from TPG’s Rise Fund, which supports entrepreneurs dedicated to achieving the United Nations’ Sustainable Development Goals. This investment will help us expand our reach, particularly to low-income workers who are disproportionately affected by diet-related diseases.

At Foodsmart, Our Mission Is To Make Nutritious Food Accessible And Affordable For Everyone, Regardless Of Economic Status. We Are Committed To a Set Of Core Values That Shape Our Culture And Work Environment

Measured: We make data-driven, truth-seeking decisions.

Impactful: We are fueled by achieving our mission and vision.

Collaborative: We help each other be better and create a positive environment.

Hungry: We maintain a healthy growth mindset, seeking to overcome challenges with courage.

Joyful: We take joy in each other, our work, and the privilege of doing this work.

Whether you're a dietitian, a commercial leader, or a technologist, working at Foodsmart means being part of a team that is passionate, supportive, and driven by a shared purpose. Join us in transforming the way people access and enjoy healthy food.

About The Role

The Data Engineer is a critical role responsible for constructing and optimizing our data pipeline architecture, collaborating closely with data scientists and analysts to facilitate data-related functionalities. The Data Engineer will be pivotal in designing, building, and maintaining highly scalable data pipelines, optimizing data delivery, and automating data processes. They will work closely with cross-functional teams to ensure efficient data flow and contribute to the success of our data-driven initiatives.

You Will

Own the optimization of data delivery for various cross-functional teams.Design, construct, install, test, and maintain highly scalable data pipelines.Collaborate closely with data architects, data scientists, and analysts to fulfill data requirements.Develop automated data processes for cleaning, validation, correction, and data mining.Identify, implement, and enhance internal process improvements, automating manual processes, and enhancing scalability.

You Are

Proactive and act as a driving force for efficient data delivery and infrastructure.Focused on quality and approach every data-related project with enthusiasm.Diligent in ensuring secure and compliant handling of data in accordance with relevant regulations.Collaborative and adept at addressing data-related technical issues and supporting stakeholders' data infrastructure needs.An expert in data warehouse architecture, data modeling, and automated data pipelines.

You Have

A minimum of 2 years of experience in a Data Engineering role.Hands-on experience with data warehouse solutions such as Snowflake or RedshiftExperience with cloud platforms such as AWS, GCP, or AzureAdvanced SQL knowledge and proficiency in working with relational databases.Familiarity with data pipeline and workflow management tools like Apache Airflow or Luigi.Strong analytical skills and the ability to thrive in a fast-paced environment.Familiarity with healthcare data standards like FHIR and HL7 is advantageous but not mandatory.Bachelor’s degree in Computer Science, Engineering, Mathematics, or related field; Master’s degree is a plus.

$130,000 - $154,999 a year

Role: Senior Data Engineer

Location: Remote

Base Salary Range: $130,000/yr to $155,000/yr + equity + benefits

Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries at our headquarters in San Francisco, California. Individual pay is determined by work location, job-related skills, experience, and relevant education or training.

About Our Benefits And Perks

Remote-First Company

Unlimited PTO

Healthcare Coverage (Medical, Dental, Vision)

401k, bonus, & stock options

Gym reimbursement

Foodsmart is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other protected class.",https://jobs.lever.co/foodsmart/3864c246-5eda-45e9-92e9-f6d804fa96b0/apply?source=LinkedIn,13
44,Aspirion ,https://www.linkedin.com/company/aspirion-health-resources-llc/life,Senior Azure SQL Data Engineer,https://www.linkedin.com/jobs/view/4250056733,4250056733,"Atlanta, GA",Remote,,2025-06-13 02:24:17,False,,"Position Overview:

Aspirion is seeking a detail-oriented and performance-driven Senior Azure SQL Data Engineer to join our growing Data Engineering team. This role will focus on designing, building, and optimizing scalable data infrastructure and services that support real-time business operations and enable advanced analytics, including AI/ML initiatives. The engineer will be responsible for driving excellence in database architecture, SQL performance tuning, and Azure resource management, while also contributing to the buildout of our Azure Lakehouse and Microsoft Fabric environment.

Key Responsibilities:

 Database Engineering & Optimization

– Write and optimize advanced T-SQL queries, stored procedures, views, and triggers.

– Analyze execution plans, tune indexes, and resolve deadlocks to maximize performance in OLTP environments.

– Design scalable and reliable schema changes for transactional and analytical systems.

 Azure SQL Infrastructure

– Provision, configure, and manage Azure SQL resources (Azure SQL Database, Managed Instances, SQL on Azure VMs).

– Implement capacity planning for compute and storage across OLTP and OLAP environments.

 Lakehouse Architecture & Fabric Integration

– Support and enhance Azure Data Lakehouse environments using Microsoft Fabric and OneLake.

– Develop ELT pipelines using Data Factory and Synapse Analytics.

– Integrate data governance practices using Microsoft Purview.

 AI & BI Readiness

– Design and prepare data layers optimized for AI/ML use cases and business intelligence consumption.

– Collaborate with analysts and data scientists to ensure performant and accessible datasets.

 Monitoring & Maintenance

– Implement proactive monitoring, logging, and alerting for SQL and data pipeline operations.

– Participate in incident response and continuous improvement of data reliability and performance.

Required Skills and Experience:

 Core SQL Expertise

– Advanced T-SQL development (queries, procedures, triggers)

– OLTP performance analysis and optimization

– Execution plan review, index tuning, deadlock resolution

 Azure Cloud Experience

– Hands-on experience with Azure SQL (all service models)

– Capacity planning for Azure SQL workloads

– Familiarity with Azure networking, access, and security for data services

 Data Engineering Tools

– Proficient with Azure Data Factory, Synapse Analytics, and Data Lake Storage

– Exposure to Microsoft Fabric and OneLake environments

– Understanding of Delta Lake and parquet formats

 Collaboration and Communication

– Proven experience working with cross-functional teams including BI, AI/ML, and DevOps

– Strong documentation and communication skills

 Experience

– 7+ years in data engineering with a focus on SQL and cloud data platforms

– at least 4 years of hands-on Azure SQL development and deployment

Preferred Qualifications:

 Microsoft Certified: Azure Data Engineer Associate or related Azure certifications Experience with Microsoft Fabric or early adoption of Lakehouse platforms Familiarity with data governance and cataloging via Microsoft Purview Exposure to tools supporting AI/ML pipelines in Azure

Educational Requirements:

 Bachelor’s degree in computer science, Data Engineering, Information Systems, or equivalent experience

About Aspirion:

For over two decades, Aspirion has delivered market-leading revenue cycle services, specializing in complex reimbursement challenges. With over 1,400 teammates, our culture is rooted in innovation, excellence, and a shared mission to deliver exceptional outcomes for our healthcare clients. We embrace flexibility, personal growth, and the intelligent use of technology—especially AI—to drive results.

Benefits:

 Competitive compensation and performance-based incentives Health, dental, vision, and life insurance from day one 401(k) with company match Flexible work environment (remote/hybrid) Professional development and certification support Mission-driven, people-first culture",https://aspirion-health-resources-llc.primepay-recruit.com/job/930067/senior-azure-sql-data-engineer?d=2025-06-12+19%3A36%3A54+UTC&s=lif,25
21,Millennium Software and Staffing,https://www.linkedin.com/company/millennium-software-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4248175921,4248175921,"Cincinnati, OH",Remote,,2025-06-12 17:02:30,True,over 100 applicants,"Hi Professional,
We have a W2 job opportunity, and we are currently looking for Data Engineer ,
Job Description: Requirements:• PL/SQL• Python• AWS Redshift",https://www.linkedin.com/jobs/view/4248175921,25
87,Analytica,https://www.linkedin.com/company/analytica-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4246666534,4246666534,United States,Remote,$120K/yr - $130K/yr,2025-06-07 17:52:51,False,,"Analytica is seeking a Data Engineer to support a complex data program for the Defense Health Agency. This role will work closely with reporting and analytics developers, data governance and data architects to build robust, high quality data pipelines that enhance productivity and operational efficiency. This position is for US Citizens only and candidate will hold (or be able to hold) a US Secret Clearance. The position is a remote position with occasional onsite meetings in at San Antonio, TX as required.
Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.
Responsibilities: Support the Military Health System (MHS) by supporting data engineering that enhance the Military Health System's (MHS) ability to leverage data as a strategic assetEnsure data interoperability, governance, and quality to support enterprise-wide decision-making and healthcare optimization play a critical role in designing and implementing data architecture frameworks that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset.Design and build robust and scalable data pipelines for managing structured and unstructured data using traditional databases (Oracle, PostgreSQL, etc.) or cloud Databases such as Amazon Redshift or AWS VerticaTranslate business needs into:data architecture solutions development within supported data systems.data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teamsMonitor and troubleshoot data import, analysis, and display errors.
Required Qualifications: Bachelor’s degree in information technology, Computer Science, Engineering or equivalent technical field3+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred)2+ years experience with Python and SQL (Java and Python preferred)Experience working on relational NoSQL and SQL databasesExperience designing and implementing various data pipeline patterns and strategiesStrong knowledge of data security principlesPrior experience with DHA M2 or MDR databases, CERNER EHR a strong plusMust be a US Citizen and must possess or be eligible for a SECRET clearance
About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

Analytica LLC is an Equal Opportunity Employer. We are committed to providing equal employment opportunities to all individuals, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other characteristic protected by applicable federal, state, or local law. As a federal contractor, we comply with the Vietnam Era Veterans' Readjustment Assistance Act (VEVRAA) and take affirmative action to employ and advance in employment qualified protected veterans.We ensure that all employment decisions are based on merit, qualifications, and business needs. We prohibit discrimination and harassment of any kind. Analytica LLC also provides reasonable accommodations to applicants and employees with disabilities, in accordance with applicable lawsWhen receiving email communication from Analytica, please ensure that the email domain is analytica.net to verify its authenticity.",https://analyticallc.applytojob.com/apply/gSgWiMyGlM/Data-Engineer,30
