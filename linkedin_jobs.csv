company,company_url,job_title,job_url,job_id,location,work_type,salary,posted_at,is_easy_apply,applicant_count,description,apply_url
Ascendion,https://www.linkedin.com/company/ascendion/life,Lead AWS Data Engineer,https://www.linkedin.com/jobs/view/4255214198,4255214198,"Seattle, WA",On-site,$120K/yr - $170K/yr,2025-06-23 17:14:18,True,over 100 applicants,"About Ascendion:Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.
Ascendion | Engineering to elevate life:We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:Build the coolest tech for world’s leading brandsSolve complex problems - and learn new skillsExperience the power of transforming digital engineering for Fortune 500 clientsMaster your craft with leading training programs and hands-on experience
Experience a community of change-makers!
Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:Job Title: Lead AWS Data Engineer
ResponsibilitiesDevelop, optimize, and maintain ETL pipelines using Glue.Build and manage data workflows and orchestration using Managed Airflow (MWAA).Write and optimize SQL and Python/PySpark scripts for data transformation and processing.Work with data platforms such as Redshift and S3 for efficient data storage and retrieval.Implement data cataloging and governance using Glue and Lake Formation.Support reporting and analytics teams by ensuring data availability for QuickSight and Tableau.Assist in automating infrastructure deployment using AWS CDK and Build Tools.Monitor, troubleshoot, and optimize data pipelines for performance and reliability.Collaborate with stakeholders to understand data requirements and business objectives.
Minimum Qualifications:Bachelor’s degree in Computer Science, Data Engineering, or a related field.7+ years of experience in data engineering, ETL development, or similar roles.Proficiency in AWS data technologies, including AWS Glue, Redshift, S3, and Lake Formation.Strong programming skills in SQL, Python, and PySpark.Experience with data pipeline orchestration using Managed Airflow (MWAA).Familiarity with data modeling, warehousing concepts, and performance tuning.Ability to work in an Agile development environment.Strong analytical and problem-solving skills.
Preferred Qualifications:Experience with AWS CDK and Build Tools for infrastructure automation.Knowledge of data security, governance, and compliance best practices.AWS certifications (e.g., AWS Certified Data Analytics, AWS Certified Solutions Architect) are a plus.
Location: 100% Onsite in Seattle, WA
Salary Range: The salary for this position is between $120K/year- $170K/Year. Factors that may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.
Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertain to the City/ State] [10-15 day of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]
Want to change the world? Let us know.
Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!",https://www.linkedin.com/job-apply/4255214198
Jacobs Levy Equity Management,https://www.linkedin.com/company/jacobs-levy-equity-management/life,Quantitative Data Engineer,https://www.linkedin.com/jobs/view/4255306437,4255306437,"Florham Park, NJ",Hybrid,,2025-06-23 18:47:50,True,over 100 applicants,"Jacobs Levy Equity Management, an institutional asset manager located in Florham Park, NJ, is seeking a motivated Quantitative Data Engineer to design and implement our proprietary quantitative investment systems.
Be part of a team that leads our future. You will be a key player on the Technology team and will research, design, code, test and deploy projects. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas.
Responsibilities Include:Implement, enhance, and manage quantitative models for equity marketsDesign and improve proprietary data repository and financial data platforms Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors Develop and manage reporting and performance analytics platforms Work closely with quantitative researchers on data requests and model development
Position Qualifications:MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentialsStrong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ dataBroad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server2+ years of solid coding experience in Python, Julia, C++, C#Experience in processing large and complex datasetsAn advanced knowledge of math and statisticsStrong communication skills and ability to collaborate across teams",https://www.linkedin.com/job-apply/4255306437
Design Manager,https://www.linkedin.com/company/designmanager/life,Data Engineer,https://www.linkedin.com/jobs/view/4253332182,4253332182,New York City Metropolitan Area,Hybrid,$125K/yr - $135K/yr,2025-06-23 22:20:06,True,over 100 applicants,"About Design Manager 
Design Manager (+DesignSpec) is a leading provider of project management and accounting software tailored specifically for interior design firms. For over 30 years, we’ve helped thousands of designers run more efficient, profitable businesses with our cloud-based platform that integrates purchasing, budgeting, time tracking, and client communication in one seamless solution. As we grow, we're investing heavily in data infrastructure to enhance product capabilities and improve client outcomes.
About the Role 
We’re looking for a skilled Data Engineer to join our product and technology team. As a Data Engineer, you’ll be responsible for building and maintaining data pipelines, integrating diverse data sources, and supporting our analytics and business intelligence efforts. You'll play a critical role in shaping how we collect, manage, and use data across the organization.
This is a hybrid role based in our office, with 3 days per week in-office required for collaboration and team interaction.
What You’ll Do 
● Design, build, and maintain scalable data pipelines and ETL processes 
● Collaborate with engineering and product teams to understand data needs 
● Develop data models and schemas for analytics and reporting 
● Monitor, troubleshoot, and optimize data pipelines for performance and reliability 
● Support the design and implementation of a modern data warehouse architecture 
● Ensure data quality, governance, and security best practices are followed 
● Work with third-party APIs and internal tools to extract, transform, and load data 
Qualifications ● 3+ years of experience in data engineering or a related field 
● Proficiency in SQL and experience with Python or another scripting language 
● Knowledge of analytics platforms such as Mode or Power BI 
● Hands-on experience with modern data platforms (e.g., Snowflake, BigQuery, Redshift) 
● Experience with ETL tools (e.g., Airflow, dbt, Fivetran) and data pipeline orchestration 
● Strong understanding of data warehousing concepts and data modeling 
● Excellent problem-solving and communication skills 
Analyze large datasets to identify trends, patterns, and actionable insights that support business decision-making.Build and maintain dashboards, reports, and visualizations to communicate key performance metrics to stakeholders.Work cross-functionally with teams (e.g., finance, operations, marketing) to define data needs and solve business problems using data.Clean, validate, and structure raw data from various sources to ensure data quality and usability.

Nice to Have 
● Experience in SaaS or B2B software environments 
● Exposure to finance or accounting 
Demonstrate an entrepreneurial mindset and self-starter attitude, as this role will be the first and only dedicated data analytics position in the company.
Why Join Us 
● Competitive base salary: $125K–$135K 
● Hybrid work flexibility with 3 in-office days 
● Opportunity to make a major impact on a growing SaaS business ● Collaborative and mission-driven team culture 
● Health, dental, and vision benefits, plus 401(k) 
● Paid time off, holidays, and parental leave",https://www.linkedin.com/job-apply/4253332182
Aditi Consulting,https://www.linkedin.com/company/aditiconsulting/life,Data Engineer II,https://www.linkedin.com/jobs/view/4255306515,4255306515,"Frisco, TX",On-site,$66/hr - $68/hr,2025-06-23 19:05:44,True,over 100 applicants,"Payrate: $66.00 - $68.00/hr.
 
Responsibilities:Develop various facets of data capture, data processing, storage and distributionUnderstand and apply AWS standard methodologies and products (compute, storage, databases)Translate marketing concepts/requirements into functional specificationsWrite clean, maintainable and well-tested codePropose new ways of doing things and contribute to the system architectureManage ETL data to and from Group entities to third party solutionsCreate and maintain functional utilities (SPAs) that provide point solutions to scale our marketing operationsDevelop scalable and highly-performant distributed systems with everything this entails (availability, monitoring, resiliency)Communicate and document solutions and design decisionsWork with business collaborators, analytics, and senior leadership to define and scope solutions that the marketing team can leverage to efficiently scale out our marketing operations 
Qualifications:Bachelor’s degree; or equivalent related professional experience5+ years of development experience, particularly in using marketing acquisition technologies to deliver automation of multiple channels and drive operational efficiencies4+ years’ experience with programming languages such as PHP, Python or Java stackExperience building data pipelines from multiple sources including APIs, CSV, event streams, NoSQL, etc. using distributed data frameworksExperience with different aspects of data systems including database design, data ingestion, data modeling, unit testing, performance optimization, SQL etcDemonstrable history creating on and leveraging AWSExperience in batch and/or stream processing (using Spark) and streaming systems/queues such as Kafka or SQSDaily practice of agile methods including use of sprints, backlog, user storiesExperience with AWS ecosystem or other big data technologies such as EC2, S3, Redshift, Batch, AppFlowAWS: EC2, S3, Lambda, DynamoDB, Cassandra, SQLHadoop, Hive, HDFS, Spark, other big data technologiesUnderstand, Analyze, design, develop, as well as implement RESTful services and APIs 
Pay Transparency: The typical base pay for this role across the U.S. is: $66.00 - $68.00 /hr. Final offer amounts, within the base pay set forth above, are determined by factors including your relevant skills, education and experience and the benefits package you select. Full-time employees are eligible to select from different benefits packages. Packages may include medical, dental, and vision benefits, 10 paid days off, 401(k) plan participation, commuter benefits and life and disability insurance.
 
For information about our collection, use, and disclosure of applicant's personal information as well as applicants' rights over their personal information, please see our Privacy Policy (https://www.aditiconsulting.com/privacy-policy).
 
Aditi Consulting LLC uses AI technology to engage candidates during the sourcing process. AI technology is used to gather data only and does not replace human-based decision making in employment decisions. By applying for this position, you agree to Aditi’s use of AI technology, including calls from an AI Voice Recruiter.
 
#AditiConsulting",https://www.linkedin.com/job-apply/4255306515
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4255156626,4255156626,"Austin, TX",On-site,$220.7K/yr - $235.4K/yr,2025-06-23 14:09:51,False,,"Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/14bbb95cbaef4341af3c94b8d332a0c6tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,AWS Data Engineer,https://www.linkedin.com/jobs/view/4255235099,4255235099,United States,Remote,$50/hr,2025-06-23 20:09:16,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Devfi, is seeking the following. Apply via Dice today!

Job Title: Data Engineer

Duration: 6+ Months

Exp: 12+ Years

Pay Rate: $50/hr

W2 Only!!

Remote!! 

Job Description:

A tech engineer with Python/AWS experience who can design payloads for processing to Grid.Perform Data Mapping, Data Transformations and then build prototypes for Payload processing.The actual implementation and development will be done by the IT of the client.Project is related to Futures/Debt / Derivatives trading analyticsData will be in Numerix (Financial Asset / risk management) platform system - extract and send the data to the grid (where we run massive models for calculating positions etc.)They re likely to use either proto buffer or Avro for data payloads.Python/AWSSomeone with 5 to 7 years experience with strong communication skillsHands on coding and development experienceThe financial services industry is a must - within finance preferred loan, structured assets, mortgage back securities data familiarity - understand the terminologies like Swap rate, maturity rate, date , index, days of conventions (business days, calendar days), instrument type etc. for data mapping and data transformations

Thanks,

Naveen S",https://click.appcast.io/t/Vcsfbz69BmapKZLFIDKnnYU1OzT_LtiXANh9F-MVa_g=
Apexon,https://www.linkedin.com/company/apexon/life,Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4255205477,4255205477,"Carmichael, CA",Hybrid,,2025-06-23 15:55:03,True,over 100 applicants,"Company Description:Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
Experience/Qualifications:Experience working with data conversion (Microsoft to Snowflake) efforts involving legacy data formats and legacy data repositories such as Mainframe.Experience in developing and monitoring data pipelines, ETL/ELT (Extract Transform Load / Extract.Experience in database design, database development, and database security.Experience in Container Platforms such as Kubernetes, OpenShift, Docker, or in Cloud Computing.Experience with AWS Glue (ETL), S3 & Snowflake.Experience developing Rest APIs, including experience with JSON.
Education: Any Bachelor’s/ Master degree in IT or relevant experience.
Disclaimer:If you feel that this is a good match for your skillsets, please submit a current word version of your resume along with a cover letter describing your skills, experience and salary expectations. We are an Equal Opportunity Employer (EOE).",https://www.linkedin.com/job-apply/4255205477
Pismo,https://www.linkedin.com/company/pismo/life,Sr Data Engineer,https://www.linkedin.com/jobs/view/4255319383,4255319383,NAMER,Remote,,2025-06-23 19:45:50,False,,"Summary

The Data Platform team at Pismo is responsible for integrating data produced by Pismo's Banking as a Service/Payments platform with its clients, whether through batch processing (files) or near real-time (events). With a robust, low-latency, and 100% cloud-based architecture, the team excels in

technology and fosters a strong culture of collaboration, partnership, and data-driven decision making for platform development and project prioritization.

What You'll Do


Serve as a senior technical contributor on Pismo’s data stack, leading the design and development of robust, scalable data pipelines using modern Big Data frameworks.Drive continuous improvement in platform performance, reliability, and cost-effectiveness, proactively identifying opportunities for innovation.Take ownership of production operations and monitoring (file- and event-based systems), ensuring adherence to SLAs and applying best practices for observability and resilience.Collaborate with cross-functional teams to architect, build, and evolve high-complexity data features, reducing technical debt and championing engineering excellence.Provide mentorship to peers through code reviews, feedback, and support, fostering an environment of knowledge sharing and professional development.Participate in on-call rotations, promptly addressing and resolving production incidents, while proactively implementing preventative measures.Build and optimize ETL/ELT processes to handle large-scale data, leveraging advanced data processing and distributed systems techniques.Maintain rigorous data governance and platform standards, working closely with data stakeholders to ensure consistent, high-quality data assets.Introduce and adopt new technologies, tools, or techniques that accelerate development cycles, improve performance, and reduce costs.Conduct root-cause analyses for critical issues, challenging and refining team processes to enhance efficiency and reliability.Influence multiple teams by promoting a culture of technical excellence, forward-thinking solutions, and continuous improvement.


Minimum Qualifications

Technical Skills:


3+ Years experience with Golang 3+ Years experience with AWS Services ( Lambda, SQS, S3 )2+ Years experience with TerraformSoftware Engineering experience ( building steaming applications / data platforms )


Desirable Qualifications


Python experience 


Pismo is an Equal Employment Opportunity employer that proudly pursues and hires a diverse workforce. Pismo does not make hiring or employment decisions on the basis of race, color, religion or religious belief, ethnic or national origin, nationality, sex, gender, gender identity, sexual orientation, disability, age or any other basis protected by applicable laws or prohibited by company policy. Pismo also strives for a healthy and safe workplace and strictly prohibits harassment of any kind.",https://job-boards.greenhouse.io/pismo/jobs/5569811004
Franklin Fitch,https://www.linkedin.com/company/franklin-fitch/life,Azure Data Engineer,https://www.linkedin.com/jobs/view/4255157034,4255157034,"Lawrenceville, GA",Hybrid,$140K/yr - $170K/yr,2025-06-23 13:46:24,True,over 100 applicants,"Senior Data Engineer | Hybrid | $130,000 – $170,000 | Lawrenceville, GA
Are you a data engineering expert who thrives on designing scalable data systems and optimizing pipelines that power enterprise-level analytics? Do you enjoy solving complex data challenges, mentoring teams, and building high-impact infrastructure in a collaborative, fast-paced environment? If you're looking for a role with real ownership, technical depth, and cross-functional influence, this Senior Data Engineer opportunity could be exactly what you’re looking for.
We’re partnered with a growing, mission-driven technology organization seeking a Senior Data Engineer to architect and enhance its next-generation data platform. This is a hands-on leadership role where you’ll shape data strategy, build robust pipelines, and ensure the integrity, security, and efficiency of enterprise data solutions across cloud environments.
Key Responsibilities:
Architecture and Engineering
Architect and implement scalable, cost-efficient data models and schemas across various storage platforms.Optimize batch and near real-time data pipelines to ensure performance, resilience, and quality.Own the technical vision and influence best practices in ETL/ELT workflows, CI/CD, and cloud-native data architecture.
Collaboration and Mentorship
Collaborate with cross-functional stakeholders to understand data needs and design fit-for-purpose solutions.Mentor junior engineers and contribute to building a high-performing, inclusive team culture.Document systems, processes, and governance policies to drive transparency and maintainability.
Governance and Compliance
Implement robust data governance and security frameworks, ensuring compliance with regulatory standards.Monitor system health, troubleshoot performance issues, and proactively improve data infrastructure.
Key Skills & Qualifications:
Required:
Bachelor’s degree in Computer Science, Engineering, or a related field (Master’s preferred).5+ years of experience in data engineering roles.Strong programming expertise in Python and/or Java.Deep proficiency with SQL and modern database systems.Expert-level experience in big data technologies such as Apache Spark and Hadoop.Hands-on experience with cloud platforms (Azure) and tools like Snowflake, Redshift, or BigQuery.Proficiency with version control (Git) and CI/CD best practices.Strong problem-solving skills and the ability to work with ambiguous requirements.
Preferred:
Experience with stream processing platforms like Kafka or Flink.Background in the hospitality industry or other customer-centric verticals.Proven ability to communicate complex technical topics to executive audiences.Familiarity with data modeling, ETL design patterns, and secure software development principles.
We’re actively scheduling interviews, apply now to secure your spot and take the next step in your data engineering journey!
Not quite the right fit? Explore other exciting opportunities on our careers page:
https://www.franklinfitch.com/us/",https://www.linkedin.com/job-apply/4255157034
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Senior AWS Data Engineer,https://www.linkedin.com/jobs/view/4255231845,4255231845,United States,Remote,,2025-06-23 20:09:12,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, eGrove Systems Corporation, is seeking the following. Apply via Dice today!

Looking for Senior AWS Data Engineer 

Role is 100% Remote

Job Description -

100% Interview & Offer!!- We had interview on Friday and he was unable to qualify, Need excellent communicator!!

Note- I really need a rockstar with a seasoned linkedin. Excellent communication skills required. I need a native citizen for the next one . A Joe or Bob. Python programming and tableau is important. 

Interview got rejected and they are open to take Native without certification. Please recruiter true AWS Data Engineer. 

Senior AWS Data Engineer

Location- 100% Remote- EST hours

6+ months

Client- PSEG

Key notes- Senior AWS Data Engineer who is very strong in Python programming. Must have s3, Redshift, Tableau, strong API's, NICE experience, data curation, Data ingestion etc. 

Feedback for some rejected candidates- They passed on all other candidates. THis was the feedback from the last candidate. The other AWS Engineer Candidates I don t get the sense that this candidate has the breadth of experience we would need in an AWS Data Engineer. For example, he has AWS Cloud Practioner certificate that is a very basic certification and infact most of us who are not even technical have one. For an AWS Data Engineer, I would expect more.

MUST HAVE AN CERTIFICATE OF AWS DATA ENGINEER

Summary- Looking Senior level - full stack AWS Data developer/Engineer who can pull, extract the data and build pipelines from NICE CXone application to ingest data into PSEGLI AWS Data Lake environment which is data analytical platform of PSEG. Expert in backend API integration experience

Note- NICE CXone is a cloud-based contact center platform that helps businesses manage and improve customer interactions across various channels

Key skills required- Must have experience of pulling, extracting data and building pipelines NICE application platform.

Required Skills & Experience- Total- 10+ years experience required

Minimum 6- 7 years of experience with below skill set.

Expert in backend API integration experience

Python Programming

Strong API's

Tableau

Building Pipelines

Data Ingestion

Data Curation

Data Transformation

Data Lake",https://click.appcast.io/t/DOrrXSY7CQdJmEwFFxRNzfbeGeTC4a8fHDj7uKyVHkQ=
WayUp,https://www.linkedin.com/company/wayup/life,"Johnson & Johnson, Data Engineer - Application via WayUp",https://www.linkedin.com/jobs/view/4255335559,4255335559,United States,Remote,$124K/yr,2025-06-23 21:01:46,False,,"This role is with Johnson & Johnson. WayUp is partnering with Johnson & Johnson to hire top talent.

At Johnson & Johnson, we believe health is everything. Our strength in healthcare innovation empowers us to build a world where complex diseases are prevented, treated, and cured, where treatments are smarter and less invasive, and solutions are personal. Through our expertise in Innovative Medicine and MedTech, we are uniquely positioned to innovate across the full spectrum of healthcare solutions today to deliver the breakthroughs of tomorrow, and profoundly impact health for humanity. Learn more at https://www.jnj.com

Job Function: Data Analytics & Computational SciencesJob Sub Function: Data EngineeringJob Category: Scientific/TechnologyAll Job Posting Locations: Raynham, Massachusetts, United States of America, Warsaw, Indiana, United States of America

 Job Description: 

Role: Data EngineerLocation: Remote At Johnson & Johnson, we believe health is everything. Our strength in healthcare innovation empowers us to build a world where complex diseases are prevented, treated, and cured, where treatments are smarter and less invasive, and solutions are personal. Through our expertise in Innovative Medicine and MedTech, we are uniquely positioned to innovate across the full spectrum of healthcare solutions today to deliver the breakthroughs of tomorrow, and profoundly impact health for humanity. Learn more at [https://www.jnj.com/. ](https://www.jnj.com/.%E2%80%AF) Job Summary We are seeking a skilled and motivated Data Engineer to join our team. The ideal candidate will have a strong background in data engineering, data architecture, and data integration. This role involves building and maintaining scalable data pipelines, ensuring data quality and integrity, and supporting data analytics initiatives. You will work closely with internal customers, product owners, core team members, and other stakeholders to ensure that quality data is available, reliable, and easily accessible in a cost effective efficient, scalable way.

 Key Responsibilities: 

Data Pipeline Development: Design, build, and maintain robust, scalable, and efficient data pipelines to collect, process, and store large volumes of data from various sources.Data Integration: Integrate data from multiple sources, including APIs, databases, and external datasets, ensuring data consistency and reliability.Data Modelling: Develop and maintain data models and schemas that support efficient data storage, retrieval, and analytics.Database Management: Manage and optimize databases, ensuring their performance, availability, and security.Data Quality: Implement and monitor data quality checks to ensure the accuracy, completeness, and consistency of data. Perform analysis required to troubleshoot data related issues and assist in the resolution of data issues.Automation: configure data extraction and load jobs and automate repetitive tasks and processes to improve efficiency and reduce errors. Improve performances of jobs and pipelines by applying optimisation and automation techniques.Documentation: Maintain clear and comprehensive documentation of data pipelines, data models, and data integration processes. Maintenance of time allocation reports.Develop understanding of business/ processes and high-level understanding of high-quality digital product deliveryWays of working: Follow Agile and SDLC processes including the creation of data related user stories. Perform code peer reviews and testing as per SDLC or industry standards.Collaboration: Work closely with internal customers, product owners, various core team members, and other stakeholders to understand their data needs and provide appropriate solutions. Technical collaboration and oversight with Vendors as required. Create SOWs and work packets with vendors as required. Education: Bachelor’s or master’s degree in computer science, data science, software engineering, information systems or a similar field or equivalent experience.Experience and skills:Proven experience (>6 years) as a Data Engineer or in a similar role.Experience with data pipeline, ETL and workflow management tools (e.g., Databricks, Data Factory).Proficiency in programming languages such as SQL, Python, R, or Scala.Substantial knowledge and experience with Python libraries such as PySpark and Pandas.Strong experience with SQL and database management (e.g., MySQL, PostgreSQL, SQL Server).Excellent problem-solving and analytical skills.Strong understanding of data architecture, data modelling, and ETL processes.Ability to work in a fast-paced, dynamic environment and manage multiple tasks simultaneously.Strong communication and collaboration skills.Experience with best practice UX / UI design & development of Business Intelligence data visualisation / exploration tools (Power Bi, Qlik etc)Familiarity with SDLC and Agile ways of working.Familiarity with DevOps tools and practices (e.g. Jenkins, Azure DevOps, etc).Familiarity with Atlassian tools like Bitbucket, JIRA, Confluence. Preferred: GxP and Non GxP SDLC experienceKnowledge of data governance and data security best practices.MS Power Platform / power apps experienceCloud DevOps knowledge (e.g. Terraform)Certifications in cloud data technologies (e.g. Azure Data Fundamentals)Basic data science experience and knowledge. Other: English language required.May require travel 10-20% of time based on project locations. This is an opportunity to work with a groundbreaking medical device operation and to be a member of a Johnson & Johnson company, with an excellent record in employee continuous professional development and business improvement. We are passionate about our work; we play vital roles across a range of professional disciplines and care deeply about our customers and communities. At Johnson & Johnson our culture enables dynamic and impactful careers. Whether you’re one of the 1000 people who work here, or you’re considering joining the team, we offer:An opportunity to be part of a global market leader.A dynamic and inspiring working environment.Opportunities to work on challenging projects and assignments.Possibilities for further personal and professional development/education Excellent Benefits \The anticipated base pay range for this position is : 77-124,000USD

_Additional Description for Pay Transparency:_

",https://www.wayup.com/i-Hospital-and-Health-Care-j-Data-Engineer-Johnson-Johnson-890295496052564/?utm_source=linkedin-xml&utm_medium=jobxml&utm_campaign=linkedin-XML-APPS-253256-36527952&refer=lnkslot-APPS-253256-36527952
Lensa,https://www.linkedin.com/company/lensa/life,Junior Data Engineer,https://www.linkedin.com/jobs/view/4255159537,4255159537,"McLean, VA",On-site,,2025-06-23 14:09:25,False,,"Lensa partners with DirectEmployers to promote this job for Infinitive.

About Infinitive

Infinitive is a data and AI consultancy that enables its clients to modernize, monetize and operationalize their data to create lasting and substantial value. We possess deep industry and technology expertise to drive and sustain adoption of new capabilities. We match our people and personalities to our clients' culture while bringing the right mix of talent and skills to enable high return on investment.

Infinitive has been named “Best Small Firms to Work For” by Consulting Magazine 7 times most recently in 2024. Infinitive has also been named a Washington Post “Top Workplace”, Washington Business Journal “Best Places to Work”, and Virginia Business “Best Places to Work.”

Job Summary

We are seeking a motivated Junior Data Engineer to support our data engineering initiatives by building and maintaining scalable data pipelines on AWS. The ideal candidate has 1–2 years of hands-on experience with Python, PySpark, and cloud-based data platforms, and is eager to grow their technical skills in a fast-paced, collaborative environment.

Key Responsibilities

Develop, test, and maintain data pipelines using Python and PySpark Ingest, transform, and clean structured and semi-structured data from multiple sources Collaborate with senior data engineers and analysts to support data infrastructure needs Deploy and monitor data workflows on AWS using services like S3, Lambda, Glue, and EMR Document processes, data models, and pipeline logic clearly for stakeholders Troubleshoot data issues and assist in performance tuning of jobs and queries 

Required Qualifications

Bachelor’s degree in Computer Science, Data Engineering, Information Systems, or related field 1–2 years of hands-on experience with Python and PySpark Working knowledge of AWS cloud services (e.g., S3, Lambda, Glue, EMR) Proficiency with SQL for querying and data manipulation Basic understanding of data warehousing concepts and ETL processes Familiarity with version control systems like Git 

Preferred Qualifications

Experience with workflow orchestration tools like Apache Airflow or AWS Step Functions Exposure to data cataloging and metadata management tools Understanding of data lake and data warehouse architectures (e.g., Redshift, Snowflake, BigQuery) Ability to work in Agile development environments Strong problem-solving and communication skills 

Powered by JazzHR

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/742c7a43c033408f8f19366920577e75tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Visionaire Partners,https://www.linkedin.com/company/visionaire-partners/life,Data Engineer,https://www.linkedin.com/jobs/view/4255089413,4255089413,"Atlanta, GA",Hybrid,$110K/yr - $115K/yr,2025-06-23 14:25:06,True,over 100 applicants,"Data EngineerExciting opportunity for a skilled Data Engineer to design, build, and maintain reliable data infrastructure for our Business Intelligence team. Strong experience with complex data systems and databases like SQL Server or Oracle is required.
RESPONSIBILITIES:Design, construct, install, and maintain scalable data infrastructure and processes, including data warehouses and data lakes.Collaborate with cross-functional teams to drive data-driven decision-making and understand complex data requirements to deliver relevant data sets and solutions.Develop and implement data integration and delivery solutions to consolidate data from various sources and ensure efficient data flow.Identify, design, and implement internal process improvements such as infrastructure redesign for scalability, optimizing data delivery, and automating manual processes.Provide technical support to stakeholders for data-related issues, including troubleshooting applications and interfaces that access data.Ensure data quality and integrity through rigorous testing, validation, cleansing, and error handlingImplement data security and privacy measures that comply with industry standards and regulations.Stay current with industry trends and technologies to continuously improve data engineering practices, provide insight into development methodologies, and adapt to existing processes.This is a direct hire position working out of the Atlanta office, 3 days onsite and 2 days remote. This is an excellent opportunity for growth at a company with excellent culture. Visionaire Partners offers all full-time W2 contractors a comprehensive benefits package for the contractor, their spouses/domestic partners, and dependents. Options include 401k with up to 4% match, medical, dental, vision, life insurance, short and long term disability, critical illness, hospital indemnity, accident coverage, and both Medical and Dependent Care Flexible Spending Accounts.",https://www.linkedin.com/job-apply/4255089413
Hire Our Heroes Veteran Job Board,https://www.linkedin.com/company/hireourheroesveteranjobboard/life,Data Engineer,https://www.linkedin.com/jobs/view/4255127684,4255127684,"Carrollton, TX",On-site,,2025-06-23 10:34:15,False,,"Graviton Solutions Inc

Carrollton, TX

Seeks applicants w/ exp'd Masters or equiv for multiple positions.. Apply if you have skills in any of Oracle Database, SQL Server or MySQL. Travel &/or reloc to unanticipated job sites in US is needed. Send resume to HR, 2410 Luna Rd, Ste 232, Carrollton, TX 75006

recblid wsoyfr48ordcikufin4j03xhxvlsde",https://hireourheroes.com/job/15522648/data-engineer/
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Senior AWS Cloud Data Engineer - SnowFlake,https://www.linkedin.com/jobs/view/4255235151,4255235151,"Boston, MA",Hybrid,,2025-06-23 20:09:13,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, EVEREST CONSULTING GROUP, INC, is seeking the following. Apply via Dice today!

Role:

Senior Cloud Data Engineer

Employment Type: Contract

Contract Minimum 6 months subject to extension

Work location: Remote/Onsite (mention no. of Onsite days)

Hybrid/ ..2 days from Boston Office or Plano Tx, or Indianapolis or Portsmouth

About The Role

for a Senior Cloud Data Engineer. Developer .This position will be a highly skilled, senior member of the team requiring deep technical expertise in SQL, Snowflake, AWS, Power BI ,Data Warehouse Design, and CI/CD pipelines, with the ability to work independently and proactively in a client-facing environment.

Job Responsibilities

SQL Expertise: Demonstrate expert-level proficiency in SQL for querying and managing data, including the use of complex joins, subqueries, window functions, and performance tuning techniques.

Snowflake Proficiency: Utilize advanced capabilities of the Snowflake Cloud Data Platform, including Snowflake SQL, schema design, Snowpipe, streams/tasks, and integration with other systems.

Data Warehousing & Modelling: Apply strong knowledge of dimensional modelling techniques (e.g., star and snowflake schemas), design of fact and dimension tables, and implementation of ETL/ELT frameworks aligned with data architecture best practices.

CI/CD & GitHub Actions: Use Git and GitHub for version control and implement GitHub Actions to automate CI/CD pipelines, including testing, deployment workflows, and environment management.

Power BI Reporting: Develop and publish data visualizations and dashboards using Microsoft Power BI, supporting both operational and strategic reporting needs.

Python Scripting: Write Python scripts for automation and data manipulation tasks, leveraging libraries such as pandas for lightweight data transformations when required.

DevOps & Tooling Familiarity: Demonstrate working knowledge of DevOps tools and practices, including containerization with Docker, monitoring tools, and relevant IDEs or project management platforms to ensure efficient and reliable delivery.

Collaborates with the Product Owner, Scrum Master, Subject Matter Experts and Development team to define and analyze user stories tracked in Jira.

Demonstrate solutions and articulates business value to business partners at sprint showcases.

Actively participate in investigating platform and perform end to end testing if needed.

Prior experience working on Agile scrum teams in a scaled framework is preferred.

Develop and maintain professional relationships with all customers.

Assist all customers and provide development support for all Applications and perform tests on all installation process for infrastructure.

Perform all tests on production/dev applications and prepare recovery procedures for all applications and provide upgrade to same.

Experience in Investment Domain

Good knowledge of batch processing

Required Education

Masters or Engineering degree

Required Experience

Experience: Minimum of 13 years of experience in development with 7+ years of experience in data engineering, with a proven track record of designing, building, and optimizing scalable data pipelines and architecturesExpert-level proficiency in SQL and strong experience in data transformation, automation, and orchestrationDeep understanding of data modeling, data warehousing, and data architecture best practicesHands-on Experience with modern public cloud-based data platforms Snowflake (preferable), AWS, AzureAdvanced proficiency in AWS services including (but not limited to): S3, Glue, Lambda, EC2, and AthenaProficient with ETL/ELT data pipelines, patterns for loading Data Warehouses, LakesSolid experience with DevOps practices and automation frameworks, including CI/CD pipelines, GitHub Actions, Bamboo, and pipeline-as-code principlesSkilled in BI/reporting tools (e.g., Power BI, Tableau) Tools/TechnologiesSQL: Expert use of SQL for querying and managing data, including complex joins, subqueries, window functions, and performance tuning.Snowflake: Advanced use of Snowflake Cloud Data Platform for data warehousing, including Snowflake SQL, schema design, Snowpipe, streams/tasks, and integration capabilities.Data Warehousing & Modelling: Dimensional modeling techniques, star and snowflake schemas, fact and dimension table design, ETL/ELT frameworks, and general data architecture best practices.GitHub Actions & CI/CD: Git and GitHub for version control, with GitHub Actions for automating CI/CD pipelines (testing, deployment workflows, environment management).Power BI: Microsoft Power BI for creating data visualizations and dashboards, basic report development and publishing processes.Python: Scripting with Python for automation and data manipulation tasks (using libraries like pandas for simple transforms, if required).DevOps/Other Tools: Familiarity with development and DevOps tools such as Docker (for containerizing data tools), monitoring tools, and any relevant IDEs or project management tools to facilitate efficient delivery (optional, as needed per project).Bonus skills:Previous experience in Investments / Asset Management, Finance Data modeling, is highly desirable.Snowflake or AWS certificationCognizance of security concerns, from access control and authentication to secured processes.A comprehensive understanding of agile environments and the ability to adapt to rapidly changing circumstances.

Work-shift Timings (If required)

9 to 5 PM EST",https://click.appcast.io/t/GN72JirLBGCgawv_VtlGp2YfohSlEpv2nRPW1JFhl3M=
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer,https://www.linkedin.com/jobs/view/4255157383,4255157383,"Austin, TX",On-site,$177.4K/yr - $196.9K/yr,2025-06-23 14:10:14,False,,"Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer Responsibilities:

Design, build, and launch data pipelines to move data across systems and build the next generation of data tools that generate business insights for a product. Analyze user needs and software requirements to determine workability and to offer support for end users on data usage. Design, architect, and develop software and data solutions that help product and business teams make data-driven decisions. Rethink and influence strategy and roadmap for building efficient data solutions and scalable data warehouse plans. Design, develop, test and launch new data models and processes into production, and provide support. Leverage homegrown extract, transform, and load (ETL) framework as well as off-the-shelf ETL tools, as appropriate. Interface closely with data infrastructure, product, and engineering teams to build and extend cross platform ETL and reports generation framework. Identify data infrastructure issues and drive to resolution. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires Master’s degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Analytics, Statistics, Mathematics, Physics, or a related field and 3 years of experience in the job offered or in a related occupation. Requires 36 months of experience in the following: Internet technologies including HTMLAnalyzing large volumes of data to provide data driven insights, gaps, and inconsistenciesData warehousing architecture and plansDimensional data modelingSchema Design
Public Compensation

$177,406/year to $196,900/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/9e90c566b35d453fa50e872c5b4ef13ctjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Lensa,https://www.linkedin.com/company/lensa/life,Remote Data Engineer,https://www.linkedin.com/jobs/view/4255155945,4255155945,"Irving, TX",Remote,,2025-06-23 14:09:30,False,,"Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

Insight Global is seeking 2 Data Engineers to join a Fortune 100 Healthcare organization and work remotely in CST / EST hours. These data engineers will be responsible for the curation of an internally built API system through JSON parsing in a Snowflake environment. Whenever services are called for the internal API system, the JSON logs are captured in a single field in a Snowflake table. These data engineers will be responsible for parsing those JSON files, curating them, loading them into LOB specific tables within Snowflake, then making it available for mass consumption while tagging each individual JSON piece.

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

7+ years of experience as a Data Engineer Extensive JSON parsing experience utilizing SQL in a Snowflake environment Experience with Azure as cloud platform Experience with Airflow as orchestration tool Strong communication skills Healthcare industry experience null 

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/6ff8d830dbf3480289e81bf277660d89tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,Data Engineer,https://www.linkedin.com/jobs/view/4255194746,4255194746,"McLean, VA",Hybrid,,2025-06-23 17:29:27,False,,"Lead Data Engineer

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Lead Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do:Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:Bachelor's DegreeAt least 4 years of experience in application development (Internship experience does not apply)At least 2 years of experience in big data technologiesAt least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)
Preferred Qualifications:7+ years of experience in application development including Python, SQL, Scala, or Java4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)4+ year experience working on real-time data and streaming applications4+ years of experience with NoSQL implementation (Mongo, Cassandra)4+ years of data warehousing experience (Redshift or Snowflake)4+ years of experience with UNIX/Linux including basic commands and shell scripting2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).

The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
Cambridge, MA: $193,400 - $220,700 for Lead Data Engineer
McLean, VA: $193,400 - $220,700 for Lead Data Engineer
New York, NY: $211,000 - $240,800 for Lead Data Engineer
Richmond, VA: $175,800 - $200,700 for Lead Data Engineer







Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).PDN-9f38545f-50c9-4858-828d-85931d81ee11",https://tracking.prodivnet.com/track/apply/9f38545f-50c9-4858-828d-85931d81ee11
Mastech Digital,https://www.linkedin.com/company/mastech/life,Sr. Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4252690801,4252690801,"Sacramento, CA",Hybrid,,2025-06-23 15:52:56,True,over 100 applicants,"What you’ll do:Lead data engineering efforts migrating legacy Microsoft platforms to Snowflake.Utilize AWS Glue for ETL, and manage data storage with AWS S3 and Snowflake.Develop and monitor ETL/ELT pipelines, REST APIs, and database solutions.
Qualifications:3+ years with ETL pipelines and Microsoft SQL Server / T-SQL.3+ years working with ETL tools and REST API development (JSON, C#).Experience with database design, security, and legacy data conversion (e.g., Mainframe).Familiarity with container platforms (Kubernetes, Docker) or cloud computing.Proficient with Microsoft Visual Studio and Git.Experience with SaaS Cloud databases on AWS or Azure.Bachelor’s degree in relevant technical field.",https://www.linkedin.com/job-apply/4252690801
ON Data Staffing,https://www.linkedin.com/company/on-data-staffing/life,Lead Data Engineer - Databricks ,https://www.linkedin.com/jobs/view/4255348920,4255348920,United States,Remote,,2025-06-23 21:37:57,True,48 applicants,"We are seeking a dynamic and experienced Databricks professional to join our client, a global consultancy, as a Lead Data Engineer.
As Lead Data Engineer, you will play a pivotal role in driving the success of our client's Databricks practice through your expertise in presales, strong technical acumen, consulting skills, and effective team management.
Key Responsibilities:Presales Expertise:Leverage your in-depth understanding of Databricks to engage with clients in presales activities, providing comprehensive insights and solutions tailored to their unique needs.Collaborate with the sales team to create compelling proposals, presentations, and demonstrations that showcase the value of Databricks solutions.Technical Mastery:Possess a robust technical background in Databricks, enabling you to guide clients through complex technical challenges and articulate the benefits of adopting Databricks technologies.Stay current with industry trends and emerging technologies related to Databricks, ensuring our client's offerings remain innovative and competitive.Consulting Excellence:Lead and participate in consulting engagements, offering strategic guidance and best practices to clients implementing Databricks solutions.Work closely with clients to understand their business objectives and translate them into actionable plans, ensuring successful project outcomes.Team Management:Foster a collaborative and high-performance culture within the Databricks practice team.Provide mentorship, guidance, and support to team members, promoting continuous learning and development.Drive recruitment efforts to expand the team's capabilities and expertise.Qualifications:Proven experience in presales activities, with a focus on Databricks solutions.Strong technical background in Databricks and related technologies.Extensive experience in consulting, advising clients on data and analytics solutions.Demonstrated success in team management, fostering a positive and collaborative work environment.Excellent communication and interpersonal skills, with the ability to build strong client relationships.Education and Certifications:Bachelor's or higher degree in a relevant field.Relevant certifications in Databricks and related technologies are highly desirable.If you are a motivated and strategic leader with a passion for driving innovation in the realm of Databricks, we invite you to apply and be a key contributor to our client's dynamic and growing team.",https://www.linkedin.com/job-apply/4255348920
DataX Connect,https://www.linkedin.com/company/datax-connect/life,Cheif Engineer- Data Center,https://www.linkedin.com/jobs/view/4253306354,4253306354,"Broomfield, CO",On-site,$120K/yr - $140K/yr,2025-06-23 18:03:32,True,3 applicants,"Data Center Chief Engineer - Critical EnvironmentsBroomfield, CO – On Site
DataX Connect are partnered with a global real estate enterprise who are dedicated to making an impactful difference to the data center/ mission critical sector. 
The position Seeking a hands-on Chief Engineer to lead critical environment operations at a data center. This role ensures efficient, safe, and compliant facility performance while managing engineering staff and working closely with vendors, contractors, and tenants. 
Responsibilities • Manage operating and capital budgets; monitor expenses and justify variances • Supervise and schedule Operating Engineers; oversee timekeeping and payroll accuracy • Conduct monthly inspections; issue work orders and equipment replacements • Ensure compliance with fire, safety, and environmental standards • Oversee tenant construction and capital improvement projects • Implement and monitor energy efficiency programs • Manage service contracts, inventory, and vendor relationships • Maintain detailed logs and reports; lead emergency response and drills • Train staff and enforce property procedures and codes 
Qualifications and previous experience • Received EPA 608. • Trained in NFPA70E • Two-year technical degree in HVAC or related field. • Seven to ten years of engineering experience or equivalent industry experience • Three to five years supervisory experience or experience as an Assistant Chief Engineer. • Must possess a thorough knowledge of building HVAC, electrical, plumbing, automation, and life safety/fire protection systems. • The ability to read and understand drawings, specifications, and various measuring and testing devices is also required.
Why Apply • Join a fantastic group of experienced data center professionals. • Continuous growth throughout the business. • Competitive salary, benefits and bonus.",https://www.linkedin.com/job-apply/4253306354
The Planet Group,https://www.linkedin.com/company/the-planet-group/life,Sr. Data Engineer,https://www.linkedin.com/jobs/view/4255204506,4255204506,"Chicago, IL",On-site,$65/hr - $70/hr,2025-06-23 16:17:01,True,over 100 applicants,"Duration - 12 monthsLocation - 5 days onsite - no remoteW2 only 
We’re Hiring: Data Engineer | Cloud & Analytics | Financial ServicesJoin our client team modernizing IT and building a next-gen Data Platform for Asset Servicing & Portfolio Accounting. You’ll design and implement scalable, cloud-native data architectures, build pipelines for complex internal/external data, and support advanced analytics in a high-impact environment.
What You’ll Do: Design & build data pipelines and real-time processing solutionsDevelop scalable, secure, cloud-native applications (Azure & Snowflake)Lead data migration, enrichment & integration projectsCollaborate with stakeholders to deliver actionable insightsEnsure data quality, consistency, and governance
Must-Have Skills: Data Architecture, Data Pipelines, Real-Time ProcessingAzure Public Cloud & Snowflake Cloud developmentSQL, Python, and familiarity with Java or cloud-native tools Experience with DevOps, APIs, Docker, CI/CD Agile methodologies and secure SDLC practices
Nice to Have: Java, Spring, Maven, GIT, REST/SOAP APIsStrong communication & ability to translate business needs to techMassive modernization project, turning into a Microsoft-first environment—opportunity to work on cutting-edge cloud solutions in Financial Services.
Interested? Let’s connect!#TECH #AZUREDATAENGINEER #SNOWFLAKE",https://www.linkedin.com/job-apply/4255204506
Capitol Federal Savings,,Data Engineer,https://www.linkedin.com/jobs/view/4255238477,4255238477,"Overland Park, KS",On-site,,2025-06-23 20:19:26,False,,"RoleProvides support of our data warehouse environment and analytics tools. This allows our business leaders access to business intelligence data in order to develop strategies for our organization to grow. The Data Engineer assists in setting overall development roadmap and standards for the Bank and helps evaluate and architect the use of data solutions, using industry best practices. As part of this job, you will be working to implement a mixture of on premise and cloud strategies for the future of data processing at the bank.

Essential Duties & ResponsibilitiesResponsible for building and maintaining optimal data pipelines, architectures and data sets required for extraction, transformation, and loading of data from a wide variety of data sources using SQL and integration technologies.Leverage existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes.Build and utilize appropriate data platform structures to organize and store data in a particular manner.Work with big data technologies on-prem, cloud and hybrid. Implement data lifecycle management strategies around the flow of data within the organization implementing policy and automated approaches.Implement BI (business intelligence) platforms both on-premises and cloud.Establish governance and strategies around visualization of creating reports and dashboards to help improve upon operational and analytical reporting. Examine, assess, translate, and classify data; recognize, collect and analyze data to encourage the advancement, execution and application of data platform systems.Analyze data to spot anomalies, trends and correlate similar data sets.Design, develop and implement natural language processing software modules.Create models and standards to govern which data is collected, and how it is stored, arranged, integrated, and use in data systems and in organizations.Perform major tasks, deliverables, and formal application delivery methodologies; deliver new or enhanced applications. Perform additional database administrator duties, as assigned.Performs work independently.Considered a high-level specialist who regularly interacts and works with senior management.Provides leadership, coaching, and/or mentoring to a subordinate group.Helps evaluate and architect the use of data solutions. May act as a team lead.Perform other duties as assigned.Participate in proactive team efforts to achieve departmental and company goals.Must comply with current applicable laws, regulations and bank policies and procedures. Comply with all safety policies, practices and procedures. Report all unsafe activities to supervisor and/or Human Resources.
Knowledge & Skills 
ExperienceFive or more years' experience with ETL/ELT tools and development. Experience in a Data Warehouse (DWH) environment with data integration of large and complex data sets.Proficient in SQL and scripting languages (e.g., Python).Hands-on experience with Snowflake data warehouse, including performance tuning and optimization.Experience with cloud-based platforms such as AWS and Azure.Knowledge of business intelligence (BI) tools such as Sigma, Power BI, SSRS, or Crystal Reports.Strong understanding of data warehousing concepts, dimensional modeling, and data lake architecture.Experience with SQL Server and Oracle database management systems.Technical proficiency with relational databases: T-SQL, SQL, and stored procedures.
Education/Certifications/LicensesBachelor’s degree in Computer Science, Information Systems, or another related field required.
SkillsCreative thinking, with the ability to evaluate a business scenario and design user-friendly,low maintenance software solutions. Excellent written, oral, and interpersonal communication skills. Effective use of multiple reporting tools. Technical expertise. Analytical thinking skills. Problem-solving skills. Leadership skills. The ability to work in a team environment and motivate or influence others is a critical part of the job, requiring a significant level of diplomacy, influence and trust.
Physical RequirementsIs able to bend, sit, and stand in order to perform primarily sedentary work with limited physical exertion and occasional lifting of up to 10 lbs. Must be capable of climbing / descending stairs in an emergency situation. Must be able to operate routine office equipment including computer terminals and keyboards, telephones, copiers, facsimiles, and calculators. Must be able to routinely perform work on computer for an average of 6-8 hours per day, when necessary. Must be able to work extended hour or travel off site whenever required or requested by management.
Working ConditionsRegular in-office attendance required.
Mental and/or Emotional RequirementsMust be able to perform job functions independently or with limited supervision and work effectively either on own or as part of a team. Must be able to read and carry out various written instructions and follow oral instructions. Must be able to complete basic mathematical calculations, spell accurately, and understand computer basics. Must be able to speak clearly and deliver information in a logical and understandable sequence. Must be capable of dealing calmly and professionally with numerous different personalities from diverse cultures at various levels within and outside of the organization and demonstrate highest levels of customer service and discretion when dealing with the public. Must be able to perform responsibilities with composure under the stress of deadlines / requirements for extreme accuracy and quality and/or fast pace. Must be able to effectively handle multiple, simultaneous, and changing priorities. Must be capable of exercising highest level of discretion on both internal and external confidential matters.",https://capfed.wd1.myworkdayjobs.com/CapFed_Careers/job/Topeka-KS/Data-Scientist_R-101056?source=Linkedin
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Azure Data Engineer,https://www.linkedin.com/jobs/view/4255232736,4255232736,United States,Remote,,2025-06-23 20:09:12,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, ePace Technologies, Inc, is seeking the following. Apply via Dice today!

Title: Sr Azure Data Engineer

Key Skills: Databricks, ADF, Azure

Job Responsibilities

Works independently to process information by loading / extraction of data.

Data Modeling

Orchestrates data pipelines to extract, transform, and load CRM data and create templated data

Performs quality control functions on all deliverables, including standard deliverables, ad-hoc loading, and extraction projects.

Meets clients to establish, monitor, and review deliverables.

Qualifications - External

Ability to translate customer needs into technical requirements.

Good verbal and written communication skills

Detail-oriented and analytical with planning and organizational skills

Excellent interpersonal skills

Specific knowledge of client business and technology/data operations in the pharmaceutical industry

Skill in troubleshooting and problem-solving.

Ability to work in an environment that is driven by process governance

Azure, Databricks, SQL, ADF

Background Data Modeling

HR Background Strongly preferred",https://click.appcast.io/t/W9UE534P1IXa-w131JyufubEAG_ZETpB3y30z981IEQ=
Insight Global,https://www.linkedin.com/company/insight-global/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4255093852,4255093852,"Cleveland, OH",Hybrid,$55/hr - $65/hr,2025-06-23 15:04:35,True,over 100 applicants,"Duration: 6 month contract to hireLocation: open to remote for contract duration - MUST RELOCATE to Pittsburgh, PA or Cleveland, OH upon conversion to FTE.Pay Rate: $55-65/hrExact compensation may vary based on several factors, including skills, experience, and education.
Benefit packages for this role will start on the 1st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.
Must Haves:7+ years’ experience as a Data EngineerStrong experience with Snowflake from a data engineering perspective Experience building out SQL queriesExperience/knowledge with Azure Data Factory Experience working with the users to gather requirementsAzure AZ 900 Fundamentals Certification or be willing to take it  Plusses:Data Architecture experience/exposureExperience with OBIEE from an admin/support perspectiveExperience with QlikView from an admin/support perspectiveExperience with DataIQSupply Chain Application experience a plusUnderstanding of CRM ApplicationsExperience with Order Management processes a plus Day to Day:Insight Global is seeking a Senior Data Engineer to support a multi-national power management client of ours. This resource will be assisting with moving data to a Snowflake environment and assisting on various projects within the Sales Domain and order management systems (4 ERP systems). This resource will be expected to build out models in Snowflake. They will also be expected to work with the users to understand/gather requirements from. They will execute on Data Modeling within Power BI so users can build out dashboards from the models. Will be supporting a DataIQ platform to prepare data and analytics for different groups. They will lightly be supporting OBIEE & QlikView until they move this off onto new platforms.",https://www.linkedin.com/job-apply/4255093852
Paragon Alpha - Hedge Fund Talent Business,https://www.linkedin.com/company/paragonalpha/life,Data Engineer - Research Platform - Quant Hedge Fund - $325k ,https://www.linkedin.com/jobs/view/4255191442,4255191442,"New York, United States",Hybrid,$150K/yr - $250K/yr,2025-06-23 16:37:00,True,over 100 applicants,"Paragon are partnered with a systematic hedge fund trading quant equities and credit, who rely on data driven processes to inform their strategies. 
The role requires scaling and augmenting a research data platform, and helping to create a system that can integrate vast and disparate data-sets into one locations, allowing Quant Researchers to easily access.
Stack: Python, AWS, Airflow, ETL 
The team is partnering with tier one Quant Traders and Researchers, and would allow you to push yourself technically and professionally. They need an excellent technologist with a trading background, and some who can understand the data quants need and why. 
If you would like the chance to work with some of the worlds best technologists and investment professionals, then please do apply.",
"SIDMANS, INC.",https://www.linkedin.com/company/sidmans-inc./life,Bigdata Engineer,https://www.linkedin.com/jobs/view/4255255434,4255255434,"Richardson, TX",Hybrid,,2025-06-23 22:52:07,True,34 applicants,"Role: Bigdata EngineerLocation: Richardson, TX or Chicago, IL - Hybrid roleNeed locals Only.
Bigdata candidates must have 10+ yrs of professional experience with strong Hadoop, scala, hive and Control-M
If interested, please send me your updated resume at satyasakuru@sidmans.com",https://www.linkedin.com/job-apply/4255255434
Space Ocean Corp,https://www.linkedin.com/company/space-ocean-corp/life,Senior System Engineer,https://www.linkedin.com/jobs/view/4255252726,4255252726,United States,Remote,,2025-06-23 23:11:31,True,2 applicants,"Compensation in the form of shares in the company, restricted share units. There is no cash salary. Fractional ownership. 
5 or more hours per week at your own pace. 
Company Description Space Ocean is pioneering the indispensable logistics layer for space operations, solving critical challenges that enable a thriving space economy. Our mission focuses on enabling a thriving orbital economy through innovative logistics infrastructure, transformative investments in space infrastructure, and building a skilled team. We provide the foundational infrastructure that space-based businesses need to succeed. Space Ocean is actively seeking strategic investors, forward-thinking customers, and innovative partners to reshape the future of space logistics and sustainability. Join us as we build the essential infrastructure for humanity's expansion into space.
 Role Description This is a part-time remote role for a Senior System Engineer. The Senior System Engineer will be responsible for systems engineering, systems design, system administration, technical support, and troubleshooting. Day-to-day tasks include designing and optimizing systems, providing technical support, maintaining system integrity, and resolving technical issues promptly.
 Qualifications Experience in Systems Engineering and Systems DesignSkills in System Administration and Technical SupportStrong Troubleshooting abilitiesExcellent problem-solving and analytical skillsStrong communication and teamwork skillsAbility to work independently and remotelyBachelor's degree in Computer Science, Engineering, or related fieldExperience in the space industry is a plus",https://www.linkedin.com/job-apply/4255252726
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4255156517,4255156517,"Trenton, NJ",On-site,$220.7K/yr - $235.4K/yr,2025-06-23 14:10:02,False,,"Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/a529c252d8c74039bec39d653bfb5050tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
London Approach,https://www.linkedin.com/company/londonapproach/life,Lead Data Engineer,https://www.linkedin.com/jobs/view/4253322877,4253322877,"Malvern, PA",On-site,$120K/yr - $140K/yr,2025-06-23 21:35:21,True,47 applicants,"Full Time PermanentUp to $140,000 plus bonus and full benefitsOnsite in Malvern, PA Corporate HQ 4 days a week requiredNo Sponsorship, No Relocation
We’re seeking a Lead Data Engineer to architect and manage our next-generation data platform. In this role, you’ll lead a team of engineers and play a hands-on role in building scalable pipelines, enabling real-time insights, and designing data solutions that serve as the backbone of enterprise reporting and analytics. You’ll work closely with business and analytics teams to ensure the data platform supports dynamic, self-service capabilities and high-trust decision-making.
Job Qualifications:Advanced proficiency with Microsoft Fabric and its components (Data Factory, Synapse, Lakehouse, and Pipelines) is required.Strong experience with Power BI, including semantic model development, DAX, Power Query, and workspace governance.Proficient in Python for data transformation, workflow automation, and integration with enterprise data pipelines.Expertise in modern data architecture, including data lakes, lakehouse models, and ELT/ETL frameworks.Fluency in SQL for building efficient, scalable queries and optimizing data performance.Strong understanding of Azure services and CI/CD practices within a data engineering context.Proven track record of leading and mentoring engineering teams, with a focus on agile delivery and code quality standards.Excellent problem-solving and communication skills with the ability to partner across technical and non-technical teams.
Required Experience:Minimum 7 years in data engineering roles with demonstrated experience in designing and deploying data solutions using Microsoft Fabric.At least 2 years in a technical leadership capacity, overseeing a team of engineers and owning project execution.Hands-on experience delivering enterprise Power BI solutions, including model performance tuning, report optimization, and deployment to business units.Experience building and supporting governed, scalable data pipelines in a cloud-based (preferably Azure) environment.Background in implementing data quality, governance, and monitoring frameworks in high-availability systems.",https://www.linkedin.com/job-apply/4253322877
Infosys,https://www.linkedin.com/company/infosys/life,AWS and Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4253320735,4253320735,"Raleigh, NC",On-site,,2025-06-23 21:07:11,False,,"Job Description

Infosys is seeking an AWS and Snowflake Data Engineer. In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards. You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.

Required Qualifications

 Candidate must be located within commuting distance of Raleigh, NC or Richardson, TX or be willing to relocate to the area. This position may require travel in the US  Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.  All applicants authorized to work in the United States are encouraged to apply  At least 4 years of Information Technology experience  At least 2 years of AWS experience  At least 2 years of Snowflake experience  Experience in end-to-end implementation using technologies such as AWS and Snowflake,  Strong knowledge and hands-on experience in SQL, Unix shell scripting, and Python 

Preferred Qualifications

 Experience in Banking/Financial domain  Strong knowledge Data structures, Data Engineering concepts, Algorithms, Collections, Multi-threading and memory management and concurrency  Experience in large scale cloud data migrations using Snowflake, Python, Spark, SQL  Sound knowledge of software engineering design patterns and practices  Good understanding of Agile software development frameworks  Strong communication and Analytical skills  Good to have experience in Azure/AWS Databricks  Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams 

The job entails sitting as well as working at a computer for extended periods of time. Should be able to communicate by telephone, email or face to face.

About Us

Infosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.

Infosys provides equal employment opportunities to applicants and employees without regard to race; color; sex; gender identity; sexual orientation; religious practices and observances; national origin; pregnancy, childbirth, or related medical conditions; status as a protected veteran or spouse/family member of a protected veteran; or disability.",https://digitalcareers.infosys.com/infosys/global-careers/apply-aws-and-snowflake-data-engineer/469323?Codes=LinkedIn
Premier Inc.,https://www.linkedin.com/company/premierinc/life,Data Engineer (CoC),https://www.linkedin.com/jobs/view/4255330575,4255330575,United States,Remote,$80K/yr - $148K/yr,2025-06-23 20:31:12,False,,"What you will be doing:

The Data Engineer will manage the acquisition, storage, modeling, and presentation of data to ensure that it is available, accurate and easily accessible for a wide range of analytical use cases. You will design and build data pipelines in Microsoft Fabric, ingesting and transforming datasets into OneLake. You will write queries, merges, views, and filters across multiple data sources including Delta, Parquet, CSV, and SQL endpoints, leveraging T-SQL, KQL or Spark SQL to optimize performance. You will configure and optimize Dataverse connections to Dynamics 365 tables to enable robust automation. You will develop, review, and troubleshoot Power Automate flows and Copilot Studio solutions with a focus on error handling, performance, and security. Working closely with Power Platform developers, you will mentor them on data engineering concepts while documenting data models, integration patterns and automation design patterns for reuse across the organization. You will support and monitor your solutions by training teammates and users, responding swiftly to critical issues and proactively identifying risks with mitigation strategies. You will continuously optimize your solutions by leveraging the latest technologies, best practices, third-party tools, and resource rationalization.

Key Responsibilities

Responsibility #1– 35%

Solution Development

 Collaborate with business stakeholders, engineers, analysts, and other staff to define end-to-end data analytics solutions. Interact directly with stakeholders to define requirements and rapidly iterate through solution development. Model and structure data for efficient analysis. Leverage advanced features of the data and visualization/reporting layers to create effective solutions. Manage key aspects of data warehouses, including resource organization, access control, query performance, and data-efficiency. Support data cleansing/standardization/normalization and other data management operations.

Responsibility #2 – 25%

Troubleshooting, Support & Communication

 Establish tests and other mechanisms for quickly detecting issues with data integrity. Troubleshoot data and other technical issues, clearly communicating with IT leadership as well as key external stakeholders. Engage quickly with high priority issues. Review incident trends and proactively register problems. Plan and implement changes to address risks and inefficiencies. Train staff & clients (engineers, analysts, data scientists…) on fundamental data concepts such as SQL scripting, data modeling, database / query tuning, and revision control. Establish standards, document change procedures, coordinate planned maintenance, and communicate impacts to stakeholders.

Responsibility #3 – 25%

Solution Optimization (15%)

 Seek to minimize code & infrastructure under management. Leverage existing and/or external (e.g., third-party managed) solutions whenever appropriate. Drive rapid resolution of issues Implement solutions in a well-described manner (e.g., using good code and self-documentation practices) so that they are easily supported by / transitioned between peers. Seek to implement the simplest, most readable solution that will meet the business need. Maintain awareness of data engineering, business intelligence / analytics, and data science trends and technology developments. Identify and follow relevant thought leaders. Consult with DevOps teams to ensure implementations adhere to best practices for security, governance, IAM and monitoring. Support auditing of user access against defined policies and implementation of automated mitigation. Apply an “enterprise view” of the business environment and the business applications running on the systems being managed to provide a more seamless end user experience.

Responsibility #4 – 15%

Project & Time Management

 Take initiative in solving high-level problems. Breakdown large projects / milestones into components & increments that can be completed within sprints. Schedule meetings to drive progress, foster collaboration, and collect frequent feedback. Participate in Agile development structures/ceremonies such as regular standups, showcases, sprint planning sessions and retrospectives.

Required Qualifications

Work Experience:

Years of Applicable Experience - 2 or more years

Education:

High School Diploma or GED (Required)

Preferred Qualifications

Skills:

 Self-Direction: Ability to break down high-level problems into components and seek out solutions with a high degree of independence. Effective Communication: Effective at communicating (verbal and written) with peers, clients, and vendors across technical and non-technical backgrounds. Scripting: Proficient at writing code to extract, load, and transform data. Must have a deep understanding of SQL, a programming language, and command line commands. Data Workflow Orchestration and Project Management: Experience writing complicated workflow orchestrations using tools like Azure Dev Ops, JIRA, etc.

Experience:

Business Intelligence / Analytics / Data Science: Experience using BI tools (e.g., Tableau, Power BI, Looker), along with strong SQL skills (T-SQL, KQL, Spark SQL) for querying, merging, and filtering large datasets. Hands-on work with Microsoft Fabric and OneLake.Agile: Scrum / Kanban, regular standups, showcases, retrospectives, and experience using Agile project management software (e.g., Jira, Rally, Planner).DevOps: Facilitating promotion of configurations / code / data through test and production environments. Experience interacting with a repository and applying Gitflow concepts for revision control.Cost Management: Understanding of cloud cost drivers (query costs, compute hours / pipeline runtime, storage, licensing).Continuous Learning: Seeks to stay current with practices and technologies through publications, training, and conferences, with a focus on Power Platform capabilities, connectors, and Copilot Studio innovations.Dataverse & Dynamics 365: Solid experience connecting to Dataverse tables (Cases, Opportunities, Business at Risk) and automating flows.Power Platform Automation: Familiarity with Power Automate (cloud flows, desktop flows) and Copilot Studio for low-code and AI-driven solutions.

Education:

Data Modeling or Engineering certification

Additional Job Requirements:

Remain in a stationary position for prolonged periods of time Be adaptive and change priorities quickly; meet deadlines Attention to detail Operate computer programs and software Ability to communicate effectively with audiences in person and in electronic formats. Day-to-day contact with others (co-workers and/or the public) Making independent decisions Ability to work in a collaborative business environment in close quarters with peers and varying interruptions 

Working Conditions: Remote

Travel Requirements: No travel required

Physical Demands: Sedentary: Exerting up to 10 pounds of force occasionally, and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body. Sedentary work involves remaining stationary most of the time. Jobs are sedentary if movement is required only occasionally, and all other sedentary criteria are met.

Premier’s compensation philosophy is to ensure that compensation is reasonable, equitable, and competitive in order to attract and retain talented and highly skilled employees. Premier’s internal salary range for this role is $80,000 - $148,000. Final salary is dependent upon several market factors including, but not limited to, departmental budgets, internal equity, education, unique skills/experience, and geographic location. Premier utilizes a wide-range salary structure to allow base salary flexibility within our ranges.

Employees also receive access to the following benefits:

 Health, dental, vision, life and disability insurance 401k retirement program Paid time off Participation in Premier’s employee incentive plans Tuition reimbursement and professional development opportunities

Premier at a glance:

Ranked #1 on Charlotte’s Healthiest Employers list for 2019, 2020, 2022, and 2023 and 21st Healthiest Employer in America (2023) Named one of the World’s Most Ethical Companies® by Ethisphere® Institute for the 16th year in a rowModern Healthcare Best in Business Awards: Consultant - Healthcare Management (2024)The only company to be recognized by KLAS twice for Overall Healthcare Management Consulting

For a listing of all of our awards, please visit the Awards and Recognition section on our company website.

Employees receive:

Perks and discountsAccess to on-site and online exercise classes

Premier is looking for smart, agile individuals like you to help us transform the healthcare industry. Here you will find critical thinkers who have the freedom to make an impact. Colleagues who share your thirst to learn more and do things better. Teammates committed to improving the health of a nation. See why incredible challenges require incredible people.

Premier is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to unlawful discrimination because of their age, race, color, religion, national origin, ancestry, citizenship status, sex, sexual orientation, gender identity, gender expression, marital status, familial status, pregnancy status, genetic information, status as a victim of domestic violence, covered military or protected veteran status (e.g., status as a Vietnam Era veteran, disabled veteran, special disabled veteran, Armed Forces Serviced Medal veteran, recently separated veteran, or other protected veteran) disability, or any other applicable federal, state or local protected class, trait or status or that of persons with whom an applicant associates. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. In addition, as a federal contractor, Premier complies with government regulations, including affirmative action responsibilities, where they apply. EEO / AA / Disabled / Protected Veteran Employer.

Premier also provides reasonable accommodations to qualified individuals with a disability or those who have a sincerely held religious belief. If you need assistance in the application process, please reply to diversity_and_accommodations@premierinc.com or contact Premier Recruiting at 704.816.5200.

Information collected and processed as part of any job application you choose to submit to Premier is subject to Premier’s Privacy Policy.",https://premierinc.wd1.myworkdayjobs.com/External_Professional/job/Remote/Data-Engineer--CoC-_R0007713?source=LinkedIn
iHire,https://www.linkedin.com/company/ihire-llc/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4255077226,4255077226,"Longview, TX",On-site,,2025-06-23 13:20:43,False,,"Capital One has partnered with iHire to reach top talent for their opening below. Check it out and apply via iHireTechnology today!

Senior Data Engineer (Enterprise Platforms Technology)

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, youll have the opportunity to be on the forefront of driving a major transformation within Capital One.

Enterprise Platforms Technology (EPTech) comprises many of Capital Ones most important enterprise platforms. We play an essential role in establishing practices for building technology solutions across the company, while also delivering capabilities that exemplify those practices.

What Youll Do

 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies  Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems  Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake  Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community  Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment  Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance 

Basic Qualifications

 Bachelors Degree  At least 3 years of experience in application development (Internship experience does not apply)  At least 1 year of experience in big data technologies 

Preferred Qualifications

 5+ years of experience in application development including Python, Pyspark or Scala  2+ years of experience with AWS  3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)  2+ year experience working on real-time data and streaming applications  2+ years of experience with NoSQL implementation (Mongo, Cassandra)  2+ years of data warehousing experience (Redshift or Snowflake)  3+ years of experience with UNIX/Linux including basic commands and shell scripting  2+ years of experience with Agile engineering practices 

At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).

The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.

Plano, TX: $144,200 - $164,600 for Senior Data Engineer

Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidates offer letter.

This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.

Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.

This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York Citys Fair Chance Act; Philadelphias Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at [phone removed] or via email at [email removed] . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.

For technical support or questions about Capital One's recruiting process, please send an email to [email removed]

Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.

Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).

",https://www.ihiretechnology.com/ppc/dp/24/482363368?utm_campaign=programmatic&utm_medium=downpost&utm_term=2025-06-23&utm_content=&utm_source=linkedin&ih_linkedinjobtype=slot&ih_date=2025-06-23
Millennium Software and Staffing,https://www.linkedin.com/company/millennium-software-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4248175921,4248175921,"Cincinnati, OH",Remote,,2025-06-12 17:02:30,True,over 100 applicants,"Hi Professional,
We have a W2 job opportunity, and we are currently looking for Data Engineer ,
Job Description: Requirements:• PL/SQL• Python• AWS Redshift",https://www.linkedin.com/job-apply/4248175921
The Middleby Corporation,https://www.linkedin.com/company/middleby-corporation/life,Azure Data Engineer,https://www.linkedin.com/jobs/view/4255228607,4255228607,"Elgin, IL",On-site,,2025-06-23 20:01:09,False,,"Job Type

Full-time

Description

Middleby Corporation is a leading global manufacturer of commercial, residential kitchen, and food processing equipment. We are committed to innovation, efficiency, and delivering high-quality products to our customers worldwide. Our Digital Technology team is at the forefront of leveraging data and technology to drive business growth and operational excellence.

We are seeking a talented Azure focused Data Engineer to join our dynamic team at Middleby. In this role, you will be instrumental in building and optimizing our data infrastructure, focusing on large-scale data projects that integrate data from various sources across our diverse business. You will contribute to data-driven decision-making, including but not limited to advanced AI applications that leverage OpenAI's Retrieval-Augmented Generation (RAG) models. This position offers the opportunity to collaborate closely with cross-functional teams, including business stakeholders, data analysts, and DevOps software engineers.

Responsibilities:

Design & Implement Data Solutions: Develop and maintain scalable architectures using Azure Data Factory, Azure Synapse, Azure Databricks, Data Lake, and Azure SQL. Build & Manage Pipelines: Create robust ETL/ELT processes for data ingestion, transformation, and integration from multiple sources. Optimize Storage & Performance: Configure data storage (e.g., Azure Data Lake, Blob Storage), implement indexing and partitioning, and monitor system performance for cost-effective, high-speed data processing. Data Quality & Governance: Enforce data standards, ensure data accuracy, and maintain compliance with regulations (GDPR, HIPAA, etc.). Security & Compliance: Implement Azure Role-Based Access Control (RBAC), encryption, and other best practices to protect sensitive information. Continuous Integration & Deployment: Use Azure DevOps for version control, automated testing, and deployment of data pipelines and infrastructure. Troubleshooting & Maintenance: Monitor pipelines, resolve incidents, and perform root-cause analysis to ensure reliable data operations. Collaboration & Documentation: Work closely with cross-functional teams (Data Scientists, Analysts, DevOps) to gather requirements, share best practices, and document solutions. Innovation & Best Practices: Keep up to date with new Azure services, features, and industry trends to continuously improve data engineering processes. 

Qualifications:

Bachelor’s degree in computer science, Information Technology, Engineering, or related field Master’s degree (optional but preferred). 3-5 years of experience in data engineering or similar roles. Strong problem-solving, analytical, and communication skills. Ability to work in agile environments with cross-functional teams. 

Required Skills:

Azure Services Expertise: Proven experience with Azure Data Factory, Azure Synapse, Azure Databricks, Azure Data Lake, and Azure SQL. ETL/ELT & Data Integration: Strong background in building and managing data pipelines using modern tools and methodologies. Programming & Scripting: Proficiency in SQL, Python, and/or Spark for data transformation and automation. Data Modeling & Architecture: Ability to design robust data structures and develop scalable architectures for large volumes of data. Performance Tuning: Skilled in optimizing data storage, querying, and processing, including partitioning and indexing strategies. Security & Compliance: Familiarity with Azure RBAC, encryption, and relevant data privacy regulations (e.g., GDPR, HIPAA). CI/CD & DevOps: Experience with Azure DevOps, GitHub Actions, and Infrastructure as Code (e.g., ARM templates, Terraform). Data Governance: Knowledge of data quality, lineage, and governance best practices. Collaboration & Communication: Ability to work closely with cross-functional teams and stakeholders to understand requirements and deliver solutions. Continuous Learning: Enthusiasm for staying up to date with evolving Azure services, tools, and industry best practices. 

Preferred Skills:

Real-Time Data Streaming: Experience with Event Hubs, IoT Hub, or Kafka. Big Data Frameworks: Familiarity with Hadoop/Spark ecosystems. Machine Learning & AI: Exposure to Azure ML and/or Azure Open AI. Data Visualization: Proficiency in Power BI, Tableau, or similar tools. Domain/Industry Expertise: Manufacturing and Service industries. 

Middleby Corporation is an Equal Employment Opportunity (EOE/M/F/Vets/Disabled) employer and welcomes all qualified applicants.",https://recruiting.paylocity.com/Recruiting/Jobs/Details/2935204?src=LinkedIn
Hirenza,https://www.linkedin.com/company/hirenza-pvt-ltd/life,Data Engineer,https://www.linkedin.com/jobs/view/4255362103,4255362103,United States,Remote,,2025-06-23 22:30:02,False,,"About The Company

The Travelers Companies, Inc. has established itself as a leading property casualty insurer with a rich history spanning over 160 years. Committed to taking care of our customers, communities, and employees, we uphold the Travelers Promise by delivering exceptional service and innovative insurance solutions. Our culture is rooted in a dedication to collaboration, continuous improvement, and a passion for excellence. We foster an environment where innovation thrives, and employees are empowered to make meaningful contributions. Join us to be part of a forward-thinking organization that values integrity, diversity, and professional growth.

About The Role

We are seeking a highly skilled Data Engineer to join our Business Intelligence (BI) Modernization Data Engineering team. This team is responsible for constructing sophisticated data pipelines that contextualize and democratize access to enterprise-wide data. As a Data Engineer, you will play a pivotal role in transforming our analytics landscape by designing, building, and deploying data solutions that support artificial intelligence, machine learning, data analytics, and business intelligence initiatives. Your expertise will enable the organization to leverage data effectively, driving insights and strategic decision-making. This role offers an exciting opportunity to work with cutting-edge technologies such as Databricks, Snowflake, AWS, and Python, contributing to the modernization of our data ecosystem and ensuring high-quality, reliable data for various business functions.

Qualifications

The ideal candidate will possess a Bachelor's Degree in a STEM-related field or equivalent experience, with at least eight years of relevant industry experience. Demonstrated expertise in leading data engineering teams using cloud platforms such as AWS, and proficiency in programming languages like Python, along with hands-on experience with Databricks and Snowflake, are essential. Strong knowledge of data management principles, including data governance, security, and quality frameworks, is required. Candidates should have a proven track record of developing scalable data solutions, optimizing data pipelines, and mentoring team members. Familiarity with AI tools for code generation, testing, and performance optimization is a plus. Excellent communication skills, problem-solving abilities, and a commitment to operational excellence are critical for success in this role.

Responsibilities

Design, develop, and operationalize complex data solutions to meet organizational needs, ensuring data accuracy, consistency, and security.Lead technical initiatives by staying current with industry trends and emerging technologies, providing innovative solutions to enhance data capabilities.Mentor and guide a team of data engineers, fostering a collaborative environment that encourages autonomy, ownership, and continuous learning.Build and maintain data pipelines using Databricks, Snowflake, AWS, and Python, ensuring efficient data ingestion, transformation, and storage.Analyze complex data sources to determine their value and applicability for analytics and business intelligence purposes.Implement data governance, security, and quality standards to ensure compliance and data integrity across all solutions.Collaborate cross-functionally with business units and technical teams to support data-driven decision-making and educate end-users on new data products and environments.Perform system analysis, troubleshoot defects, and implement resolutions to maintain system stability and performance.Monitor data ecosystem jobs, support ingestion processes, and perform upgrades and defect fixes as needed.Conduct testing of data movement, transformation scripts, and system components to ensure reliability and performance.Participate in ongoing process improvements, contribute to engineering best practices, and support organizational initiatives.

Benefits

Travelers offers a comprehensive benefits package designed to support the well-being and professional growth of our employees. Health insurance coverage is available from day one for employees and their eligible family members, including spouses, domestic partners, and children. We provide a 401(k) retirement plan with dollar-for-dollar matching contributions up to 5% of eligible pay, along with a Pension Plan funded entirely by Travelers. Our paid time off policy includes a minimum of 20 days annually, complemented by nine paid holidays, fostering work-life balance. The wellness program offers tools, discounts, and resources to promote physical and mental health, including access to professional counseling services. We encourage community involvement through volunteer programs and matching gift initiatives, supporting employees in giving back to their communities. Additional benefits include educational assistance, professional development opportunities, and a supportive work environment focused on diversity and inclusion.

Equal Opportunity

Travelers is an equal opportunity employer committed to fostering a diverse and inclusive workplace. We value the unique perspectives and talents each individual brings and believe that a diverse workforce enhances our ability to serve our customers and communities effectively. We do not discriminate based on race, color, religion, gender, gender identity, sexual orientation, national origin, age, disability, or any other protected characteristic. We encourage all qualified candidates to apply and join our team in creating a respectful, innovative, and dynamic work environment.",https://www.bestjobtool.com/job-description/2968831552?source=LinkedIn
ClifyX,https://www.linkedin.com/company/clifyx/life,AI Data Engineering/Architect,https://www.linkedin.com/jobs/view/4255355943,4255355943,"New Jersey, United States",Hybrid,,2025-06-23 22:31:24,True,8 applicants,"Title: AI Data Engineer/ArchitectPosition: FulltimeLocation: New Jersey, Atlanta GA, Dallas TX, Chicago IL & VirginiaMust have experience with Top consulting company.
Job Description:AI Data Engineer/Architect is a hands-on, client-facing role within TCS’s AI & Data group (Americas), responsible for building and maintaining the data pipelines and infrastructure that power AI agent systems. This role transforms architectural blueprints into production-ready pipelines, ensuring AI agents have continuous access to clean, timely, and relevant data. You will work across industries—BFSI, Manufacturing, Life Sciences, Telecom, Retail, and more—handling diverse data types and ensuring they are ingested, transformed, and delivered to AI systems efficiently and securely.
Key Responsibilities:•Data Ingestion: Build pipelines to extract data from databases, APIs, files, and streaming sources using tools like Python, Kafka, and ETL frameworks.•Data Transformation: Clean, normalize, and enrich raw data using Spark, SQL, or cloud-native tools to make it AI-ready.•Data Loading: Load processed data into target systems such as vector databases, ElasticSearch, graph DBs, or cloud warehouses.•Real-Time Feeds: Implement streaming pipelines using Kafka, Flink, or cloud services for real-time AI applications.•Automation & Scheduling: Use Airflow, cloud triggers, or Lambda functions to automate and orchestrate data workflows.•API Integration: Develop connectors or services to fetch data from external APIs or systems on demand.•RAG & Knowledge Base Updates: Collaborate with AI Data Architects to ingest and embed documents for retrieval-augmented generation (RAG).•Testing & Validation: Implement unit, integration, and data validation tests to ensure pipeline reliability and data quality.•Performance Optimization: Tune SQL queries, Spark jobs, and infrastructure to meet SLAs and minimize latency.•Documentation & Handover: Create runbooks, pipeline documentation, and train client teams for post-deployment support.•Industry-Specific Handling: Adapt pipelines for domain-specific needs (e.g., HIPAA compliance in Healthcare, SOX in Finance).•Agile Collaboration: Work in agile teams, participate in sprint planning, and coordinate with client stakeholders.•Pipeline Maintenance: Monitor and evolve pipelines post-deployment, handling schema changes, scaling, and troubleshooting.•Continuous Learning: Stay current with evolving tools, frameworks, and best practices in data engineering and AI integration.
Qualifications: •5–8+ years of experience in data engineering/Architect, with exposure to AI/ML data workflows.•Strong programming skills in Python and SQL; familiarity with Java/Scala is a plus.•Experience with ETL tools (Airflow, AWS Glue, Azure Data Factory) and big data frameworks (Spark, Hadoop).•Proficiency in streaming technologies (Kafka, Flink, Event Hubs).•Hands-on experience with cloud platforms (AWS, Azure, GCP) and cloud-native data services.•Familiarity with vector databases, ElasticSearch, and graph databases.•Strong understanding of data formats (JSON, Parquet, Avro) and parsing techniques.•Experience with API integration, RESTful services, and authentication protocols.•DevOps familiarity: Git, CI/CD, Docker, and basic Linux scripting.•Strong debugging and problem-solving skills for data pipeline issues.•Attention to data quality, validation, and anomaly detection.•Excellent communication and documentation skills for client and team collaboration.•Agile mindset with the ability to adapt to changing requirements and priorities.•Domain awareness and ability to understand industry-specific data structures and compliance needs.•Familiarity with data serialization, file formats, and cloud storage (S3, ADLS, GCS).•Experience with monitoring tools (CloudWatch, ELK, Prometheus) and testing frameworks (PyTest, Great Expectations).•Bonus: Understanding of AI/ML concepts such as feature engineering, training vs inference data, and data leakage prevention.
RegardsMohd Faisal908-279-1281faisal@clifyx.com",https://www.linkedin.com/job-apply/4255355943
MetroStar,https://www.linkedin.com/company/metrostar-systems/life,Associate Database Engineer (5897),https://www.linkedin.com/jobs/view/4253319780,4253319780,"Quantico, VA",,,2025-06-23 20:51:09,True,67 applicants,"As Associate Database Engineer, you’ll help design, develop, and maintain enterprise database systems that support large-scale logistics and sustainment operations. You’ll work with Oracle technologies to optimize performance, support system changes, and ensure data integrity across development and production environments. This is an ideal opportunity for early-career professionals looking to grow in a mission-driven, Agile team.
We know that you can’t have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you’ll do:

Assist in the development and optimization of database schemas, queries, and stored procedures.
Support the implementation of software change requests (SCRs) and configuration updates across development and production environments.
Monitor database performance and assist with tuning and capacity planning.
Participate in validation testing and system engineering activities to ensure data integrity and performance.
Collaborate with senior engineers and Agile teams to support the delivery of secure, scalable, and high-performing database solutions.
Document database configurations, changes, and performance metrics.

What you’ll need to succeed:

Bachelor’s degree in Computer Science, Information Systems, or a related technical field.
0–2 years of experience in database engineering, development, or administration.
Foundational knowledge of relational database systems, particularly Oracle.
Ability to write and troubleshoot SQL queries, stored procedures, and functions.
Familiarity with database design principles, normalization, and indexing strategies.
Understanding of basic performance tuning, capacity planning, and data integrity practices.
Willingness to work in a collaborative, Agile environment supporting mission-critical systems.
Active DoD Secret Clearance, or ability to obtain one.
 To apply for this position, please submit your resume via the form below or through our careers page: https://www.metrostar.com/jobs/
Application Deadline:  Applications will be accepted on a rolling basis until the position is filled; candidates are encouraged to apply as early as possible for full consideration.
Additional Compensation: This role may also be eligible for bonuses and/or additional incentives based on individual and company performance.
Benefits: All full-time employees are eligible to participate in our benefits programs:

Health, dental, and vision insurance
401(k) retirement plan with company match
Paid time off (PTO) and holidays
Parental Leave and dependent care
Flexible work arrangements
Professional development opportunities
Employee assistance and wellness programs

Like we said, we are big fans of our people. That’s why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Commitment to Non-Discrimination
All qualified applicants will receive consideration for employment based on merit and without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, status as a protected veteran, or any other status protected by applicable federal, state, local, or international law.
 What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
 Not ready to apply now? 
Sign up to join our newsletter here.",https://www.linkedin.com/job-apply/4253319780
Jobot Consulting,https://www.linkedin.com/company/jobot-consulting/life,AI & Data Engineer - Python,https://www.linkedin.com/jobs/view/4255099163,4255099163,"Cincinnati, OH",Remote,$75/hr - $100/hr,2025-06-23 15:17:40,True,over 100 applicants,"Want to learn more about this role and Jobot Consulting? Click our Jobot Consulting logo and follow our LinkedIn page!

Job details

About Us

We leverage the latest in AI and data to provide highly specific and accurate results to our clients across the country.

Job Details

Are you a good fit?

 Work with cutting-edge AI and data technologies to deliver precise results. Collaborate in a serverless-first, containerized environment. Utilize Python, SQL, Spark, AWS, Amazon Athena, CDK, Terraform, and Docker.Proficiency in Python and SQLExperience with AWS and DockerFamiliarity with infrastructure-as-code tools like Terraform

Want to learn more about this role and Jobot Consulting?

Click our Jobot Consulting logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255099163
InterEx Group,https://www.linkedin.com/company/interex-group/life,AWS St Data Engineer,https://www.linkedin.com/jobs/view/4252673017,4252673017,"New York, United States",Remote,,2025-06-23 11:43:51,True,over 100 applicants,"A big client of ours in New York is currently searching for a AWS Data Engineer.
They are one of the largest Retail businesses globally with over 100,000 employees whereby they are working extensively with AWS and are looking to build a team of AWS Data Engineers to support the build-out of a brand new data platform on AWS Data Lake. They are looking to hire 5 people for this team and are open to hiring both Contract and Full Time Employees.
Role Essentials:AWS ExperienceData ModernizationData BricksData LakeData FactoryAWS certifications are a plus",https://www.linkedin.com/job-apply/4252673017
"Diverse Team, LLC",https://www.linkedin.com/company/diverse-team-llc/life,GCP Data Engineer,https://www.linkedin.com/jobs/view/4255250962,4255250962,"Texas, United States",Remote,,2025-06-23 23:06:55,True,29 applicants,"Required Minimum Qualifications:· Education requirement: Graduation from high school or the possession of GED, HiSET or TASC Certificate · Experience Requirement: Five (5) years of Information Technology experience performing user support desktop, legacy systems, and/or Information Technology communication systems· Education/Experience Equivalency: Additional appropriate education may be substituted for the minimum experience requirement License/Certifications: Must obtain Criminal Justice Information Services (CJIS) clearance within 30 days of start date.",https://www.linkedin.com/job-apply/4255250962
Robert Half,https://www.linkedin.com/company/robert-half-international/life,Data Engineer,https://www.linkedin.com/jobs/view/4255207106,4255207106,"Asheville, NC",Hybrid,$70/hr,2025-06-23 15:40:09,True,over 100 applicants,"We are recruiting for a Senior Data Engineer. If you are an expert with Azure SQL database, ETL pipeline development, data warehousing, and Python scripting - please apply! This position requires some occasional onsite in Asheville, NC.
Duties:Designs, Develops, and Implements data models to visualize and create a big picture view of the data needs of business use cases. Working with team members to develop, test and implement the exchange of data, on-going operations (including quality and automation), and work continuously to improve the build and development platform.Develops functional technical specifications and implement best practices to gather, store, and transform data to meet the needs of the functional specifications of the business use cases. Performs, tests, validates data flows and prepares ETL processes according to business requirements by incorporating business requirements into the design specifications.Analyzing data on target systems, analyses of data issues, and coordinate with data analyst to validate requirements.
Qualifications:5+ years in data engineering, including hands-on experience working with Azure SQL Database, Azure Data Factory, and Python. Educational qualifications such as Azure data certifications or a B.S. or higher in computer technology are desired.Experience with Azure data services including Data Bricks, Synapse Analytics, Azure Logic Apps, etc are preferred but not required.",https://www.linkedin.com/job-apply/4255207106
Highbrow LLC,https://www.linkedin.com/company/highbrow-llc/life,Solutions Engineer – Data,https://www.linkedin.com/jobs/view/4253328066,4253328066,"West Deerfield, IL",Hybrid,,2025-06-23 21:05:19,False,,"W2Deerfield, IL (Hybrid - 3 days/week onsite)Posted 5 hours ago

Website Highbrow LLC

Job Title: Solutions Engineer – Data

Job Location: Deerfield, IL (Hybrid – 3 days/week onsite)

# Positions: 1

Work Eligibility: All Work Authorizations are Permitted – No Visa Transfers

What You Will Do

Act as the lead engineer for each domain

Collaborate with Solution Architects to ensure understanding needed for engineering level guidance to nearshore teams

Translate architecture design to engineering level guidance

Own specifying architectural designs into sprint-ready engineering tasks for assigned domain

Ensure delivery to the solution architecture within domain

Answer design questions for sprint teams

Participate in backlog refinement sessions to align team with the architecture

Act as the internal technical expert supporting Product design and Engineering teams on demonstrations and technical discussions.

Create/Update documentation such as solution design patterns and engineering standards.

Lead technical discussions around APIs, integrations, data flow, compliance and security.

Serve as a trusted advisor on best practices, technical implementation, and scalability.

Recruiter Submission Template

Evaluation Benchmarks Self-Assessment (1-5) Experience (In years) Notes/Comments Experience with Python, Spark, Databricks, Azure Event Hub, ADF, Delta Lake Tables Able to write optimized ETL pipelines, manage distributed compute jobs, and handle large-scale data transformations. Experience with Azure Data Factory (orchestration) and Azure Event Hub (ingestion/event streaming) should know how to manage data movement, ingestion triggers, and pipeline tuning in a production Azure environment. Able to code in Spark, and understand execution models, partitioning strategies, caching, broadcast joins, and how these affect performance in Azure’s ecosystem.

To apply for this job email your details to jobs@highbrow-tech.com",https://highbrow-tech.com/job/solutions-engineer-data-3/
CTP,https://www.linkedin.com/company/ctpconsulting/life,Sr. Data Engineer,https://www.linkedin.com/jobs/view/4255211045,4255211045,"San Francisco, CA",Hybrid,$170K/yr - $220K/yr,2025-06-23 16:35:07,True,over 100 applicants,"Job Title: Sr Data Engineer
Location: San Francisco (Financial District) – Onsite Tuesday, Wednesday, Thursday
The Company:Headquartered in San Francisco, this investment firm is one of the leading pure-play investors with over $45 billion in AUM. They are expanding their data analytics capabilities to better service their firm and research teams with robust market insights.
Platform / Stack:You will work with technologies that include Azure Data Factory, SQL and Snowflake.
Compensation Expectation : $170,000 – $220,000
What You’ll Do As a Sr Data Engineer:Enable our research team to make data-driven decisions based on timely, consistent, complete, and correct data, reports and analyticsWork closely with engineering team members to support our existing ETL processes and solving our analytics needsBe the subject matter expert in all things data pipelines, data lakes and warehouse, tooling & frameworks, integration and sourcing of dataCollaborate with cross-functional teams to gather and analyze business requirements.Develop and optimize ETL processes to ensure data accuracy and integrity.Implement data governance and security measures to protect sensitive information.Provide technical leadership and mentorship to other BI engineersPossess the vision to guide us to the next stage of our data journey.Be detail-oriented, with a healthy level of skepticism that drives you to investigate even minor data discrepanciesQuickly learn new languages, tools, and frameworks, and be willing to explore uncharted territories to achieve your objectives
QualificationsYou could be a great fit if you have:B.S. in Computer Science, or related data field6+ years of experience in Data Engineering, ETL, Data Science or related roleExpert knowledge of Azure, Data Factories, SQL, FabricProficiency in BI tools such as Tableau, Power BI, or LookerProficiency in ETL tools and processesKnowledge of SnowflakeExcellent problem-solving and analytical skillsStrong communication and collaboration abilitiesExperience in the financial services or investment industry
This client requires that a background check be completed. A background check is required to protect our company/client and its stakeholders by ensuring that we hire individuals with a trustworthy history, which helps maintain a safe and secure workplace. This proactive measure minimizes potential risks and promotes a culture of integrity within the organization.
Benefits Offered:Employer provides access to:100% coverage of medical, dental and vision premiums for you and your family with no waiting period2:1 charitable contribution matching to non-profit organizationsAn annual bonus based on several factors including both profitability and individual performance.PSL is carried over year over yearSan Francisco has the following sick leave policy: accrue 1 hour for every 30 hours worked up to 72 hours",https://www.linkedin.com/job-apply/4255211045
Veridic Solutions,https://www.linkedin.com/company/veridic-solutions/life,Sr Snowflake Data Engineer   ,https://www.linkedin.com/jobs/view/4253309157,4253309157,"Sacramento, CA",Hybrid,,2025-06-23 17:55:14,True,over 100 applicants,"Job Title: Sr Snowflake Data EngineerCompany: DeloitteTerms: 6 month contract + (Client reserves right to hire after 6 months)Location: Hybrid (Sacramento California)
Requirements:3+ years of experience developing and monitoring data pipelines and ETL/ELT processes.3+ years of hands-on experience with Microsoft SQL Server and T-SQL.3+ years of experience using ETL tools, preferably AWS Glue.3+ years of experience building and integrating REST APIs, working with C# and JSON.1+ year of experience with database design, development, and security.1+ year of experience working on data conversion from legacy formats, including mainframe systems.1+ year of experience with container platforms (Docker, Kubernetes, or OpenShift).1+ year of experience with Microsoft Visual Studio and Git.1+ year working in cloud-based SaaS database environments (AWS or Azure).",https://www.linkedin.com/job-apply/4253309157
Galent,https://www.linkedin.com/company/galenthq/life,"Data Engineer (AWS, Fabric)",https://www.linkedin.com/jobs/view/4253316559,4253316559,"Newark, NJ",On-site,,2025-06-23 20:11:35,True,over 100 applicants,"Title: Data Engineer(AWS, Fabric)Location: Newark, NJ Hiring Mode: C2CMode of work: Hybrid (onsite 3 days a week)Experience : 12+ years
Job Description:Design, develop, and maintain scalable data models to support business needs.Create and optimize data pipelines for efficient data processing and transformation.Lead and manage the data ingestion processes, leveraging AWS and Fabric platforms to ensure seamless integration and high-quality data flow.Collaborate with cross-functional teams to define requirements and architect solutions spanning end-to-end implementation from data sources to the Gold layer.Utilize AWS stack (e.g., AWS S3, AWS Glue, AWS Lambda, AWS Redshift) and Fabric technologies to build robust and secure data ecosystems.Ensure adherence to best practices for data governance, security, and compliance.Mentor and guide team members, fostering collaboration and professional growth.Monitor and troubleshoot data architecture issues, continuously improving system performance.",https://www.linkedin.com/job-apply/4253316559
Hire Our Heroes Veteran Job Board,https://www.linkedin.com/company/hireourheroesveteranjobboard/life,Data Engineer III,https://www.linkedin.com/jobs/view/4255125748,4255125748,"San Diego, CA",On-site,$156.8K/yr - $209.1K/yr,2025-06-23 10:32:17,False,,"AMAZON WEB SERVICES, INC., an Amazon.com company - San Diego, California. Data Engineer III: Design, develop, implement, test, document, & operate large-scale, high-volume, high-performance data structures for business intelligence analytics. Salary Range $156,811/year to $209,100/year. Job Number: AMZ9084422. Multiple job openings. Standard company benefits available. Apply online: www.amazon.jobs search by Job Number. EOE.

recblid 8ux25io5bejkk0tfw7xr8xufxbje06",https://hireourheroes.com/job/15522714/data-engineer-iii/
Analytica,https://www.linkedin.com/company/analytica-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4240380993,4240380993,"Falls Church, VA",Remote,,2025-05-04 22:41:03,True,over 100 applicants,"Analytica is seeking a Data Engineer to support a complex data program for the Defense Health Agency. This role will work closely with reporting and analytics developers, data governance and data architects to build robust, high quality data pipelines that enhance productivity and operational efficiency. This position is for US Citizens only and candidate will hold (or be able to hold) a US Secret Clearance. The position is a remote position with occasional onsite meetings in at San Antonio, TX as required.

Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.   

Responsibilities: Support the Military Health System (MHS) by supporting data engineering that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset Ensure data interoperability, governance, and quality to support enterprise-wide decision-making and healthcare optimization play a critical role in designing and implementing data architecture frameworks that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset.Design and build robust and scalable data pipelines for managing structured and unstructured data using traditional databases (Oracle, PostgreSQL, etc.) or cloud Databases such as Amazon Redshift or AWS Vertica Translate business needs into: data architecture solutions development within supported data systems. data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems. Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams Monitor and troubleshoot data import, analysis, and display errors.
Required Qualifications: Bachelor’s degree in information technology, Computer Science, Engineering or equivalent technical field  3+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred) 2+ years experience with Python and SQL (Java and Python preferred) Experience working on relational NoSQL and SQL databases Experience designing and implementing various data pipeline patterns and strategies Strong knowledge of data security principles Prior experience with DHA M2 or MDR databases, CERNER EHR a strong plusMust be a US Citizen and must possess or be eligible for a SECRET clearance 
About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S.  Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider. 
 ",https://www.linkedin.com/job-apply/4240380993
Jobot,https://www.linkedin.com/company/jobot/life,Data Engineer,https://www.linkedin.com/jobs/view/4255095432,4255095432,"San Diego, CA",On-site,$110K/yr - $150K/yr,2025-06-23 15:17:56,True,over 100 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Data Engineer opportunity available with growing e-commerce agency! (San Diego, CA) - send your resume to marcus.curiel@jobot.com

This Jobot Job is hosted by Marcus Curiel

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $110,000 - $150,000 per year

A Bit About Us

We are a high-growth, data-driven eCommerce partner specializing in helping brands succeed on the world’s largest online marketplaces. Our team of strategists, analysts, and creatives work together to drive measurable performance across advertising, operations, logistics, and content. By combining deep industry knowledge with advanced analytics and proprietary tools, we help our partners unlock sustainable growth and protect their brand equity in an ever-evolving digital landscape.

Headquartered in Southern California, our company supports a diverse portfolio of category-leading brands and is powered by a passionate team dedicated to innovation, transparency, and results.

We are growing and looking for a Data Engineer to join our team!



Why join us?


 Work with a high-performing team focused on growing top-tier brands on an e-commerce platform. Play a key role in how data influences millions in marketplace decisions. Competitive salary, performance bonus, and full benefits Position works closely with the leadership team locally in the San Diego market.

Job Details

Responsibilities

 Design, develop, and maintain robust ELT pipelines using DBT and GCP-native tools (BigQuery, Cloud Functions, etc.) Build scalable data models to support analytics dashboards and business reporting Collaborate with cross-functional teams (Advertising, Logistics, Client Strategy) to gather requirements and translate them into data solutions Develop and enforce best practices for data governance, pipeline reliability, and documentation Continuously improve data workflows to ensure accuracy, scalability, and performance Monitor and troubleshoot production pipelines and system performance

Qualifications

 3+ years of experience in data engineering, analytics engineering, or a related field Strong experience with DBT (Data Build Tool) for data transformation and modeling Hands-on experience with Google Cloud Platform, especially BigQuery Proficiency in SQL and at least one programming language (e.g., Python) Solid understanding of data warehousing concepts and performance optimization Ability to work independently and communicate effectively with non-technical stakeholders Bonus Familiarity with Amazon/eCommerce data or marketplace ecosystems

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255095432
Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Engineer, Entry Level",https://www.linkedin.com/jobs/view/4255202321,4255202321,"Lombard, IL",Remote,,2025-06-23 15:26:42,False,,"At Jobright, we help high-growth startups hire top talent for their key roles.
About AnuvuFor over a decade, our clients in aviation and maritime have used our technology-driven products and services to keep their passengers entertained and connected to the things they love, from anywhere in the world. Brands such as Southwest Airlines, Norwegian Cruise Lines, Emirates, and Celebrity Cruises have trusted us to provide solutions from high-speed broadband internet to movies, television, and games.
Role SummaryWe are currently looking to hire a Data Engineer who will report into the Director of Air Network and become the newest member of our team. They should have a strong background in Python and SQL along with good problem-solving skills, critical thinking and the ability to dig in and work your way backwards on your own. Successful candidates will grasp our infrastructure with ease and also understand data and business rules. If this is you, we look forward to hearing from you.
What You’ll Be DoingAnalyze complex data elements and systems, data flows, dependencies, and relationships to troubleshoot data issues across the business and presents solutions to development team.Perform ad-hoc analyses of data stored in Air view and write SQL and/or Python scripts, stored procedures, functions.Design and build scalable pipelines to process terabytes of data.Focus on the design, implementation, and operation of data management systems to meet business needs. This includes designing how the data will be stored, consumed, and integrated into our systems.Developing metrics using data infrastructure to monitor performance of systems.Creation and management of databases to support large scale aggregation processes.Contribute to the vision for data infrastructure, data science, and analytics.
What We’re Looking ForBachelor’s Degree or higher2-4 years of working experience as a database engineering support personnel.Strong knowledge of Python.Experience with MySQL server and administration.Strong SQL skills.Comfortable navigating in a Linux environment, with bash shell scripting a bonusExperience building and deploying on AWS, especially with RDS, EC2, S3, EMR and Redshift.Experience building custom ETL, data warehousing, and pipeline infrastructure.Expertise transforming and standardizing and aggregating large datasets. And validating your work.Comfort with the DevOps side of engineering.Experience with Web Development Frameworks such as Django is a big plus.Interest in machine learning and statistics.
The Benefits of Working Here Generous 401(k) MatchingCompany Paid Short-Term & Long-Term DisabilityCompany Paid Life/AD&D InsuranceCompany Paid Wellness ProgramsCompany Health Savings Account ContributionsEmployee Assistance ProgramFlexible Spending Accounts for Dependent Care, Medical & TransitPaid Parental Leave and more",https://jobright.ai/jobs/info/6851cef7fe2edaf8853c05a6?utm_source=1124&utm_campaign=6851cef7fe2edaf8853c05a6&tob=true
ON Data Staffing,https://www.linkedin.com/company/on-data-staffing/life,AWS Principal Data Engineer – AWS Practice Lead,https://www.linkedin.com/jobs/view/4255353391,4255353391,United States,Remote,,2025-06-23 21:34:44,True,23 applicants,"AWS Principal Data Engineer – AWS Practice Lead

Our client, a growing Data Consultancy, is seeking a visionary and technically accomplished AWS Principal Data Engineer to spearhead the launch and leadership of their brand-new AWS practice. This is a unique opportunity to define and shape the direction of a critical growth area from the ground up.
Key Responsibilities:Practice Leadership – Build from Scratch:Establish and lead the AWS Data Engineering practice, defining strategic direction, technical standards, and go-to-market offerings.Act as the technical and thought leader for AWS-based data solutions across the organization and to clients.Collaborate with leadership to drive business development, marketing, and client engagement strategies focused on AWS.Presales & Client Engagement:Partner with sales and business development teams to lead presales conversations, solutioning, and proposal development.Craft compelling demonstrations and technical narratives that showcase the power and value of AWS solutions.Engage directly with clients to understand their data architecture needs and tailor AWS solutions that align with business goals.Technical Expertise & Delivery:Leverage deep knowledge of AWS data services (e.g., Redshift, Glue, EMR, S3, Lambda, Lake Formation, etc.) to design scalable, modern data platforms.Provide high-level architectural guidance and hands-on support during project delivery when needed.Stay ahead of the curve on AWS innovations and integrate best practices into practice offerings.Consulting & Advisory:Act as a trusted advisor to clients, guiding them through their cloud transformation and data modernization journeys.Translate complex business problems into technical solutions leveraging AWS's full stack of services.Team Growth & Management:Hire, mentor, and lead a high-performing team of AWS data engineers and consultants.Foster a culture of innovation, technical excellence, and continuous learning within the AWS practice.Oversee capability development, training, and internal knowledge sharing across the team.Qualifications:Extensive experience delivering complex data solutions using the AWS cloud ecosystem.Proven background in presales, consulting, and client-facing roles within the data and cloud space.Strong understanding of cloud-native architecture, data lakes, data warehousing, and real-time data processing on AWS.Experience leading technical teams and mentoring engineers across all levels.Excellent communication, presentation, and stakeholder engagement skills.Demonstrated ability to drive practice growth and define strategic direction in a consultancy setting.Education and Certifications:Bachelor’s or higher degree in Computer Science, Engineering, or a related field.AWS certifications such as AWS Certified Data Analytics – Specialty, Solutions Architect – Professional, or DevOps Engineer – Professional are highly desirable.",https://www.linkedin.com/job-apply/4255353391
AMEND Consulting,https://www.linkedin.com/company/amend/life,Data Engineer,https://www.linkedin.com/jobs/view/4253321891,4253321891,"Cincinnati, OH",On-site,,2025-06-23 21:10:14,False,,"About AMEND: AMEND is a management consulting firm based in Cincinnati, OH with areas of focus in operations, analytics, and technology, focused on strengthening the people, processes, and systems in organizations to generate a holistic transformation. Our three-tiered approach provides a distinct competitive edge and allows us to build strong relationships and create customized solutions for every client. We work each day to change lives and transform businesses, and we are constantly striving to make a positive impact on our community! The AMEND team continues to grow at a rapid pace, and our technical team will continue to be an important part of that journey.
Overview:The Data Engineer consultant role is an incredibly exciting position in the fastest growing segment of AMEND. You will be working to solve real-world problems by designing cutting edge analytic solutions while surrounded by a team of world class talent. You will be entering an environment of explosive growth with ample opportunity for development. We are looking for individuals who can go into a client and optimize (or re-design) companies data architecture, who are the combination of a change agent, technical leader and passionate about transforming companies for the better. We need someone who is a problem solver, a critical thinker, and is always wanting to go after new things; you’ll never be doing the same thing twice!
Job Tasks:Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metricsWork with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needsDefine project requirements by identifying project milestones, phases, and deliverablesExecute project plan, report progress, identify and resolve problems, and recommend further actionsDelegate tasks to appropriate resources as project requirements dictateDesign, develop, and deliver audience training and adoption methods and materials
Qualifications:Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Databricks and DBT experience is a plusExperience building and optimizing data pipelines, architectures, and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementStrong analytic skills related to working with structured and unstructured datasetsBuild processes supporting data transformation, data structures, metadata, dependency, and workload managementA successful history of manipulating, processing, and extracting value from large, disconnected datasetsAbility to interface with multiple other business functions (internally and externally)Desire to build analytical competencies in others within the businessCuriosity to ask questions and challenge the status quoCreativity to devise out-of-the-box solutionsAbility to travel as needed to meet client requirements
What’s in it for you?Competitive pay and bonusContinued education and individual development plansUnlimited VacationFull Health, Vision, Dental, and Life BenefitsPaid parental leavePTO for your Birthday3:1 charity match
All this to say – we are looking for talented people who are excited to make an impact on our clients. If this job description isn’t a perfect match for your skillset, but you are talented, eager to learn, and passionate about our work, please apply! Our recruiting process is centered around you as an individual and finding the best place for you to thrive at AMEND, whether it be with the specific title on this posting or something different. One recruiting conversation with us has the potential to open you up to our entire network of opportunities, so why not give it a shot? We’re looking forward to connecting with you.
*Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of employment Visa at this time.*",https://app.jobvite.com/CompanyJobs/Careers.aspx?k=Apply&j=oQNRufwI&s=linkedIn&__jvst=Job+Board&__jvsd=LinkedIn
LTIMindtree,https://www.linkedin.com/company/ltimindtree/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4253311249,4253311249,"Irving, TX",On-site,$79.9K/yr - $107.7K/yr,2025-06-23 18:36:08,False,,"
About Us:LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Title: Senior Data Engineer  Work Location-Irving TX  Job Description:  Certification Mandatory requiredWe are seeking a SDET profile with proficiency in python programming and a strong background in automation The candidate should be able to design and implement the automated testing frameworks and well versed in SQL and Data Visualization Tool
Key Responsibilities
Develop and Maintain Test Automation frameworks using PythonBuild Manage and optimize real time data pipelines using Apache KafkaProficiency in Apache Spark for batch and stream data processingDesign and optimize databases queries and procedures in PostgreSQLImplement reliable scalable and high performing distributed systemsCollaborate with cross functional team for feature development and system improvementsExperience with CICD pipelines and version controls in GITFamiliarity with containerization tools such as Docker and orchestration tools like KubernetesFamiliarity with big data ecosystems including HadoopKnowledge of RESTful and GraphQL APIsExpertise in data visualization tools like Power Bi Tableau QlikView for creating dashboardsExperience with message driven architectures and event sourcingKnowledge of Agile methodologies and ToolsExperience with testing frameworks eg Junit pytestKnowledge on Unix scriptingHands on experience on other RDBMS like Oracle DB
SkillsMandatory Skills : Flask, Apache Spark, Python, Nginx, Django, Jupiter Notebook, Scala, Apache Spark Scala, Python - Data ScienceGood to Have Skills: Flask, Nginx, Python - Data Science, Django
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):
Benefits and Perks:Comprehensive Medical Plan Covering Medical, Dental, VisionShort Term and Long-Term Disability Coverage401(k) Plan with Company matchLife InsuranceVacation Time, Sick Leave, Paid HolidaysPaid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office:In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
",https://r.ripplehire.com/s/c1Kmd
Aspirion ,https://www.linkedin.com/company/aspirion-health-resources-llc/life,Senior Azure SQL Data Engineer,https://www.linkedin.com/jobs/view/4250056733,4250056733,"Atlanta, GA",Remote,,2025-06-13 02:24:17,False,,"Position Overview:

Aspirion is seeking a detail-oriented and performance-driven Senior Azure SQL Data Engineer to join our growing Data Engineering team. This role will focus on designing, building, and optimizing scalable data infrastructure and services that support real-time business operations and enable advanced analytics, including AI/ML initiatives. The engineer will be responsible for driving excellence in database architecture, SQL performance tuning, and Azure resource management, while also contributing to the buildout of our Azure Lakehouse and Microsoft Fabric environment.

Key Responsibilities:

 Database Engineering & Optimization

– Write and optimize advanced T-SQL queries, stored procedures, views, and triggers.

– Analyze execution plans, tune indexes, and resolve deadlocks to maximize performance in OLTP environments.

– Design scalable and reliable schema changes for transactional and analytical systems.

 Azure SQL Infrastructure

– Provision, configure, and manage Azure SQL resources (Azure SQL Database, Managed Instances, SQL on Azure VMs).

– Implement capacity planning for compute and storage across OLTP and OLAP environments.

 Lakehouse Architecture & Fabric Integration

– Support and enhance Azure Data Lakehouse environments using Microsoft Fabric and OneLake.

– Develop ELT pipelines using Data Factory and Synapse Analytics.

– Integrate data governance practices using Microsoft Purview.

 AI & BI Readiness

– Design and prepare data layers optimized for AI/ML use cases and business intelligence consumption.

– Collaborate with analysts and data scientists to ensure performant and accessible datasets.

 Monitoring & Maintenance

– Implement proactive monitoring, logging, and alerting for SQL and data pipeline operations.

– Participate in incident response and continuous improvement of data reliability and performance.

Required Skills and Experience:

 Core SQL Expertise

– Advanced T-SQL development (queries, procedures, triggers)

– OLTP performance analysis and optimization

– Execution plan review, index tuning, deadlock resolution

 Azure Cloud Experience

– Hands-on experience with Azure SQL (all service models)

– Capacity planning for Azure SQL workloads

– Familiarity with Azure networking, access, and security for data services

 Data Engineering Tools

– Proficient with Azure Data Factory, Synapse Analytics, and Data Lake Storage

– Exposure to Microsoft Fabric and OneLake environments

– Understanding of Delta Lake and parquet formats

 Collaboration and Communication

– Proven experience working with cross-functional teams including BI, AI/ML, and DevOps

– Strong documentation and communication skills

 Experience

– 7+ years in data engineering with a focus on SQL and cloud data platforms

– at least 4 years of hands-on Azure SQL development and deployment

Preferred Qualifications:

 Microsoft Certified: Azure Data Engineer Associate or related Azure certifications Experience with Microsoft Fabric or early adoption of Lakehouse platforms Familiarity with data governance and cataloging via Microsoft Purview Exposure to tools supporting AI/ML pipelines in Azure

Educational Requirements:

 Bachelor’s degree in computer science, Data Engineering, Information Systems, or equivalent experience

About Aspirion:

For over two decades, Aspirion has delivered market-leading revenue cycle services, specializing in complex reimbursement challenges. With over 1,400 teammates, our culture is rooted in innovation, excellence, and a shared mission to deliver exceptional outcomes for our healthcare clients. We embrace flexibility, personal growth, and the intelligent use of technology—especially AI—to drive results.

Benefits:

 Competitive compensation and performance-based incentives Health, dental, vision, and life insurance from day one 401(k) with company match Flexible work environment (remote/hybrid) Professional development and certification support Mission-driven, people-first culture",https://aspirion-health-resources-llc.primepay-recruit.com/job/930067/senior-azure-sql-data-engineer?d=2025-06-12+19%3A36%3A54+UTC&s=lif
Apexon,https://www.linkedin.com/company/apexon/life,Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4255202481,4255202481,"Sacramento, CA",On-site,$100K/yr,2025-06-23 15:39:17,True,over 100 applicants,"Role: Sr. Snowflake Data EngineerLocation- SACRAMENTO, CA/HYBRID.Co-Location - 2-3 days/weekWest Coast/CA/Sacramento preferred.Duration: Full-time with ApexonPerform data engineering in support of a migration effort between the client's legacy platform (in Microsoft) to Snowflake.Experience with AWS Glue as the ETL Tool Experience with AWS S3 and Snowflake MQ: At least three (3) years of experience in developing and monitoring data pipelines, ETL/ELT (Extract Transform Load / ExtractAt least three (3) years of experience with Microsoft SQL Server and T-SQL.At least three (3) years of experience developing in ETL Tools.At least three (3) years of experience developing REST APIs, including experience with JSON and C#.At least one (1) year of experience in database design, database development, and database security.At least one (1) year of experience working with data conversion efforts involving legacy data formats and legacy data repositories such as Mainframe.At least one (1) year of experience in Container Platforms such as Kubernetes, OpenShift, Docker, or in Cloud Computing.At least one (1) year of experience with Microsoft Visual Studio and Git Code Repositories.At least one (1) year of experience with SaaS Cloud Database in either AWS or AzureBachelor’s Degree in computer science, engineering, information systems, math, or a technology-related field.",https://www.linkedin.com/job-apply/4255202481
Fruitist,https://www.linkedin.com/company/fruitist/life,Data Engineer,https://www.linkedin.com/jobs/view/4224370814,4224370814,United States,Remote,,2025-05-06 13:56:29,False,,"Fruitist is a modern food company on a mission to revolutionize snacking—and the numbers prove we’re well on our way. With over $1B in lifetime sales and $400M in revenue in the last year alone, we’re not chasing trends—we’re building a lasting brand at the intersection of health, sustainability, and innovation with our suite of snackable superfruits including berries and cherries.

Our momentum is powered by a leadership team drawn from the likes of McKinsey, Calm, Red Bull, and Netflix, and backed by global investors who specialize in consumer and tech-forward ventures. Together, we’re leveraging data, supply chain technology, and a sustainability-first model to disrupt a $500 billion-dollar industry.

We’re hiring builders. Operators. Creators. People who want to move fast, learn faster, and grow alongside a company that’s reshaping snacking. If you’re looking for a company where your work shapes the business—and your career accelerates with it—welcome to Fruitist.

At Fruitist, we’re harnessing the power of data to revolutionize sustainable agriculture, and we’re looking for Data Engineers to help lead the way. Reporting to the Head of Engineering, you’ll design, build and optimize data pipelines and analytics working on solution teams leading delivering operationally critical insights to end-users globally. Working closely with cross-functional teams, you'll ensure clean, structured data, of the highest integrity is available for consuming applications and direct end-users. With a focus on automation and accessibility, you'll optimize workflows, identify gaps, and improve data maturity. This role requires experience in data architecture, engineering, analysis, and data science workflows, all applied to transforming large amounts of data into accessible and usable data products for the business. This role is remote, with priority given to those who reside in Chicago, Boston, New York, Dallas or Miami. 

What You’ll Do

Collaborate with data scientists to ensure high-quality, accessible data for analytical and predictive modeling Design and implement data pipelines (ETL’s) tailored to meet business needs and digital/analytics solutions Enhance data integrity, security, quality, and automation, addressing system gaps proactively Support pipeline maintenance, troubleshoot issues, and optimize performance Lead and contribute to defining detailed scalable data models for our global operations Ensure data security standards are met and upheld by contributors, partners and regional teams through programmatic solutions and tooling 

What You’ll Need

BS/MS degree in Computer Science, Engineering (Mathematics and Statistics background is a plus) 5- 8 years hands-on experience in data engineering and databases/data warehouses Experience with Data Platforms (e.g., AWS, Databricks, Azure) in production environments Familiarity with Big Data platforms (e.g., Hadoop, Spark, Hive, HBase, Map/Reduce) Expert level understanding of Python (e.g., Pandas) Proficient in shell scripting (e.g., Bash) and Python data application development (or similar) Excellent collaboration and communication skills with teams Strong analytical and problem-solving skills, essential for tackling complex challenges Experience working with BI teams and tooling (e.g. PowerBI), supporting analytics work and interfacing with Data Scientists 

If you meet most of the qualifications above and connect with our mission to inspire enjoyable and nutritious snacking, we want to hear from you! While we are tech-forward in our embrace of AI to enhance our growing capabilities, we’re human-centric in our hiring. This means all applications will be reviewed by humans, including yours. We aim to respond to applicants within two weeks. We look forward to hearing from you!",https://jobs.ashbyhq.com/fruitist/43c26fc5-1875-46c4-bd02-4fb7d9b449cf?utm_source=utm_source%3Dsrc%3DLinkedIn
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Power BI Developer,https://www.linkedin.com/jobs/view/4255235138,4255235138,United States,Remote,,2025-06-23 20:09:14,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Innosoul inc, is seeking the following. Apply via Dice today!

Job ID: TX-70125073

Remote/Local TX Govt Power BI Developer/Data Analyst (15+) with Python, M, DAX, Git/BitBucket, SAS, Jira, SAP, Tableau experience

Location: Austin, TX (TEA)

Duration: 2 months

Skills:

8 Required Mastery in data analytics

4 Required Using Power BI creating dashboards and interactive visual reports

4 Required Create, test, and deploy Power BI Scripts (Python,M and DAX) as well as execute efficient deep analysis

4 Preferred Prior Experience In Data-related Tasks Is Preferred

1 Preferred Previous experience with Source Control, Git/BitBucket is preferred

1 Preferred Previous experience with SAS and its dataset (.sas7bdat) is preferred

1 Preferred Previous Experience With Jira Is Preferred

1 Preferred Previous state experience is preferred

1 Preferred Previous experience with BI tools ( SAP, Tableau) is preferred

Description:

Power BI developer builds complex dimensional data models and reports from the bottom up, visualizes compelling data stories on the report canvas, collaborates with other teams to engineer revolutionary agency wide data solutions, develops and implements data visualization solutions.

4 or more years of experience, relies on experience and judgment to plan and accomplish goals, independently performs a variety of complicated tasks, may lead and direct the work of others, a wide degree of creativity and latitude is expected.",https://click.appcast.io/t/_sk1g-KP7OlU6Cq2xdlpldOEFnd5Po9LIxNRx274950=
Durlston Partners,https://www.linkedin.com/company/durlston-partners/life,Senior Data Engineer - AI startup,https://www.linkedin.com/jobs/view/4255093882,4255093882,"New York, United States",On-site,$210K/yr - $350K/yr,2025-06-23 15:04:47,True,over 100 applicants,"We're looking for an experienced Senior Data Engineer to help us turn raw data into reliable insights that shape product direction and drive operational excellence. You’ll be instrumental in designing data models, building scalable pipelines, and ensuring data quality across our platform.What You’ll Do:Build and maintain robust, scalable data pipelines and infrastructureDesign efficient data models to support analytics and product needsEnsure accuracy, integrity, and performance across data workflowsCollaborate closely with product, analytics, and engineering teamsSupport internal and client-facing data access and reporting toolsWhat We’re Looking For:4+ years of hands-on experience in data engineeringStrong Python skills and experience with SQL databases (PostgreSQL preferred)Familiarity with ETL frameworks, cloud-based data tools (e.g., Snowflake, AWS)Strong communication skills and a detail-oriented mindsetComfortable working in a fast-moving, high-ownership environmentThis is an opportunity to shape data systems at the core of a mission-driven product in a team that values clarity, impact, and collaboration.",https://www.linkedin.com/job-apply/4255093882
The Brixton Group,https://www.linkedin.com/company/the-brixton-group-llc/life,Cloud Data Engineer,https://www.linkedin.com/jobs/view/4253324763,4253324763,United States,Remote,,2025-06-23 23:00:11,True,87 applicants,"Location: 100% remote (must be willing to work EST or CST hours) 
*** U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. ***
Our client is modernizing its data environment by transitioning from an on-premise SQL infrastructure to a scalable Snowflake cloud platform. 
We're seeking a hands-on Cloud Data Engineer to support this initiative by developing proof of concepts (POCs), building data pipelines, and enabling advanced analytics for high-visibility business use cases.
Responsibilities:Design and build APIs and ETL processes to ingest and transform data from SQL into SnowflakeDevelop and operationalize data pipelines and organize datasets within the Snowflake environmentSupport data governance and ensure data integrity and availability for analytics teamsCollaborate with the Snowflake professional services team and internal stakeholdersEnable business outcomes by building use cases that demonstrate value to executive leadership
Requirements:Proven experience migrating data from on-prem SQL systems to SnowflakeStrong knowledge of API development and ETL pipeline creationHands-on experience with cloud data architecture and Snowflake platformExcellent communication and collaboration skills, with the ability to gather requirements and provide updates across teams
Nice to Have:Exposure to data science workflows and enabling analytics teams",https://www.linkedin.com/job-apply/4253324763
Wiraa,https://www.linkedin.com/company/wiraa/life,Data Engineer,https://www.linkedin.com/jobs/view/4255311942,4255311942,United States,Remote,$100K/yr - $135K/yr,2025-06-23 20:46:18,False,,"About The Company

Eastwall, founded in 2022, is a specialized professional services firm focused on delivering transformative business outcomes through Microsoft Azure cloud solutions. As a Microsoft partner, we bring modern technology and operational models to life for clients across industries. We help organizations lead with innovation in data, AI, and cloud infrastructure.

About The Role

We are looking for a Data Engineer to join our growing team. This is a mid-level, client-facing technical position ideal for someone who is passionate about architecting and implementing modern data infrastructure on Microsoft Azure. You will work directly with enterprise architecture and technology leaders to help plan, build, and optimize solutions using Azure Data Lake, Microsoft Fabric, Synapse, Databricks, and more.

This role is fully remote within the continental United States and offers the opportunity to work on cutting-edge cloud data projects across multiple client environments.

Responsibilities

Design and implement modern Azure data infrastructure using Microsoft Fabric, Azure Data Lake, Azure SQL, Synapse, and DatabricksAnalyze and model source systems to build conformed data warehousing structures and define medallion architecture layers (Bronze, Silver, Gold)Reengineer legacy workloads from Microsoft SQL BI stack (SSIS, SSAS, SSRS) to modern Azure platformsDevelop semantic models and Power BI reports that promote self-service analyticsCollaborate with stakeholders to define requirements and deliver high-quality data solutionsSupport the full delivery lifecycle including architecture, development, testing, and documentationApply performance testing and monitoring best practices to production workloadsParticipate in data governance, security, and disaster recovery planningComplete Microsoft certification exams (DP-900, DP-203, DP-600) within the first 12 weeks of hire

Qualifications

1–3 years’ experience working with Azure data tools (Azure SQL, Data Factory, Synapse, etc.)1–2 years’ experience with CI/CD pipelines using Azure DevOps or similar toolsFamiliarity with infrastructure-as-code (ARM templates, Bicep, or Terraform)Experience developing ETL pipelines, dimensional models, and Lakehouse architectureStrong understanding of high availability and data recovery strategiesExperience with Microsoft Fabric, Snowflake, or Databricks is preferredPrior experience in a consulting or multi-client environment is a plusExcellent communication skills, including technical writing and presentationsExperience in AI/ML notebooks (Python, R) is a bonus

Benefits

Competitive base salary ($100,000–$135,000 based on experience)Quarterly performance-based bonusesMedical, dental, vision, and life insurance through United Healthcare401(k), FSA, and Lifestyle Spending Account (LSA) optionsThree (3) weeks paid time off plus eight (8) company holidaysAnnual training/conference stipendFully remote work environment with access to local co-working spaces

Equal Opportunity

Eastwall is proud to be an equal opportunity employer. We are committed to a diverse and inclusive workplace and do not discriminate on the basis of race, gender, age, disability, sexual orientation, national origin, veteran status, or any other protected category.",https://www.wiraa.com/?source=Linkedin#/job-description/DFD7A5146D540DC547A4C717F314688F
Anblicks,https://www.linkedin.com/company/anblicks/life,Lead Data Engineer,https://www.linkedin.com/jobs/view/4255306732,4255306732,"Dallas, TX",On-site,,2025-06-23 19:04:22,True,over 100 applicants,"Skills:AWS CloudMDM(Master Data Management)Snowflake PythonAirflow Data Quality frameworks like Collibra, Alation
Summary:We are seeking a highly motivated and experienced Lead Data Quality Engineer to drive the implementation and execution of data quality initiatives within our organization. This role is a 50/50 blend of Data Quality and Data Engineering, requiring proven experience with enterprise-level data quality implementations, along with strong SQL and Python or ETL skills.In this leadership role, you will be responsible for leading a team of data quality engineers, defining data quality standards, implementing automated data quality checks, and ensuring the reliability and integrity of our data assets. Experience with SODA (or similar data quality frameworks like Collibra, Great Expectations, Deequ, etc.) is essential.Responsibilities:• Utilize the data domain and Critical Data Elements (CDE) inventory provided by the domain architect to develop comprehensive data quality rules.• Design, implement, and manage data quality rules using Soda, ensuring they are effectively integrated and applied within data pipelines to maintain high data integrity and accuracy.• Lead and mentor a team of Data Quality and Data Engineers, providing technical guidance and fostering a culture of data quality excellence.• Define and promote data quality best practices, standards, and procedures across the organization.• Collaborate with cross-functional teams (e.g., Data Engineering, Data Analytics, Business Intelligence) to ensure data quality is integrated into all data processes.• Act as a subject matter expert on data quality, providing guidance and support to stakeholders.• Implement and maintain data quality frameworks and tools, with a focus on SODA (or similar frameworks).• Configure and customize data quality tools to meet specific business requirements.• Develop and implement data quality rules, checks, and validations.• Automate data quality monitoring, alerting, and reporting processes.• Monitor data quality metrics and KPIs, and track progress against data quality goals.• (Preferred) Demonstrate deep understanding and hands-on experience with SODA (or similar data quality frameworks).• Utilize SODA to define data quality checks, configure data sources, and generate data quality reports.",https://www.linkedin.com/job-apply/4255306732
BGSF,https://www.linkedin.com/company/work4bgsf/life,Log Data Engineer,https://www.linkedin.com/jobs/view/4255194192,4255194192,"Owings Mills, MD",Hybrid,,2025-06-23 16:42:11,True,over 100 applicants,"Initial Assignment Duration: 6 monthsWork Location: Owings Mills, Maryland (Hybrid 2 days on-site)
Overview of Role:This role is a mix of infrastructure and data engineering (DataOps). We are therefore looking for experience in both infrastructure and data management for the ideal candidates. They should also have autonomy and be able to prioritize their work with minimal oversight.  Responsibilities:  Develop data processing pipelines using programming languages like Java, Javascript and Python on Unix server environments to extract, transform, and load log data Implement scalable, fault-tolerant solutions for data ingestion, processing, and storage. Support systems engineering lifecycle activities for data engineering deployments, including requirements gathering, design, testing, implementation, operations, and documentation. Automating platform management processes through Ansible or other scripting tools/languages Troubleshooting incidents impacting the log data platforms Collaborate with cross-functional teams to understand data requirements and design scalable solutions that meet business needs. Develop training and documentation materials Support log data platform upgrades including coordinating testing of upgrades with users of the platform Gather and process raw data from multiple disparate sources (including writing scripts, calling APIs, writing SQL queries, etc.) into a form suitable for analysis Enable log data, batch and real-time analytical processing solutions leveraging emerging technologies Participate in on-call rotations to address critical issues and ensure the reliability of data engineering systems
Required Technical Expertise:  Expertise in AWS and implementation of CICD pipelines supporting log ingestion. Expertise with AWS computing environments such as ECS, EKS, EC2 and Lambda 3-5 years’ Experience in designing, developing, and deploying data lakes using AWS native services (S3, Firehose, IAM, Terraform) Experience with data pipeline orchestration platforms Expertise in Ansible/Terraform scripts and Infrastructure as Code scripting is required Implement version control and CI/CD practices for data engineering workflows to ensure reliable and efficient deployments (e.g. Gitlab) Proficiency in distributed Linux environments Proficiency in implementing monitoring, logging, and alerting solutions for data infrastructure (e.g., Prometheus, Grafana) Experience writing data pipelines to ingest log data from a variety of sources and platforms. Implementation knowledge in data processing pipelines using programming languages like Java, Javascript and Python to extract, transform, and load (ETL) data Create and maintain data models, ensuring efficient storage, retrieval, and analysis of large datasets Troubleshoot and resolve issues related to data processing, storage, and retrieval. Experience in development of systems for data extraction, ingestion and processing of large volumes of data",https://www.linkedin.com/job-apply/4255194192
KAPITAL,https://www.linkedin.com/company/kapital-data-corp/life,Power BI Developer,https://www.linkedin.com/jobs/view/4253301620,4253301620,"Austin, TX",Remote,,2025-06-23 17:36:48,True,over 100 applicants,"Power BI Developer
Location: Austin, TX
Fully Remote
Start Date - 7/16/2025
End Date - 8/31/2025
Submissions due by 7/3/2025
 This a Power BI Developer position (Data Analyst 3) that will work closely with the Texas Education Agency. This person will build complex dimensional data models and reports from the bottom up, and is expected to have a mastery of data analytics. Extensions are likely.  Key ResponsibilitiesVisualize compelling data stories on the report canvasCollaborate with other teams to engineer revolutionary agency wide data solutionsDevelop and implement data visualization solutionsUtilize experience and judgment to plan and accomplish goals, independently perform a variety of complicated tasks Minimum RequirementsYearsRequired/PreferredExperience8RequiredMastery in data analytics4RequiredUsing Power BI creating dashboards and interactive visual reports4RequiredCreate, test, and deploy Power BI Scripts (Python,M and DAX) as well as execute efficient deep analysis4PreferredPrior experience in data-related tasks is preferred1PreferredPrevious experience with Source Control, Git/BitBucket is preferred1PreferredPrevious experience with SAS and its dataset (.sas7bdat) is preferred1PreferredPrevious experience with Jira is preferre1PreferredPrevious state experience is preferred1PreferredPrevious experience with BI tools ( SAP, Tableau) is preferred",https://www.linkedin.com/job-apply/4253301620
Healthcare IT Leaders,https://www.linkedin.com/company/healthcare-it-leaders/life,Tableau Developer (Healthcare),https://www.linkedin.com/jobs/view/4255223776,4255223776,United States,Remote,,2025-06-23 19:39:49,True,over 100 applicants,"We are seeking an experienced Tableau Developer to create and maintain

data visualization solutions for a leading healthcare organization.

3-month contract (possible extension)100% Remote.

In this position you will be responsible for developing and implementing

data visualization solutions, creating interactive dashboards, and providing

insights through advanced analytics. The ideal candidate will have extensive healthcare industry experience and a proven track record of delivering high-quality Tableau solutions.

Requirements

5+ years of experience with Tableau Desktop and Tableau ServerStrong background in healthcare analytics and reportingExpertise in SQL and data manipulationExperience with healthcare data structures and terminologyProven ability to translate complex requirements into intuitive visualizationsStrong communication skills and ability to work with stakeholdersBachelor's degree in CS, IS, or related fieldExperience with data warehousing concepts and ETL processes

Healthcare IT Leaders is a national leader in IT workforce solutions, connecting healthcare provider, payer and life sciences organizations with experienced technology talent for consulting and full-time hiring. For more information, visit us on the web at www.healthcareitleaders.com .

Healthcare IT Leaders provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

",
Lensa,https://www.linkedin.com/company/lensa/life,REMOTE - Data Engineer,https://www.linkedin.com/jobs/view/4255158336,4255158336,"Woonsocket, RI",Remote,$50/hr - $55/hr,2025-06-23 14:10:04,False,,"Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

An employer is seeking a Data Engineer to join a leading healthcare and pharmaceutical company. Responsibilities include:

 Be a technologist and work with other Engineers in planning, prioritizing, and performing assigned tasks within deadlines

 Will be responsible for end-to-end application development & delivery including production deployment, application operationalization, and observability.

 Develop data pipelines to ingest, process and broadcast data across heterogenous databases.

 Build and deploy using GitHub, CircleCI, Harness as part of CI/CD process in leading Cloud Platforms - AWS, GCP or Azure etc.

 Continuously checking and monitoring App health and KPIs, support triage of any production issues as and when needed

 Be an advocate for and implementer of security best practices

 Adopt and apply industry technology best practices

 Partner with application owners, business partners and peer groups regarding long and short-range technical solutions that meet business requirements.

 Analyze and contribute to project and business requirements based on product team milestones and priority.

 Actively participate in Agile Scrum team activities including Sprint Planning, Grooming, Scrum, Reviews and Retrospectives

$50-55/hour

Exact compensation may vary based on several factors, including skills, experience, and education.

Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401K retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

7+ years experience in software engineering from ideation to production deployment of software

 7+ years experience in full software development life cycle including ideation, coding, coding standards, testing, code reviews and production deployments

 Experience in creating/managing GCP storage Buckets, Data Composer workflow, Dataflow jobs, IAM (Service Account/Roles) Management.

 Experience in data extraction, transformation, loading (ETL), data quality checks, database management Experience in Writing automated unit and mock test cases for code coverage

 Deployment and troubleshooting application in Cloud environment (GCP)

 Experience with Kubernetes, Apigee, GCP -GKE, Dataflow, Airflow. MongoDb, SQL, Postgres SQL, SOAP services, IntelliJ and Devops: Git, Jenkins, Github Actions.

 Experience in log ingestion and building dashboards using Prometheus and Grafana.

 Proficiency in SQL and NoSQL database technologies

 Query and decipher logging entries by application tiers and components using trace logs

 Form requests in tooling like Postman , SOAP UI to invoke endpoints

 Experience working in a Scrum/Agile development methodology

 Ability to work independently and part of a team

 Healthcare Domain experience null

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/4f7d7ee5f19e4cfe95a814aa4ba7affftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
BCforward,https://www.linkedin.com/company/bcforward/life,Data Engineer,https://www.linkedin.com/jobs/view/4255347139,4255347139,"Dallas, TX",On-site,$60/yr - $65/yr,2025-06-23 21:04:46,True,over 100 applicants,"Job Id: DLTJP00044503Job Title: ETL TalendLocation: Dallas, TX (Onsite)
Responsibilities: Work with BA’s and understand the Business Requirements and Mapping Document. Extensive hands on development using Talend ELT Components to implement the data ingestion from files in blob storage to snowflake, transform the data and archive the files as specified in the mapping document. Extensive hands on development using Snowflake stored procedures and Snowflake SQL. Parameterize and Publish the jobs to TMC via cloud. Check-In code to GIT and create artifacts using TeamCity for deployment through CI/CD pipeline. Create CI/CD pipeline to publish the job to higher environments. Debug existing Talend Jobs to address any bugs or requirements change. Qualifications needs to be updated to below.Required:Extensive hands on experience of at least 4 years in Talend, Snowflake and Oracle.Extensive experience working on ETL tools and data pipeline.Experience working on ETL and ELT processes.TalendSnowflake SQL and Stored ProceduresBlob StorageELT ProcessOraclePreferred:AzureCI/CD pipeline",https://www.linkedin.com/job-apply/4255347139
TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,"Sr. Data Engineer, Navigator Platform (Python, AWS, Spark)",https://www.linkedin.com/jobs/view/4255301085,4255301085,Dallas-Fort Worth Metroplex,Hybrid,,2025-06-23 17:38:59,False,,"Sr. Data Engineer, Navigator Platform (Python, AWS, Spark)

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
About the team:Join our world-class engineering team building cutting-edge SaaS products for car dealerships on the Capital One Navigator platform. We're an integral part of the Revenue Operations experience, designing and maintaining innovative tools that seamlessly onboard dealers, drive significant revenue, and optimize product pricing. Our core responsibility is architecting highly available and resilient data pipelines, ensuring real-time, 100% accurate data for the platform. We also manage data governance, infrastructure monitoring, and continuous optimization. This role is a chance to tackle complex engineering challenges and contribute to a high-impact product.
What You'll Do:Be a part of team designing and building Enterprise Level scalable, low-latency, fault-tolerant streaming data platform that provides meaningful and timely insightsBuild the next generation Distributed Streaming Data Pipelines and Analytics Data Stores using streaming frameworks (Flink, Spark Streaming) using programming languages like Java, Scala, PythonBe part of a group of engineers building data pipelines using big data technologies (Spark, Flink, Kafka, Snowflake, AWS Big Data Services, Snowflake, Redshift) on medium to large scale datasetsWork in a creative & collaborative environment driven by agile methodologies with focus on CI/CD, Application Resiliency Standards, and partnership with Cyber & Security teamsShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:Bachelor's DegreeAt least 3 years of experience in application development (Internship experience does not apply)At least 1 year of experience in big data technologies
Preferred Qualifications:5+ years of experience in application development including Python, SQL, Scala, or Java2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)3+ years experience with Distributed data computing tools (Kafka, Spark, Flink)2+ year experience working on real-time data and streaming applications2+ years of experience with NoSQL implementation (DynamoDB, OpenSearch)2+ years of data warehousing experience (Redshift or Snowflake)3+ years of experience with UNIX/Linux including basic commands and shell scripting2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
Plano, TX: $144,200 - $164,600 for Senior Data Engineer










Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).PDN-9f385459-d6a3-404a-9562-87acd9f5d2f0",https://tracking.prodivnet.com/track/apply/9f385459-d6a3-404a-9562-87acd9f5d2f0
THOUGHT BYTE,https://www.linkedin.com/company/thought-byte/life,Graph Database Engineer,https://www.linkedin.com/jobs/view/4253307318,4253307318,United States,Remote,,2025-06-23 18:05:02,True,43 applicants,"Position: Graph DB EngineerLocation: 100% RemoteJob Type: FTE (Preferred) Or FTC- W2
Who are we looking for?We are seeking an experienced and passionate Graph Database Engineer with 6+ years of expertise in graph databases . As a key member of our team, you will contribute to cutting-edge projects, leveraging your proven experience in graph database design, development, and optimization. Your expertise will drive innovation and efficiency, ensuring seamless data management and retrieval. Collaborating closely with cross-functional teams, you’ll contribute to the success of our projects and elevate technical capabilities.
Responsibilities:
Skills – Must have:· Translate business requirements into technical solutions using graph databases.· Design and develop Graph database solutions on business requirements.· Create and optimize graph database models/schemas for querying and traversal.· Experience on RDF based graph databases.· Implement indexing and caching strategies for data retrieval.· Extensive work experience on knowledge graph implementation.· Ensure scalability, reliability, and performance of graph databases.· Fine-tune graph queries for optimal performance.· Monitor and troubleshoot graph database performance metrics.· Handle schema migrations and version upgrades.· Manage database security, access controls, and backups.· Collaborate with cross-functional teams to integrate graph databases into application ecosystem.· Work closely with data scientists, software developers, and product managers to align graph database solutions with overall project goals.· Guide junior developers on technical implementation.· Strong understanding of graph theory, traversal algorithms, and data structures.· Experience with data modeling, development and analytics implementation.· Familiarity with industry standards and best practices on Graph implementation· Proficiency in graph database technologies (e.g., rdf4j , OrientDB, Neo4j , Nebula, ArangoDB, GraphBase, Neptune, TigerGraph, etc.)
Skills – Good to have:· AI/ML exposure.· Python programming· Analytics exposure
EDUCATION QUALIFICATION· Graduate in Engineering OR Masters in Computer Applications.
Process Skills:· General SDLC processes· Understanding of Agile and Scrum software development methodologies· Skill in gathering and documenting user requirements and writing technical specifications.
Behavioral Skills :· Good Attitude and Quick learner.· Well-developed design, analytical & problem-solving skills· Strong oral and written communication skills· Excellent team player, able to work with virtual teams.· Self-motivated and capable of working independently with minimal management supervision.
Certification (Good to have):· Any Graph Certification",https://www.linkedin.com/job-apply/4253307318
Pearson Carter,https://www.linkedin.com/company/pearson-carter/life,Data Engineer (Remote - EST) -  Christian Faith ,https://www.linkedin.com/jobs/view/4255157278,4255157278,"Orlando, FL",Remote,$80K/yr - $100K/yr,2025-06-23 13:50:26,True,over 100 applicants,"Job Title: Data EngineerLocation: Remote or Orlando, FL
What You’ll DoAs a Senior Data Engineer, you'll be the backbone of our data operations, enabling strategic decision-making across fundraising, finance, and operations.You’ll:Design and maintain a robust data warehouse pulling from systems like Salesforce, NetSuite, Dynamics CRM, HubSpot, and ADP.Build and optimize ETL pipelines and integrations across multiple enterprise applications.Customize business systems to streamline operations and align with ministry goals.Develop reports and dashboards using Power BI and SSRS to provide actionable insights.Collaborate closely with Finance, Fundraising, and IT teams to ensure accuracy and efficiency in data-driven efforts.Support database administration, performance tuning, and data security.Stay current on cloud data technologies (Azure, AWS, etc.) and recommend improvements.
Who You AreA devoted Christian who senses a calling to use your gifts in ministry.An expert in Microsoft SQL Server with advanced knowledge of T-SQL, performance tuning, and database architecture.Experienced in enterprise system integration, including Salesforce, NetSuite, and Dynamics CRM.Skilled in Power BI and SQL Server Reporting Services (SSRS), including paginated reports.Comfortable working with financial and accounting data, with knowledge of nonprofit fundraising and compliance a plus.A servant leader with integrity, humility, and a collaborative spirit.Adept at remote teamwork and eager to learn and grow.
Preferred Qualifications:Bachelor’s or Master’s in Computer Science, Data Science, Finance, or related field.Experience with cloud-based data platforms.
What We OfferA mission-driven team environment rooted in faith and purpose.Paid professional development, including education, training, and certification attempts.Three weeks of paid vacation to start, plus holidays.Health insurance for you and your family.One paid conference per year.",https://www.linkedin.com/job-apply/4255157278
Jobot,https://www.linkedin.com/company/jobot/life,Sr. Business Intelligence Developer,https://www.linkedin.com/jobs/view/4255098530,4255098530,"Santa Monica, CA",Remote,$120K/yr - $145K/yr,2025-06-23 15:18:10,True,52 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Sr. Business Intelligence Developer-Remote-Must work PST

This Jobot Job is hosted by Robert Reyes

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $120,000 - $145,000 per year

A Bit About Us

Imaging Application Analyst 100% onsite in Los Angeles, CA.

Prestige Hospital System

Placed #1 in both California in a broad assessment of excellence in hospital-based patient care.



Why join us?


Imaging Application Analyst 100% onsite in Los Angeles, CA.

Competitive Salary$$

Stellar Benefits (Medical, Dental Vision, Life Insurance)

Flexible Schedule

Job Stability

Career growth

The position offers a competitive salary

If you are passionate, thrive in a fast-paced environment and are ready to take your career to the next level, we would love to hear from you.

This position offers a unique opportunity to work with a diverse team of professionals and to contribute to our company's growth and success. If you have a passion for data analysis and business intelligence, and you are ready to take your career to the next level, we would love to hear from you.

Job Details

Job Details

We are looking for an experienced Sr. Business Intelligence Developer to join our dynamic Tech Services team. This role is an exciting opportunity to work on complex and sophisticated data systems in a fast-paced, innovative environment. The successful candidate will be responsible for designing, developing, and maintaining business intelligence solutions to support data analysis and decision-making. This role requires a deep understanding of healthcare and clinical data, coupled with strong technical skills in Azure SQL, QLIK, PL/SQL, MSSQL, SQL Server, and Power BI. If you are a problem solver who thrives on challenges and loves to stay ahead of technology trends, this role is for you!

Summary

The Business Intelligence Developer Senior will work in Keck Medicine IS team to develop and test, implement business intelligence solutions for various Keck Medicine departments. The position will serve as the subject expert as it relates to dashboards. Also, this position will be responsible for identifying and documenting information technology design specifications based on analysis/assessment of user needs and generate need-gap analyses. Also, this role ensuring data integrity, maintaining system security, extracting, analyzing, and transforming data. The IT Business Intelligence Developer Senior also understands the functional workflow and processes of the Departments he/she supports and maintains system functionality leverage USC's various technologies including but not limited to QlikView, Cerner Command Language (CCL) and other applications, Hadoop, and RDBMS. Applies learned skills to perform problem resolution across integrated platforms, systems, processes, and departments. Supports the mission, vision, values, and strategic goals of Keck Medicine of USC.

Minimum Education

Bachelor’s degree in Computer Science, Information Systems, Computer Engineering, or related field required.

In lieu of bachelor’s degree will consider 10 years’ experience in relevant business support and/or IT support.

Minimum Experience/Accountabilities

Minimum 6 years’ relevant experience including dashboards /BI reporting design, documentation, maintenance, implementation, upgrades, and troubleshooting. If no bachelor’s degree, must have 10+ years of experience.

Must have experience with Structured Query Language (MS SQL Server, Oracle)

2 Years’ Experience QlikView And QlikSense Preferred.

Must have other BI tools experience, such as Tableau, PowerBI, etc.

Must be able to demonstrate experience with database systems and experience with system design, capacity planning, and capacity management.

Requires hands-on expertise with database services.

Dashboard SDLC methodology to develop, test and deploy enterprise dashboards promptly.

Data Visualization Design Apply sound user experience design to incorporate visualization in dashboards and reports.

Training Trains new staff members on applicable systems/applications. Responsible for working with customer and/or vendors with training on new systems being implemented and rolled out for use in the departments.

Priority Management Must work several assignments at one time, manage priorities, deadlines, and time. The work is highly technical, requires collaboration across multiple disciplines and groups. The ability to work independently is also required.

Customer Service Addresses customer questions, concerns, enhancement requests, communicates with customers, handles services problems and tickets politely and efficiently, always available for customers, follows procedures, utilizes problem solving skills, maintains pleasant and professional image. Customers may include both internal department users, vendors, and peers within IS.

Reporting Generate ad-hoc as well as routine reports and data dump for further consumption.

Business Intelligence Transform disparate data into meaningful analysis. Provide information that can be used for decision making. Predict outcomes and prescribe appropriate actions.

Documentation Prepare documentation related to dashboards and reports (e.g., FRD). Draft a mock-up data visualization before start building dashboard.

Other Duties and On-Call Ability to fulfill On-Call requirements and other duties as assigned.

This is a unique opportunity to join a forward-thinking team that values innovation and creativity. If you are passionate about leveraging data to drive business success, and you have the skills listed above, we would love to hear from you!

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255098530
ManpowerGroup,https://www.linkedin.com/company/manpowergroup/life,Lead Data Center Engineer,https://www.linkedin.com/jobs/view/4255332933,4255332933,"Broomfield, CO",On-site,$40/hr - $50/hr,2025-06-23 20:55:44,False,,"Our client, a leader in the data center industry, is seeking a Lead Data Center Engineer to join their team. As a Lead Data Center Engineer, you will be part of the engineering team supporting critical environments. The ideal candidate will have strong leadership skills, excellent problem-solving abilities, and a commitment to safety, which will align successfully with the organization.

Job Title: Lead Data Center Engineer

Location: Broomfield, CO

Pay Range: $40/hr. - $50/hr.

What's the Job?

Assist in the preparation of expense and capital budgets, ensuring adherence to cost justification guidelines. Supervise the operating engineers, ensuring accurate time records and justifying overtime worked. Inspect physical conditions of buildings and equipment, preparing work orders for necessary repairs. Coordinate tenant construction and capital improvements, reviewing project instructions and blueprints. Maintain efficient energy use in operating buildings and train team members in energy management practices. 

What's Needed?

Two-year technical degree in HVAC or related field. Seven to ten years of engineering experience in a relevant industry. Three to five years of supervisory experience or experience as an Assistant Chief Engineer. Thorough knowledge of building HVAC, electrical, plumbing, automation, and life safety systems. Ability to read and interpret technical documents and communicate effectively with tenants and contractors. 

What's in it for me?

Opportunity to work in a dynamic and innovative environment. Professional development and training opportunities. Collaborative team culture focused on safety and excellence. Chance to lead and mentor a team of skilled engineers. Engagement in projects that enhance the value of client assets. 

Upon completion of waiting period consultants are eligible for:

Medical and Prescription Drug Plans Dental Plan Vision Plan Health Savings Account Health Flexible Spending Account Dependent Care Flexible Spending Account Supplemental Life Insurance Short Term and Long Term Disability Insurance Business Travel Insurance 401(k), Plus Match Weekly Pay 

If this is a role that interests you and you'd like to learn more, click apply now and a recruiter will be in touch with you to discuss this great opportunity. We look forward to speaking with you!

 About ManpowerGroup, Parent Company of: Manpower, Experis, Talent Solutions, and Jefferson Wells 

ManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands – Manpower, Experis, Talent Solutions, and Jefferson Wells – creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2023 ManpowerGroup was named one of the World's Most Ethical Companies for the 14th year - all confirming our position as the brand of choice for in-demand talent.

",https://click.appcast.io/t/L0rT2j5KXYHyTg_srPIlgAYEPvIHAdAsXwFRkjDnpfA=
ZenQuill,https://www.linkedin.com/company/zenquill/life,AI Engineer Intern (Replit or Cursor Experience Required),https://www.linkedin.com/jobs/view/4255356933,4255356933,"Austin, TX",Remote,,2025-06-23 22:49:51,True,25 applicants,"About ZenQuillZenQuill is a modern journaling and mindfulness platform powered by AI. We combine voice journaling, psychoanalytic insights, and meditation tracking into one powerful app designed for personal growth. Backed by real clinical partners and growing fast, we're on a mission to make inner peace and self-awareness radically accessible.
Role OverviewWe’re seeking a hands-on AI Engineer Intern with real development experience using Replit or Cursor. You’ll work directly with our CEO and Lead Engineer on cutting-edge AI features, including real-time journaling analysis, voice agent logic, and GPT-based tools.This is not a theory-heavy research role — it’s a shipping-first internship where your code will impact real users.
🛠 ResponsibilitiesBuild and improve AI-powered journaling experiencesWrite, debug, and deploy full-stack features (Node.js, TypeScript, React)Integrate and fine-tune OpenAI/GPT-4 APIsUse Replit or Cursor for AI-assisted pair programming and code automationCollaborate closely with design and product on rapid experiments✅ RequirementsProficient in JavaScript/TypeScript and full-stack developmentExperience using Replit AI or Cursor AI for coding workflowsFamiliarity with AI/LLM concepts (OpenAI API, embeddings, prompt engineering)Comfortable working in fast-paced, startup-style environmentsPassion for building things that matter — especially in wellness, mental health, or AI🎁 Bonus PointsExperience with Postgres, Drizzle ORM, or Hume AIInterest in journaling, self-improvement, or meditationPrevious contributions to open-source or indie projects🧠 What You’ll GainOwnership of real AI features shipped to usersMentorship from a small but elite founding teamAccess to ZenQuill’s entire product and technical roadmapResume-building experience with bleeding-edge LLMs and product developmentPotential pathway into a future paid role
Apply NowSend a short message about why this excites you + a GitHub or project link to: tony@zenquill.ai",https://www.linkedin.com/job-apply/4255356933
Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer - Remote,https://www.linkedin.com/jobs/view/4255156509,4255156509,"Decatur, GA",Remote,,2025-06-23 14:10:03,False,,"Lensa partners with DirectEmployers to promote this job for TWO95 International.

Job Title: Power BI Developer

Location: Remote

Type: Long Term Contract

Rate: $Open /hr.

Expertise as a PowerBI developer with experience as a sole contributor in the design and develop of reports and dashboards Strong proficiency in data modeling, report building, and data visualization and reporting Ability to organize and structure data to be easier to work with and understand; design interactive Power BI dashboards Expertise in DAX functions including complex formulas 

Note: If interested please send your updated resume and include your rate requirement along with your contact details with a suitable time when we can reach you. If you know of anyone in your sphere of contacts, who would be a perfect match for this job then, we would appreciate if you can forward this posting to them with a copy to us.

We look forward to hearing from you at the earliest!

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/10384fa25f2c46f59641e3e63bb7655atjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Claritev,https://www.linkedin.com/company/claritev/life,Data Engineer (remote),https://www.linkedin.com/jobs/view/4245580105,4245580105,"Naperville, IL",Remote,$80K/yr - $100K/yr,2025-06-05 21:46:26,False,,"At Claritev, we pride ourselves on being a dynamic team of innovative professionals. Our purpose is simple - we strive to bend the cost curve in healthcare for all. Our dedication to service excellence extends to all our stakeholders - internal and external - driving us to consistently exceed expectations. We are intentionally bold, we foster innovation, we nurture accountability, we champion diversity, and empower each other to illuminate our collective potential.

Be part of our amazing transformational journey as we optimize the opportunity towards becoming a leading technology, data, and innovation voice in healthcare. Onward and Upward!!!

Job Roles And Responsibilities

 Understand business processes and how they are modeled in various systems Work with business users, technology teams, and executives to understand their data needs to create innovative solutions to fulfil them Implement data structures, workflows, and integrations between enterprise platforms to ensure the accurate and timely execution of business processes. Maintain scalable data pipelines to support continuing increases in data volume and complexity. Adhere to established best practices on data integration/engineering, as well as the future of our data infrastructure Managing and improving the performance of our database, queries, tools, and solutions Creating and maintaining data warehouse, databases, tables, SQL queries, and ingestion pipelines to power report, dashboards, predictive models, and downstream analysis Writing complex and efficient queries to transform raw data sources into easily accessible models for our teams and reporting platforms Prepare data for predictive and prescriptive modeling Identify and analyze data patterns Identify ways to improve data reliability, efficiency and quality Work with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure Collaborate, coordinate, and communicate across disciplines and departments. Ensure compliance with HIPAA regulations and requirements. Demonstrate Company's Core Competencies and values held within. Please note due to the exposure of PHI sensitive data -- this role is considered to be a High Risk and priveleged Role. The position responsibilities outlined above are in no way to be construed as all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.

,

JOB REQUIREMENTS (Education, Experience, And Training)

 Minimum high school diploma and four (4) years' related experience, three (3) of which should be inclusive of experience with OOP, SQL, schema designing, data modeling, designing, building, and maintaining data processing systems. Bachelors' degree in computer science, information technology or a similarly relevant field is highly preferred. Experience with advanced analytics tools with Python and PySpark. Experience using SQL, SPARK, and Azure Data Factory (ADF). Experience in triaging data issues, analyzing end-to-end data pipelines and working with business users in resolving issues. Experience in working with data governance/data quality and data security teams and specifically data stewards and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification. Experience with Databricks and SSIS are nice to have. Exposure to Big Data Development using Hive, Impala, Spark, and familiarity with Kafka Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics Excellent communication skills (verbal, listening and written) Ability to build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. Ability to work with both IT and business in integrating analytics and data science output into business processes and workflows. An agile learner who brings strong problem-solving skills and enjoys working as part of a technical, cross functional team to solve complex data problems Able to prioritize and manage multiple projects and requests at any one time Strong attention to detail when identifying data relationships, trends, and anomalies. Thinking through long-term impacts of key design decisions and handling failure scenarios. Ability to effectively share technical information, communicate technical issues and solutions to all levels of business Ability to meet strict deadlines, work on multiple tasks and work well under pressure

Compensation

The salary range for this position is $80-100k. Specific offers take into account a candidate’s education, experience and skills, as well as the candidate’s work location and internal equity. This position is also eligible for health insurance, 401k and bonus opportunity.

Benefits

We realize that our employees are instrumental to our success, and we reward them accordingly with very competitive compensation and benefits packages, an incentive bonus program, as well as recognition and awards programs. Our work environment is friendly and supportive, and we offer flexible schedules whenever possible, as well as a wide range of live and web-based professional development and educational programs to prepare you for advancement opportunities.

Your Benefits Will Include 

Medical, dental and vision coverage with low deductible & copayLife insurance Short and long-term disabilityPaid Parental Leave401(k) + matchEmployee Stock Purchase PlanGenerous Paid Time Off - accrued based on years of serviceWA Candidates: the accrual rate is 4.61 hours every other week for the first two years of tenure before increasing with additional years of service10 paid company holidaysTuition reimbursementFlexible Spending AccountEmployee Assistance ProgramSick time benefits - for eligible employees, one hour of sick time for every 30 hours worked, up to a maximum accrual of 40 hours per calendar year, unless the laws of the state in which the employee is located provide for more generous sick time benefits 
EEO STATEMENT

Claritev is an Equal Opportunity Employer and complies with all applicable laws and regulations. Qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability or protected veteran status. If you would like more information on your EEO rights under the law, please click here.

APPLICATION DEADLINE

We will generally accept applications for at least 15 calendar days from the posting date or as long as the job remains posted.

",https://recruiting.adp.com/srccar/public/RTI.home?r=5001125594906&c=1204301&d=External&rb=LinkedIn
CVS Health,https://www.linkedin.com/company/cvshealth/life,Associate Data Engineer,https://www.linkedin.com/jobs/view/4255339801,4255339801,"Austin, TX",Remote,$61.8K/yr - $123.6K/yr,2025-06-23 20:51:39,False,,"At CVS Health, we’re building a world of health around every consumer and surrounding ourselves with dedicated colleagues who are passionate about transforming health care.

As the nation’s leading health solutions company, we reach millions of Americans through our local presence, digital channels and more than 300,000 purpose-driven colleagues – caring for people where, when and how they choose in a way that is uniquely more connected, more convenient and more compassionate. And we do it all with heart, each and every day.

Position Summary

Designing and building data pipelines to load data from various sources into the EDP layer.Work on processing and transformation of the raw data to make it ready for consumption by the end users.Work on automating the data retrieval processes.Work on re-engineering the pipelines from the existing Enterprise Data Warehouse platform to BigQuery.Work on on-boarding the users in the BigQuery platform.Tuning the existing processes in order to reduce the overhead on the platform.

Required Qualifications

Working proficiency with Teradata, BigQuery, Python, Tableau, and SAS

Preferred Qualifications

Thorough understanding of the BigQuery architecture

Education

Bachelor's degree or higher

Anticipated Weekly Hours

40

Time Type

Full time

Pay Range

The Typical Pay Range For This Role Is

$61,800.00 - $123,600.00

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

Our people fuel our future. Our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.

Great Benefits For Great People

We take pride in our comprehensive and competitive mix of pay and benefits – investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. In addition to our competitive wages, our great benefits include:

Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan.No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.

For more information, visit https://jobs.cvshealth.com/us/en/benefits

We anticipate the application window for this opening will close on: 07/04/2025

Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.",https://jobs.cvshealth.com/us/en/job/CVSCHLUSR0619080EXTERNALENUS/Associate-Data-Engineer?utm_source=Linkedin&utm_medium=phenom-feeds
Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Engineer, Entry Level",https://www.linkedin.com/jobs/view/4255204410,4255204410,"Newport Beach, CA",Remote,,2025-06-23 15:46:50,False,,"At Jobright, we help high-growth startups hire top talent for their key roles.
About MIG Real EstateMIG Real Estate (“MIG”) is a private real estate investment company headquartered in Newport Beach, California, with over $3.4 billion of real estate investments in multifamily, industrial, office, hotel and retail properties located primarily in the Western U.S. and Sun Belt. The Company is dedicated to acquiring and repositioning real estate in a way that transforms communities, creatively enhances quality of life and dramatically improves the environments where our customers live, work and play. The company is focused on significant growth through acquisitions, redevelopment and repositioning of assets, continuously creating value with long-term stability throughout all market cycles.
Role SnapshotAs a Data Engineer on our team, you will be instrumental in building scalable data pipelines, modeling clean and reliable datasets, and helping ensure that MIG’s analytics infrastructure continues to evolve alongside our growing business. This role requires a strong technical foundation, attention to detail, and excellent interpersonal skills to collaborate across technical and non-technical teams.
Key ResponsibilitiesIngest data from multiple structured and unstructured sources (e.g., APIs, flat files, cloud services, etc.).Implement pipeline orchestration, scheduling, and monitoring to ensure timely data delivery.Develop and maintain well-structured data models that serve reporting and analytics needs.Collaborate with technology team and business users to translate reporting requirements into dimensional models.Write complex SQL queries, stored procedures, and CTEs to perform robust data transformations.Develop reusable SQL patterns and documentation to promote best practices within the team.Perform root cause analysis and data lineage tracing for issues identified by users or systems.Contribute to the documentation of data dictionaries, business logic, and metadata.Manage compute and storage resources to balance cost and performance.
Must-Have QualificationsBachelor’s degree or equivalent hands-on experience.1-3 years’ experience in a data engineering type role, minimum.Demonstrated ability to work closely with users and stakeholders to translate business needs into technical solutions.Experience in Python for API and pipeline scripting.Advanced in SQL, including performance tuning and query optimization.Strong data modeling skills.Hands-on experience with cloud data tools such as Azure Synapse / Data Factory, Databricks, or comparable platforms.Strong project execution abilities and a track record of cross-functional collaboration.Willing & able to travel domestically 15-20% of the year.
Preferred QualificationsExperience in the real estate or property asset management industry.Exposure to Power BI, DAX, or Microsoft Fabric is a plus.Familiarity with Git-based version control and CI/CD practices.
Why Join MIG?Join a values-driven, people-first organization that fosters professional development and encourages autonomy.Make an immediate impact on real estate investment strategy through high-visibility data initiatives.Work with a modern cloud-based data stack in a growth-oriented environment.Competitive compensation, performance-based bonuses, comprehensive benefits, and generous PTO.",https://jobright.ai/jobs/info/685448af8cb3a6046c704d4d?utm_source=1124&utm_campaign=685448af8cb3a6046c704d4d&tob=true
Jobot,https://www.linkedin.com/company/jobot/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4255094452,4255094452,"Harrison, NY",On-site,$120K/yr - $135K/yr,2025-06-23 15:18:02,True,over 100 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Come join one of the largest community-based, non-profit organizations in the United States!

This Jobot Job is hosted by Michaela Finn

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $120,000 - $135,000 per year

A Bit About Us

Come join one of the largest community-based, non-profit organizations in the United States!



Why join us?


Mission driven, excellent work life balance

Benefits start day 1, strong benefits

Competitive base salary

Tuition reimbursement

403-B match after 1 year 5% (an additional 1% after each year

Job Details

Job Details

Our firm is seeking a dynamic and experienced Senior Data Engineer to join our team. Senior Data Engineer, you'll be a pivotal figure in defining and advancing our data infrastructure vision. Reporting to the Director of Data Engineering, your role will be crucial in designing, implementing, and refining databases, data pipelines, and data interfaces to ensure scalability and performance. Your proficiency in SQL, Python, and cloud environments (Azure, AWS, or Google Cloud) will empower you to develop solutions that are both robust and

optimally aligned with our strategic goals.

Responsibilities

 Data Pipeline Design & Optimization Design, implement, and optimize robust and scalable data pipelines using SQL, Python, and cloud-based ETL tools such as Databricks. Develop and refine data models to accurately represent business processes. Develop set processes for data mining, data modeling, and data production. Translate complex functional and technical requirements into detailed architecture and design. Ensure systems meet business requirements and industry practices. Integrate up-to-date data management technologies and software engineering tools into existing structures. Create custom software components and analytics applications. Research opportunities for data acquisition and new uses for existing data. Develop data set processes for data modeling, mining, and production. Employ a variety of languages and tools to marry systems together. Recommend ways to improve data reliability, efficiency, and quality. Collaborate with data scientists on big data initiatives and implement new technologies as needed.

Qualifications

 Bachelor’s degree in Computer Science, Engineering, or a related field. 5+ years of experience in a Data Engineer role Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' data pipelines, architectures, and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Proficient in Python and data-related libraries. Experience with big data tools Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services EC2, EMR, RDS, Redshift. Experience with stream-processing systems Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages Python, Java, C++, Scala, etc. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255094452
Elder Research,https://www.linkedin.com/company/elder-research-inc/life,Data Engineer (Remote),https://www.linkedin.com/jobs/view/4255347607,4255347607,"Arlington, VA",Remote,,2025-06-23 21:19:04,False,,"Job Title: Data Engineer

Workplace: Remote (preference for candidates located in the National Capital Region)

Clearance Required: must have an active Secret clearance

Position Overview

Supports the Department of Defense's (DoD) Chief Digital and Artificial Intelligence Officer's (CDAO) Mission Analytics / Core Analytics Products, Search Products Portfolio, and Unstructured Data Search Enhanced by AI/ML (UD/SEAM).

Major Duties/Tasks

Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy mission partner requirements and support a data analytics and DevOps pipeline to drive rapid delivery of functionality to the client;Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities;Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and servicesWork under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment;Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles;Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutionsMaintain an existing collection of web scraping tools used as the initial step of the ETL processIdentify and implement scalable and efficient coding solutions

Requirements

Required Education:

Bachelors degree in computer science, computer engineering, Mathematics, statistics, data science or field directly related to the position plus 5-7 years experience, or a Masters Degree plus 3 years of experience.

Required Skills/Experience

Experience with Big Data systems, including Apache Spark / Databricks;Experience with ETL processes;Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0;Applying DoD Security Technical Implementation Guides (STIGs) and automating that processExperience with multiple coding languages

Why apply to this position at Elder Research?

Competitive Salary: for a mid-level Data EngineerImportant Work / Make a Difference: develop AI enhanced search capabilities for the Department of DefenseJob Stability: Elder Research is not a typical government contractor, we hire you for a career not just a contract.Remote Work: in an industry of declining remote work opportunities.People-Focused Culture: we prioritize work-life-balance and provide a supportive work environment as well as opportunities for professional growth and advancement.Company Stock Ownership: all employees are provided with shares of the company each year based on company value and profits.

About Elder Research, Inc

People Centered. Data Driven

Elder Research is a fast growing consulting firm specializing in predictive analytics. Being in the data mining business almost 30 years, we pride ourselves in our ability to find creative, cutting edge solutions to real-world problems. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.

Our team members are passionate, curious, life-long learners. We value humility, servant-leadership, teamwork, and integrity. We seek to serve our clients and our teammates to the best of our abilities. In keeping with our entrepreneurial spirit, we want candidates who are self-motivated with an innate curiosity and strong team work.

Elder Research believes in continuous learning and community - each week the entire company attends a �Tech Talk� and each office location provides lunch. Elder Research provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and extremely flexible time off - Elder Research enables and encourages its employees to serve others and enjoy their lives.

Elder Research, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Elder Research is a Government contractor and many of our positions require US Citizenship.",https://www.adzuna.com/details/5092494309?v=B993B703D56AB21AC4630FFB50B3CC44E72CE23E&r=19648398&frd=b6bf1001a4728a49a04181629106bdc9&ccd=ebb13a345bf64c535ed6cb38f077bcc1&utm_source=linkedin7&utm_medium=organic&chnlid=1931&title=Data%20Engineer%20%28Remote%29&a=e
Robert Half,https://www.linkedin.com/company/robert-half-international/life,Data Engineer,https://www.linkedin.com/jobs/view/4255240545,4255240545,"Irving, TX",Hybrid,$53/hr - $58/hr,2025-06-23 20:37:21,True,over 100 applicants,"We are seeking a highly motivated and enthusiastic Data Engineer to join our data team. This is an excellent opportunity in building and maintaining data pipelines within a modern cloud-based data ecosystem. You will play a key role in ensuring our data is accurate, accessible, and ready for analysis, supporting various business functions.
Responsibilities:Assist in the design, development, and maintenance of ETL (Extract, Transform, Load) processes to ingest data from various sources into our data warehouse.Write and optimize Python scripts for data extraction, transformation, and loading, ensuring data quality and efficiency.Work with our Azure cloud platform to provision, configure, and manage data services such as Azure Data Lake Storage, Azure SQL Database, and potentially Azure Data Factory.Develop and maintain data schemas and tables within Snowflake, our cloud data warehouse.Collaborate with senior data engineers and data analysts to understand data requirements and assist in translating them into technical solutions.Monitor data pipelines for performance and data integrity issues, troubleshooting and resolving problems as they arise.Participate in data quality initiatives, identifying and addressing discrepancies in data.Document data flows, ETL processes, and data models to ensure clear understanding and maintainability.Learn and apply best practices in data engineering, data governance, and data security.Contribute to the continuous improvement of our data platform and tooling.
Required Skills and Experience:Bachelor's degree in Computer Science, Engineering, Information Systems, or a related technical field.3-5 years of professional experience in a data-related role (e.g., intern, data analyst, entry-level developer) with a strong desire to specialize in data engineering.Strong foundational knowledge of Python and experience with its data manipulation libraries (e.g., Pandas).Understanding of ETL (Extract, Transform, Load) concepts and data warehousing principles.Familiarity with cloud platforms, specifically Microsoft Azure, including an understanding of services like Azure Data Lake Storage or Azure SQL Database.Basic understanding of relational databases and SQL.Eagerness to learn and work with Snowflake as a cloud data warehouse.Excellent problem-solving skills and a keen eye for detail.Strong communication and collaboration skills, with the ability to work effectively in a team environment.",https://www.linkedin.com/job-apply/4255240545
Jobot,https://www.linkedin.com/company/jobot/life,Data Platform Engineer,https://www.linkedin.com/jobs/view/4255098435,4255098435,"Lubbock, TX",Remote,$130K/yr - $160K/yr,2025-06-23 15:17:46,True,33 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Established company, HQ in Austin, that is innovating the preventive healthcare space seeks a Data Platform Engineer to join their team!

This Jobot Job is hosted by Rachel Hilton Berry

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $130,000 - $160,000 per year

A Bit About Us

We are seeking a dynamic and highly skilled Data Platform Engineer to join our rapidly growing team in the healthcare industry. This is a permanent, full-time position where you'll be at the forefront of revolutionizing healthcare data management. You will be working on cutting-edge technologies and will have the opportunity to make a significant impact in a fast-paced, innovative AI environment.



Why join us?


Innovative health screening company.Looking to innovative with AI.Stable company with 25+ years in the making

Job Details

Responsibilities

 Designing, developing, and maintaining scalable data platforms using Azure Fabric, PowerBI, Spark, Databricks, and other advanced technologies. Implementing robust data governance strategies to ensure data integrity, accessibility, and protection. Ensuring compliance with all relevant data security regulations and standards. Leveraging Python, Java, or Scala to develop data processing algorithms and ETL processes. Collaborating with cross-functional teams to understand data needs and deliver comprehensive solutions. Troubleshooting and resolving data-related issues in a timely manner. Continually monitoring the latest trends in data engineering and implementing innovative solutions to enhance data operations.

Qualifications

The ideal candidate for this role should have

 A bachelor's degree in Computer Science, Data Science, or a related field. A master's degree is a plus. A minimum of 5 years of experience in data platform engineering, preferably in the healthcare sector. Proficiency in Azure Fabric, PowerBI, Spark, Databricks, and other data platform technologies. Strong knowledge of data governance principles and practices. Experience in ensuring data security and compliance in a highly regulated industry. Excellent programming skills in Python, Java, or Scala. Exceptional problem-solving skills, attention to detail, and the ability to handle multiple tasks simultaneously. Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.

This is an exciting opportunity for a seasoned Data Platform Engineer who is passionate about leveraging data to drive healthcare innovation. If you're ready to take your career to the next level and make a real difference in the healthcare industry, we'd love to hear from you.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255098435
The Judge Group,https://www.linkedin.com/company/the-judge-group/life,Data Center Engineer,https://www.linkedin.com/jobs/view/4253325485,4253325485,"Rochelle, IL",On-site,$45/hr - $55/hr,2025-06-23 21:53:14,True,45 applicants,"Our Client is currently seeking a Data Center Engineer
This position is onsite in Rochelle, IL.
Key Competencies8+ years managing physical infrastructure in large data centersExpert knowledge in structured cabling (fiber/copper technologies and design)Expert knowledge in physical capacity management around cabling, space, power, and cooling
Qualifications8+ years’ experience with physical data center infrastructureStrong organizational skills, work prioritization, effective communication, and the ability to react quicklyExperience working with ServiceNowExperience with change managementAbility to lift, carry, and move up to 50 pounds when neededMaintain a positive work environment, encourage healthy organizational morale, and promote adherence to all policies and proceduresThrive in a fast-paced environment with tight timelines and changing prioritiesUnderstanding of core network design and topologyData center infrastructure utilization such as back-up power, generator technology,Experience managing a teamStrong, interpersonal, written, and communication skillsAbility to work weekends/off-hours and be on-call for emergenciesExpert knowledge of structured cabling, fiber and copperExpert knowledge of data center power and cooling designExperience working with high and low voltage cabling vendorsExperience with vendor managementExperience with data center migrationsExperience with DCIM tools (i.e. Nlyte, Device42, etc.)All work to be conducted on-siteRate: $45-55/HR",https://www.linkedin.com/job-apply/4253325485
Jobot,https://www.linkedin.com/company/jobot/life,Power BI report developer w/ SAP experience,https://www.linkedin.com/jobs/view/4255096280,4255096280,"Nashville, TN",On-site,$90K/yr - $100K/yr,2025-06-23 15:17:49,True,34 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Hybrid!

This Jobot Job is hosted by Ellie Staver

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $90,000 - $100,000 per year

A Bit About Us

This client of Jobot is a growing manufacturing company that has seen 3 years of consistent growth and poised for continued growth for the foreseeable future. We are seeking a highly skilled Reporting Specialist to design, develop, and maintain reports and dashboards using SAP and Power BI. This role is essential in helping business stakeholders effectively access and utilize data to make informed decisions. The ideal candidate will have strong expertise in both SAP and Power BI platforms and the ability to work closely with various business users to understand and meet their reporting needs through innovative data visualizations. Our client is a growing manufacturing company that implemented SAP a year ago. They pull their SAP data into PowerBI for reporting on all things.



Why join us?


Hybrid remote - in office 3 days in Nashville, 2 days remote

Innovative company that is growing and keeping up to date on tech!

Job Details

Key Responsibilities

Collaborate with various business units to gather reporting requirements and translate them into technical specifications.

Utilize SAP native tools and Power BI to design, develop, test, and maintain reports and dashboards.

Ensure the accuracy and integrity of data in all reports.

Optimize the performance of reports and dashboards for efficient data retrieval and display.

Integrate SQL queries into SAP and Power BI reports to enhance data accuracy and comprehensiveness.

Provide training and support for end-users on SAP and Power BI tools and techniques.

Stay up to date with the latest trends in SAP, Power BI, and SQL reporting and implement best practices for report design and development.

Key Skills & Competencies

Strong data visualization and analytical skills.

Expertise in SAP and Power BI.

Proficiency in SQL querying and database management.

Ability to work autonomously with minimal oversight.

Excellent problem-solving, troubleshooting, and communication skills.

Proven ability to collaborate with business users and IT teams.

Required Qualifications

5+ years of professional experience with data analytics, report development, and presentation in an SAP environment.

5+ years of Power BI experience, with proven experience in creating dashboards and reports.

A Bachelor’s degree in Information Technology, Computer Science, Business Administration, or a related field.

Relevant certifications in SAP, Power BI, and SQL are a plus.

Experience in database administration, automotive manufacturing, or data warehousing is a plus.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255096280
Lensa,https://www.linkedin.com/company/lensa/life,Remote Data Engineer- Analytics,https://www.linkedin.com/jobs/view/4255160071,4255160071,"Irving, TX",Remote,,2025-06-23 14:09:38,False,,"Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

Insight Global is seeking a Data Engineer to sit remotely and support a Fortune 100 Healthcare client. This individual will be supporting the underwriting team, specifically digging into client underwriting data to backtrack data sources to find where the data points are coming from, and then building out a data lineage for the underwriting side. This individual will be working closely with the finance and underwriting groups, and this role will be more analytics focused to begin with. Upon completion of the first phase, the position will transition into a true Data Engineer position where this individual will be developing data pipelines using Python and SQL.

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

5-7 years of experience as a Data Engineer Extensive experience with Python and SQL for pipeline creation Data analytics experience- specifically backtracking the data source to find where the data points are coming from. Experience with database management tools like SQL Server, Teradata Experience with Azure as cloud platform null 

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/7c2055bee0184e99b914356f9782ae8dtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
VBeyond Corporation,https://www.linkedin.com/company/vbeyond-corporation/life,Azure Data Engineer,https://www.linkedin.com/jobs/view/4255363044,4255363044,"Dallas, TX",On-site,,2025-06-23 22:40:56,True,44 applicants,"Role : Data Engineer LeadMode : FulltimeLocation : Dallas TX
Required Skills/Qualifications:7-12 years of relevant experienceBachelor's and/or master’s degree in computer science or equivalent experience.Strong communication, analytical and problem-solving skills with a high attention to detail.
Desired Experience:At least two years of experience building and leading highly complex, technical engineering teams.Strong hands-on experience in DatabricksImplement scalable and sustainable data engineering solutions using tools such as Databricks, Azure, Apache Spark, and Python. The data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases.Experience managing distributed teams preferred.Comfortable working with ambiguity and multiple stakeholders.Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.Expertise on Azure Cloud platformGood SQL knowledgeKnowledge on orchestrating workloads on cloudStrong experience with Unity CatalogAbility to set and lead the technical vision while balancing business drivers Strong experience with PySpark, Python programmingProficiency with APIs, containerization and orchestration is a plusExperience handling large and complex sets of data from various sources and databasesSolid grasp of database engineering and design principlesFamiliarity with CI/CD methods desiredGood to have Teradata Experience (not Mandatory)",https://www.linkedin.com/job-apply/4255363044
Synergis,https://www.linkedin.com/company/synergis/life,Staff Data Engineer,https://www.linkedin.com/jobs/view/4255211027,4255211027,"Atlanta, GA",Hybrid,,2025-06-23 16:50:03,True,over 100 applicants,"Staff Data EngineerLocation: Atlanta, GA (Vinings area - hybrid 3-4 days onsite per week)Perm Position (No sponsorship)
Staff Data Engineer Responsibilities:Own and drive the data architecture roadmap for applications and enterprise data platformsLead development and expansion of enterprise data platformsDefine vision and requirements for solutions addressing business challenges; evaluate, recommend, and roadmap optimal solutionsMeet with senior management to understand business drivers and functional requirementsGuide data design and architecture discussions with engineering teams to ensure secure and scalable solutionsCommunicate data architecture across stakeholders and collaborate with domain architects to formalize standardsLead data architecture, design, and code reviewsOversee delivery to ensure adherence to best practices, automation, quality, and operational readinessProvide technical leadership and mentorship to lead data engineers and broader technical teamsImplement dashboards to enhance data-driven decision makingLead AI use cases or proof-of-concept project implementationsDevelop using Python and/or JavaScriptBuild and maintain medallion architecture on AzureDesign and deliver PowerBI dashboards and reportsGather requirements and translate them into technical solutionsPresent and communicate effectively with executives and internal stakeholdersOther duties as assigned
Staff Data Engineer Requirements:Bachelor’s degree in Computer Science, Software Engineering, or related field (Master’s preferred)8+ years of experience in software engineering and technologyStrong knowledge of data management (governance, quality, modeling, integration)Expertise in data structures, object-oriented and functional programmingExperience developing and maintaining MDM systemsSkilled in Java, JEE, and modern features of functional programmingProficiency with Big Data/NoSQL tech (e.g., Cassandra, Big Table)Strong SQL experience, particularly in Azure Fabric or equivalent databasesExperience designing event-driven, pub/sub, and high-volume batch systemsExperience with enterprise data strategy, integrations, architecture, reporting, and analyticsUnderstanding of OLTP and OLAP workloads across RDBMS, NoSQL, Big Data, and cloud-native solutionsFamiliarity with data lake and SaaS integration platforms such as Snowflake or Azure SynapseExperience with automated testing (unit and integration)Hands-on cloud experience (Azure preferred, GCP or AWS acceptable)Knowledge of destructive testing methodologies and toolsExperience or familiarity with AI/ML and statistical modelingExcellent analytical, problem-solving, oral, and written communication skillsComfortable working in ambiguity and learning new skills outside comfort zoneCreative, innovative, and open-minded approach to technical problem-solving
Staff Data Engineer Preferred Experience:Proven success delivering dashboards and reports with PowerBIDemonstrated implementation of AI/ML proof-of-concept projectsStrong experience with Azure cloud services and medallion architecture
Synergis is proud to be an Equal Opportunity Employer. We value diversity and do not discriminate on the basis of race, color, ethnicity, national origin, religion, age, gender, gender identity, political affiliation, sexual orientation, marital status, disability, military/veteran status, or any other status protected by applicable law.For immediate consideration, please forward your resume to christy.cifreo@synergishr.comIf you require assistance or an accommodation in the application or employment process, please contact us at christy.cifreo@synergishr.com.The annual salary for this position is dependent on factors including but not limited to client requirements, experience, statutory considerations, and location).*Note: Disclosure as required by the Equal Pay for Equal Work Act (CO), NYC Pay Transparency Law, and sb5761 (WA)Synergis is a workforce solutions partner serving thousands of businesses and job seekers nationwide. Our digital world has accelerated the need for businesses to build IT ecosystems that enable growth and innovation along with enhancing the Total Experience (TX). Synergis partners with our clients at the intersection of talent and transformation to scale their balanced teams of tech, digital and creative professionals. Learn more about Synergis at www.synergishr.com.",https://www.linkedin.com/job-apply/4255211027
Gartner for HR,https://www.linkedin.com/company/gartner-for-hr/life,Sr Data Engineer,https://www.linkedin.com/jobs/view/4255249930,4255249930,"Irving, TX",Hybrid,$82K/yr - $107K/yr,2025-06-23 22:31:20,False,,"Hiring near our Irving, TX Center of Excellence, with a flexible environment. 

About Gartner IT

Join a world-class team of skilled engineers who build creative digital solutions to support our colleagues and clients. We make a broad organizational impact by delivering cutting-edge technology solutions that power Gartner. Gartner IT values its culture of nonstop innovation, an outcome-driven approach to success, and the notion that great ideas can come from anyone on the team.

About The Role

Senior Data engineer for production support who will provide daily end-to-end support for daily data loads & manage production issues.

What Will You Do

 Monitor & support various data loads for our Enterprise Data Warehouse.  Support business users who are accessing POWER BI dashboards & Datawarehouse tables. Handle incidents, service requests within defined SLAs.  Work with team on managing Azure resources including but not limited to Databricks, Azure Data Factory pipelines, ADLS etc.  Build new ETL/ELT pipelines using Azure Data Products like Azure Data Factory, Databricks etc.  Help build best practices & processes.  Coordinate with upstream/downstream teams to resolve data issues.  Work with the QA team and Dev team to ensure appropriate automated regressions are added to detect such issues in future.  Work with the Dev team to improve automated error handling so manual interventions can be reduced.  Analyze process and pattern so other similar unreported issues can be resolved in one go.  Responsible for platform-related activities, including performance tuning of daily loads to ensure optimal system efficiency.  Responsible for conducting impact analysis on changes initiated by the source team. 

What You Will Need

Strong IT professional with  3-4 years  of experience in Data Engineering. The candidate should have strong analytical and problem-solving skills.

Must Have

 3-4 years of experience in Data warehouse design & development and ETL/ELT using Azure Data Factory (ADF)  Experience in writing complex TSQL procedures on MPP platforms - Synapse, Snowflake etc.  Experience in analyzing complex code to troubleshoot failure and where applicable recommend best practices around error handling, performance tuning etc.  Ability to work independently, as well as part of a team and experience working with fast-paced operations/dev teams.  Good understanding of business process and analyzing underlying data Understanding of dimensional and relational modelling  Detailed oriented, with the ability to plan, prioritize, and meet deadlines in a fast-paced environment.  Knowledge of Azure cloud technologies  Exceptional problem-solving skills 

Nice To Have

 Experience crafting, building, and deploying applications in a DevOps environment utilizing CI/CD tools  Understanding of dimensional and relational modeling  Relevant certifications  Basic knowledge of Power BI. 

Who Are You

 Bachelor’s degree or foreign equivalent degree in Computer Science or a related field required  Excellent communication skills.  Able to work independently or within a team proactively in a fast-paced AGILE-SCRUM environment.  Owns success – Takes responsibility for the successful delivery of the solutions.  Strong desire to improve upon their skills in tools and technologies 

Don’t meet every single requirement? We encourage you to apply anyway. You might just be the right candidate for this, or other roles.

What You Will Get

 Competitive compensation.  Limitless growth and learning opportunities.  Ongoing mentorship and apprenticeship; Leadership courses, development programs, technical courses, certification opportunities and more!  A collaborative and positive culture - join a diverse team of professionals that are as smart and driven as you.  A chance to make an impact – your work will contribute directly to our strategy.  A flexible work environment—enjoy the flexibility of working from home and the energy of collaborating with peers in our dynamic offices.  20+ PTO days plus holidays and floating holidays in your first year.  Extensive medical, dental insurance and vision plan.  401K with corporate match, immediate vesting.  Health-and-wellness-related allowance programs.  Parental leave.  Tuition reimbursement.  Employee Stock Purchase Plan.  Employee Assistance Program.  Gartner Gives Charity Match. And much more! 

 Who are we? 

At Gartner, Inc. (NYSE:IT), we guide the leaders who shape the world.

Our mission relies on expert analysis and bold ideas to deliver actionable, objective insight, helping enterprise leaders and their teams succeed with their mission-critical priorities.

Since our founding in 1979, we’ve grown to more than 21,000 associates globally who support ~14,000 client enterprises in ~90 countries and territories. We do important, interesting and substantive work that matters. That’s why we hire associates with the intellectual curiosity, energy and drive to want to make a difference. The bar is unapologetically high. So is the impact you can have here.

 What makes Gartner a great place to work? 

Our sustained success creates limitless opportunities for you to grow professionally and flourish personally. We have a vast, virtually untapped market potential ahead of us, providing you with an exciting trajectory long into the future. How far you go is driven by your passion and performance.

We hire remarkable people who collaborate and win as a team. Together, our singular, unifying goal is to deliver results for our clients.

Our teams are inclusive and composed of individuals from different geographies, cultures, religions, ethnicities, races, genders, sexual orientations, abilities and generations.

We invest in great leaders who bring out the best in you and the company, enabling us to multiply our impact and results. This is why, year after year, we are recognized worldwide as a great place to work .

 What do we offer? 

Gartner offers world-class benefits, highly competitive compensation and disproportionate rewards for top performers.

In our hybrid work environment, we provide the flexibility and support for you to thrive — working virtually when it's productive to do so and getting together with colleagues in a vibrant community that is purposeful, engaging and inspiring.

Ready to grow your career with Gartner? Join us.

Gartner believes in fair and equitable pay. A reasonable estimate of the base salary range for this role is 82,000 USD - 107,000 USD. Please note that actual salaries may vary within the range, or be above or below the range, based on factors including, but not limited to, education, training, experience, professional achievement, business need, and location. In addition to base salary, employees will participate in either an annual bonus plan based on company and individual performance, or a role-based, uncapped sales incentive plan. Our talent acquisition team will provide the specific opportunity on our bonus or incentive programs to eligible candidates. We also offer market leading benefit programs including generous PTO, a 401k match up to $7,200 per year, the opportunity to purchase company stock at a discount, and more.

The policy of Gartner is to provide equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, ancestry, disability, veteran status, or any other legally protected status and to seek to advance the principles of equal employment opportunity.

Gartner is committed to being an Equal Opportunity Employer and offers opportunities to all job seekers, including job seekers with disabilities. If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access the Company’s career webpage as a result of your disability. You may request reasonable accommodations by calling Human Resources at +1 (203) 964-0096 or by sending an email to ApplicantAccommodations@gartner.com .

Job Requisition ID:101148

By submitting your information and application, you confirm that you have read and agree to the country or regional recruitment notice linked below applicable to your place of residence.

Gartner Applicant Privacy Link: https://jobs.gartner.com/applicant-privacy-policy

 For efficient navigation through the application, please only use the back button within the application, not the back arrow within your browser.",https://jobs.gartner.com/jobs/job/101148-sr-data-engineer/
Apexon,https://www.linkedin.com/company/apexon/life,Sr. SnowFlake Data Engineer,https://www.linkedin.com/jobs/view/4255203854,4255203854,"Sacramento, CA",Hybrid,,2025-06-23 16:12:57,True,over 100 applicants,"Role: Sr. Snowflake Data EngineerLocation- SACRAMENTO, CA (HYBRID)Position – 1West Coast/CA/Sacramento preferred.Responsibilities:Perform data engineering in support of a migration effort between client's legacy platform (in Microsoft) to Snowflake.Experience with AWS Glue as the ETL Tool Experience with AWS S3 and Snowflake Minimum Qualifications: Minimum three (3) years of experience in developing and monitoring data pipelines, ETL/ELT (Extract Transform Load / ExtractMinimum (3) years of experience with Microsoft SQL Server and T-SQL.Minimum (3) years of experience developing in ETL Tools.Minimum (3) years of experience developing Rest APIs, including experience with JSON and C#.Minimum (1) year of experience in database design, database development, and database security.Minimum (1) year of experience working with data conversion efforts involving legacy data formats and legacy data repositories such as Mainframe.Minimum (1) year of experience in Container Platforms such as Kubernetes, OpenShift, Docker, or in Cloud Computing.Minimum (1) year of experience with Microsoft Visual Studio and Git Code Repositories.Minimum (1) year of experience with SaaS Cloud Database in either AWS or AzureBachelor’s Degree in computer science, engineering, information systems, math or a technology- related field.",https://www.linkedin.com/job-apply/4255203854
Mastech Digital,https://www.linkedin.com/company/mastech/life,Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4252686860,4252686860,"Sacramento, CA",Hybrid,,2025-06-23 15:42:23,True,over 100 applicants,"Title: Snowflake Data Engineer Duration: 6 Months + Location: Sacramento ,CA (ONLY W2)
Job Description: Perform data engineering in support of a migration effort between client's legacy platform (in Microsoft) to Snowflake.Experience with AWS Glue as the ETL Tool Experience with AWS S3 and Snowflake 
Required Skills: At least three (3) years of experience in developing and monitoring data pipelines, ETL/ELT (Extract Transform Load / ExtractAt least three (3) years of experience with Microsoft SQL Server and T-SQL.At least three (3) years of experience developing in ETL Tools.At least three (3) years of experience developing Rest APIs, including experience with JSON and C#.At least one (1) year of experience in database design, database development, and database security.At least one (1) year of experience working with data conversion efforts involving legacy data formats and legacy data repositories such as Mainframe.At least one (1) year of experience in Container Platforms such as Kubernetes, OpenShift, Docker, or in Cloud Computing.At least one (1) year of experience with Microsoft Visual Studio and Git Code Repositories.At least one (1) year of experience with SaaS Cloud Database in either AWS or Azure",https://www.linkedin.com/job-apply/4252686860
Cloudflare,https://www.linkedin.com/company/cloudflare/life,Systems Engineer - BI Team,https://www.linkedin.com/jobs/view/4253312901,4253312901,"Austin, TX",Hybrid,,2025-06-23 19:58:02,False,,"About Us

At Cloudflare, we are on a mission to help build a better Internet. Today the company runs one of the world’s largest networks that powers millions of websites and other Internet properties for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.

We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!

Location - Austin, TX Must be willing to relocate if not in area.

About The Team

The Business Intelligence team at Cloudflare is responsible for building a centralized cloud data lake and an analytics platform that enables our internal Business Partners and Product teams with actionable insights and also provides a 360 view of our business. Our goal is to democratize data, support Cloudflare’s critical business needs, provide reporting and analytics via self-service data applications to fuel existing and new business critical initiatives.

About The Role

We are looking for a Systems Engineer to join our Austin team to scale our development efforts on Platform Tooling initiatives. You will work on building data application tools and frameworks which empowers our Internal Teams. You will work closely with the Data Engineers on the team to add new features and maintain our data pipelines written in Go. You can expect to interact with various languages and technologies including, but not limited to Go, React, and Clickhouse.

What You'll Do

Design and implement key features of our internal applicationsWork closely with a cross functional team to design features and solutions that solve critical business problemsContribute in improving an evolving platform architecture for scalability, observability and reliabilityContinuously involved in knowledge sharing and mentorship


Examples Of Desirable Skills, Knowledge And Experience

B.S. or M.S in Computer Science, Statistics, Mathematics, or other quantitative fields Experience working with Go, Python, Java, or equivalent programming languagesKnowledge of SQL and common relational database systems such as PostgreSQL and MySQLExperience with CI/CD, Gitlab CIExcellent communication & problem solving skills Ability to collaborate with cross functional teams and work through ambiguous business requirements


Bonus Points

Familiarity with container based deployments such as Docker & KubernetesFamiliarity with Google Cloud Platform or something similarFamiliarity with Javascript, Typescript, and React


What Makes Cloudflare Special?

We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.

Project Galileo: Since 2014, we've equipped more than 2,400 journalism and civil society organizations in 111 countries with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.

Athenian Project: In 2017, we created the Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration. Since the project, we've provided services to more than 425 local government election websites in 33 states.

1.1.1.1: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.

Sound like something you’d like to be a part of? We’d love to hear from you!

This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.

Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.

Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.",https://boards.greenhouse.io/cloudflare/jobs/7000601?gh_jid=7000601&gh_src=5ylsd31
Elevait Solutions,https://www.linkedin.com/company/elevaittech/life,Data Engineer,https://www.linkedin.com/jobs/view/4255203617,4255203617,"California, United States",Remote,,2025-06-23 15:46:12,True,over 100 applicants,"Elevait Solutions is hiring a Data Engineer to build and manage scalable data pipelines and lead the orchestration of our Snowflake data platform. If you’re passionate about data infrastructure, automation, and engineering excellence – we want to hear from you!
What You’ll DoBuild and manage data pipelines with strong SLAs.Administer and monitor Snowflake (RBAC, warehouses, usage).Develop CI/CD pipelines using GitHub, Docker, and Kubernetes.Automate data ingestion workflows via Airflow.Design and maintain dbt models.Write test cases to ensure data accuracy and consistency across layers.Document full data flows, test strategies, and deployment processes.
Must-Have SkillsStrong hands-on experience with Snowflake, especially RBAC and warehouse monitoring.Proven ability to set up GitHub-based CI/CD pipelines.Working knowledge of Docker, Kubernetes (bonus).Experience building robust workflows using Airflow.Solid development experience with dbt.Excellent documentation habits.
Nice-to-HaveProficient in Python and SQL.Understanding of data models, ETL/ELT pipelines, and analytical workflows.Strong analytical mindset with a problem-solving attitude.",https://www.linkedin.com/job-apply/4255203617
Analytica,https://www.linkedin.com/company/analytica-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4246666534,4246666534,United States,Remote,$120K/yr - $130K/yr,2025-06-07 17:52:51,False,,"Analytica is seeking a Data Engineer to support a complex data program for the Defense Health Agency. This role will work closely with reporting and analytics developers, data governance and data architects to build robust, high quality data pipelines that enhance productivity and operational efficiency. This position is for US Citizens only and candidate will hold (or be able to hold) a US Secret Clearance. The position is a remote position with occasional onsite meetings in at San Antonio, TX as required.
Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.
Responsibilities: Support the Military Health System (MHS) by supporting data engineering that enhance the Military Health System's (MHS) ability to leverage data as a strategic assetEnsure data interoperability, governance, and quality to support enterprise-wide decision-making and healthcare optimization play a critical role in designing and implementing data architecture frameworks that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset.Design and build robust and scalable data pipelines for managing structured and unstructured data using traditional databases (Oracle, PostgreSQL, etc.) or cloud Databases such as Amazon Redshift or AWS VerticaTranslate business needs into:data architecture solutions development within supported data systems.data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teamsMonitor and troubleshoot data import, analysis, and display errors.
Required Qualifications: Bachelor’s degree in information technology, Computer Science, Engineering or equivalent technical field3+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred)2+ years experience with Python and SQL (Java and Python preferred)Experience working on relational NoSQL and SQL databasesExperience designing and implementing various data pipeline patterns and strategiesStrong knowledge of data security principlesPrior experience with DHA M2 or MDR databases, CERNER EHR a strong plusMust be a US Citizen and must possess or be eligible for a SECRET clearance
About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

Analytica LLC is an Equal Opportunity Employer. We are committed to providing equal employment opportunities to all individuals, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other characteristic protected by applicable federal, state, or local law. As a federal contractor, we comply with the Vietnam Era Veterans' Readjustment Assistance Act (VEVRAA) and take affirmative action to employ and advance in employment qualified protected veterans.We ensure that all employment decisions are based on merit, qualifications, and business needs. We prohibit discrimination and harassment of any kind. Analytica LLC also provides reasonable accommodations to applicants and employees with disabilities, in accordance with applicable lawsWhen receiving email communication from Analytica, please ensure that the email domain is analytica.net to verify its authenticity.",https://analyticallc.applytojob.com/apply/gSgWiMyGlM/Data-Engineer
Lensa,https://www.linkedin.com/company/lensa/life,Jr. Data Engineer,https://www.linkedin.com/jobs/view/4255157375,4255157375,"San Antonio, TX",On-site,,2025-06-23 14:10:15,False,,"Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

This is a unique opportunity to join a high-impact team working on critical data pipelines and transformations that support information security, fraud detection, and access management. Youll work with a modern data stack and gain exposure to cloud technologies, data reliability practices, and business-facing analytics. Data is generally clean, but candidate may handle some transformations, error handling, and sensitive data scrubbing. Role involves working with information security data sets (member and employee access management, external/internal fraud). Some knowledge of fraud models and machine learning is a plus, but not mandatory.

Build and maintain data pipelines using Snowflake, DBT, and other tools in our stack (e.g., DataStage, Talend, Kafka). Support data transformations for reporting (DL4), error handling, and sensitive data scrubbing. Collaborate with cross-functional teams to deliver clean, reliable data for internal and external fraud detection and access management. Contribute to monitoring and reliability efforts, with opportunities to learn tools like Grafana, Fortuna, and Datadog. Work in a cloud-first environment, applying best practices in data engineering and security. 

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

Technical Requirements

Core Technologies: Proficiency with Snowflake and DBT.

Experience with DataStage, Talend, and Kafka (receiving side).

Monitoring & Reliability: Proficient in Grafana, Fortuna, or Datadog for SLO/SLA monitoring.

Familiarity with Google SRE or data reliability engineering principles.

Project Focus Areas: experience in information security datasets (e.g., access management, fraud detection).

Handling clean but sensitive data, including transformations, error handling, and scrubbing.

Supporting business-facing data transformations (DL4/data for reporting).

Cloud Experience: Required, especially in the context of Snowflake/DBT deployments. AI + Machine Learning: Some knowledge of fraud models or ML concepts is a plus, but not required.

Certifications in tools like Snowflake or DBT null

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/e7dcf9a7f74c45a2b0686731b0b9b4catjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Snowflake Data engineer,https://www.linkedin.com/jobs/view/4255231812,4255231812,United States,Remote,,2025-06-23 20:09:15,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, VDart, Inc., is seeking the following. Apply via Dice today!

Job Title: Snowflake Data engineer

Location: Remote (Canada)

Duration/Term: Long-Term Contract

Job Summary:

We are looking for a seasoned Snowflake Data Engineer with strong expertise in Snowflake, SQL, and Informatica IICS to design, build, and optimize large-scale cloud-based data pipelines. This role involves end-to-end data integration, performance tuning, and data modeling to support enterprise analytics and reporting initiatives.

Key Responsibilities:

Write and optimize complex SQL queries and stored procedures for high-performance data processing.Design, develop, and manage ETL/ELT pipelines using Informatica IICS across hybrid and cloud environments.Architect scalable and efficient Snowflake data warehouses, including schema design, Snowpipe, Streams, Tasks, and tuning.Integrate Snowflake with cloud storage platforms like AWS S3 or Azure Blob Storage.Implement data modeling techniques such as star and snowflake schemas to support analytical workloads.Develop and maintain Snowflake objects including warehouses, databases, schemas, tables, and access control structures.Perform data quality checks, validation, and apply transformation rules to ensure data accuracy.Monitor and troubleshoot job performance and optimize compute resources across Snowflake and IICS.

Required Skills & Experience:

Proficiency in SQL, query performance tuning, and working with large datasets.Hands-on experience with Snowflake architecture, data pipelines, and automation.Strong experience using Informatica IICS for ETL/ELT in cloud and hybrid settings.Knowledge of data modeling principles, including dimensional modeling techniques.Familiarity with cloud data storage solutions like AWS S3 and Azure Blob.Experience in data integration, validation, and reconciliation techniques.Strong communication and coordination abilities across technical and business teams.

Key Skills:

SQL, Snowflake, Informatica IICS, Data Modeling, ETL/ELT Pipelines, Snowpipe, Streams & Tasks, Cloud Storage Integration (AWS/Azure), Performance Tuning, Data Quality

VDart Group, a global leader in technology, product, and talent management, empowers businesses with comprehensive solutions through our four distinct, industry-leading business units With a diverse team of over 4,000 professionals across 13 countries, we deliver strong results across various industries, including Fortune 500 companies

Committed to ""People, Purpose, Planet,"" we prioritize social responsibility and sustainability, as evidenced by our EcoVadis Bronze Medal Certification and participation in the UN Global Compact

Our dedication to delivering strong results has earned us recognition as a trusted advisor for businesses seeking to drive innovation and growth, including many Fortune 500 companies Join our network! Partner with VDart Group to leverage our global network, industry expertise, and proven track record with a diverse clientele",https://click.appcast.io/t/3q6ASzzHWn-iO60-vn7hSBzmFTQTkSkAoR8Fs49FHG4=
Onebridge,https://www.linkedin.com/company/onebridgetech/life,Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4252688821,4252688821,"Indiana, United States",Hybrid,,2025-06-23 15:45:52,True,over 100 applicants,"Onebridge is a Consulting firm with an HQ in Indianapolis, and clients dispersed throughout the United States and beyond. We have an exciting opportunity for a highly skilled Snowflake Data Engineer to join an innovative and dynamic group of professionals at a company rated among the top “Best Places to Work” in Indianapolis since 2015.
Employment Type: Full Time or ContractLocation: Indianapolis, IN - HybridIndustry: IT & Services
Snowflake Data Engineer | About YouAs a Snowflake Data Engineer, you are responsible for designing, building, and maintaining scalable, secure, and high-performing data infrastructure centered around Snowflake to support critical business priorities. You will play a key role in advancing the data products roadmap, implementing data-as-a-product and data mesh strategies, and driving governance and data quality initiatives. You thrive on solving complex data challenges by building efficient pipelines and developing actionable solutions that empower organizations to unlock the full potential of their data. A detail-oriented and results-driven professional, you excel in collaborative environments and leverage your technical expertise to drive impactful decisions and long-term success.Snowflake Data Engineer | Day-to-DayDesign, develop, and maintain scalable data pipelines using Snowflake as the core data platform.Build and manage ETL/ELT workflows and data integration processes using Informatica IICS and Snowflake’s native capabilities.Collaborate with stakeholders to prioritize and operationalize data use cases, showcasing data products.Document insights and deliverables to support data-as-a-product and data mesh strategies.Ensure data quality, governance, and security while expanding governance practices.Monitor and optimize system performance, resolving bottlenecks and driving efficiency.Snowflake Data Engineer | Skills & Experience5+ years of experience in data engineering, data infrastructure development, or a related field.Proficiency in Snowflake, including its architecture, features, and data services.Expertise in SQL and experience managing both relational and non-relational databases.Strong understanding of ETL/ELT processes and hands-on experience with Informatica IICS, Apache Airflow, or Apache Kafka.Familiarity with cloud platforms (AWS, Azure, or Google Cloud) and their integration with Snowflake.Excellent communication skills with the ability to synthesize complex information and document findings clearly.Hands-on experience with tools such as Power BI, Collibra, and big data technologies is a plus.
A Best Place to Work in Indiana since 2015",https://www.linkedin.com/job-apply/4252688821
Parallel Consulting,https://www.linkedin.com/company/parallel-consulting/life,Data Engineer,https://www.linkedin.com/jobs/view/4255148640,4255148640,New York City Metropolitan Area,On-site,,2025-06-23 13:27:31,True,over 100 applicants,"Data Engineer – Content & Analytics Platform
A leading technology company is seeking a skilled Data Engineer to support data-driven operations across a global digital platform. This role focuses on analyzing, transforming, and loading complex datasets, supporting data aggregation, statistics reporting, and data warehousing. A strong foundation in Oracle and PostgreSQL, including experience with PL/SQL packages, is essential.
You’ll be involved in building pipelines for content ingestion, statistical reporting, and customer-facing dashboards - playing a key role in helping content providers gain insight into how their materials are accessed and used.
This is a chance to join a forward-thinking team focused on solving real-world problems for digital content stakeholders. You'll have the opportunity to influence core data services within a mission-critical platform used by organizations worldwide.
Key ResponsibilitiesAnalyse and transform data from diverse sources for ingestion into a modern analytics platformDevelop and optimize ETL processes for large-scale data loads and warehousingImplement and manage data aggregations for analytics and usage reportingUse Oracle PL/SQL packages and procedures to support data workflowsDesign and maintain reporting structures for platform-level insightsTroubleshoot and resolve data quality issues across Oracle and PostgreSQL systemsCollaborate with developers and analysts to meet evolving data requirementsContribute to performance tuning, indexing strategies, and data integrity validationSupport data integration for new customers by mapping and transforming legacy datasets
Required Skills & ExperienceStrong hands-on experience with Oracle (PL/SQL) and PostgreSQLExcellent SQL skills and ability to write complex queries and data transformation logicSolid understanding of data aggregation, analytics, and warehousing techniquesFamiliarity with usage statistics standards (e.g., COUNTER) or similar reporting protocolsExperience handling large datasets in production environmentsAbility to debug and optimize ETL pipelinesKnowledge of XML-based content formats (e.g., JATS, ONIX, NLM) is a plusScripting knowledge (e.g., Python, Bash) for automation is desirable
Preferred QualificationsDegree in Computer Science, Data Engineering, Information Systems, or a related fieldExperience working in publishing, digital content platforms, or scholarly communicationUnderstanding of metadata and data models used in academic or STM publishing",https://www.linkedin.com/job-apply/4255148640
OSI Engineering,https://www.linkedin.com/company/osi-engineering-inc./life,Data Engineer III,https://www.linkedin.com/jobs/view/4255244170,4255244170,"Sunnyvale, CA",On-site,$70/hr - $80/hr,2025-06-23 20:50:59,True,over 100 applicants,"Hello 
I hope you're doing well!
I have an exciting opportunity with Apple as Data Engineer III
Here are the details:
A globally leading tech company is seeking a Data Engineer III with 6+ years of experience in software development and 3+ years in big data and data warehouse technologies (e.g., Trino, Spark, Snowflake). The role involves data modeling, database optimization, pipeline development, and full-stack integration using Java, Python, and SQL. If you're passionate about building scalable, data-driven solutions, we encourage you to apply!
Job Responsibilities:
•  Data Modeling and Database Design: Identifying entities, data, and their relationships within the application, considering the constraints of the stack (e.g. SQL, vector, and unstructured data). This is needed to support applications, query, analytics, and reporting. Creating and maintaining database schemas, choosing appropriate data types, and understanding normalization and denormalization.• Database Optimization: Optimizing and tuning databases and queries for maximum performance and reliability, considering SQL, vector, and unstructured database.• Data Security & Compliance: Ensuring compliance with data security and privacy regulations.• Data Engineering: Designing and building data pipelines, implementing ETL processes, optimizing databases and queries, and ensuring data security.• Full-Stack Development: Developing front-end and back-end components of data-driven applications, creating APIs, and integrating data with various technologies.
Required Experience and Skills:
• 6 years of software development on Data Modeling, Database Design and Optimization. and• 6 years of Full-stack development using Java, python, SQL• 3 years of big data, vector database, and data warehouse technology, e.g. Trino, Spark, Snowflake.
Education:
• Bachelor's or Master's degree or above in a relevant field of Computer science and Software.
Type: Contract (W2 only)Duration: 12 monthsSchedule: On-site (Sunnyvale, CA)Pay range: $ 71.00 - $ 83.00 (DOE)",https://www.linkedin.com/job-apply/4255244170
Russell Tobin,https://www.linkedin.com/company/russell-tobin-&-associates-llc/life,Data Engineer 5,https://www.linkedin.com/jobs/view/4255236373,4255236373,"Salem, OR",,$65/hr - $70/hr,2025-06-23 20:23:26,False,,"What are we looking for in our Data Engineer 5?

Job Title: Data Engineer – Manager

Location: Remote

Duration: 6 months

Compensation: $65 to $70/Hourly

Job Summary:

We are seeking an experienced Data Engineer – Manager to design, build, and optimize database management systems while ensuring data accessibility, security, and quality. This role involves developing database standards, evaluating system performance, and working with large-scale data pipelines.

As a key team member, you will collaborate with cross-functional teams to establish data integration strategies, database specifications, and security policies. If you have a passion for managing complex data systems and working with cutting-edge technologies, we want to hear from you!

Key Responsibilities:

Establish and maintain database management systems, standards, and guidelines.Design and implement data models, logical databases, and capacity planning.Develop and optimize data pipelines to ensure efficient data flow and storage.Evaluate and install database management systems for performance and security.Write complex programs and build windows, screens, and reports.Assist in designing user interfaces and business application prototypes.Participate in quality assurance processes and test application code.Work with large-scale datasets to extract meaningful insights and optimize performance.Ensure proper data governance, security policies, and data maintenance plans.

Required Qualifications:

Bachelor’s degree in a relevant field with 10+ years of experience (or equivalent: PhD + 8 years, Master’s + 9 years, Associate’s + 11 years, High School + 12 years).Extensive experience in data engineering, database management, and system architecture.Strong understanding of relational databases, data storage, and optimization.

Must-Have Skills:

Databricks

AWS

Apache Spark

Python

PySpark

Preferred Skills:

Experience with big data processing frameworks.Strong problem-solving skills and the ability to troubleshoot data issues.Excellent communication and collaboration skills with cross-functional teams.

Benefits Info:

Pride Global offers eligible employee’s comprehensive healthcare coverage (medical, dental, and vision plans), supplemental coverage (accident insurance, critical illness insurance and hospital indemnity), 401(k)-retirement savings, life & disability insurance, an employee assistance program, legal support, auto, home insurance, pet insurance and employee discounts with preferred vendors.

#CB

",https://russelltobin.com/job/technology/data-engineer-5-418360
Capitol Federal Savings,,Data Engineer,https://www.linkedin.com/jobs/view/4255237218,4255237218,"Topeka, KS",On-site,,2025-06-23 20:06:05,False,,"RoleProvides support of our data warehouse environment and analytics tools. This allows our business leaders access to business intelligence data in order to develop strategies for our organization to grow. The Data Engineer assists in setting overall development roadmap and standards for the Bank and helps evaluate and architect the use of data solutions, using industry best practices. As part of this job, you will be working to implement a mixture of on premise and cloud strategies for the future of data processing at the bank.

Essential Duties & ResponsibilitiesResponsible for building and maintaining optimal data pipelines, architectures and data sets required for extraction, transformation, and loading of data from a wide variety of data sources using SQL and integration technologies.Leverage existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes.Build and utilize appropriate data platform structures to organize and store data in a particular manner.Work with big data technologies on-prem, cloud and hybrid. Implement data lifecycle management strategies around the flow of data within the organization implementing policy and automated approaches.Implement BI (business intelligence) platforms both on-premises and cloud.Establish governance and strategies around visualization of creating reports and dashboards to help improve upon operational and analytical reporting. Examine, assess, translate, and classify data; recognize, collect and analyze data to encourage the advancement, execution and application of data platform systems.Analyze data to spot anomalies, trends and correlate similar data sets.Design, develop and implement natural language processing software modules.Create models and standards to govern which data is collected, and how it is stored, arranged, integrated, and use in data systems and in organizations.Perform major tasks, deliverables, and formal application delivery methodologies; deliver new or enhanced applications. Perform additional database administrator duties, as assigned.Performs work independently.Considered a high-level specialist who regularly interacts and works with senior management.Provides leadership, coaching, and/or mentoring to a subordinate group.Helps evaluate and architect the use of data solutions. May act as a team lead.Perform other duties as assigned.Participate in proactive team efforts to achieve departmental and company goals.Must comply with current applicable laws, regulations and bank policies and procedures. Comply with all safety policies, practices and procedures. Report all unsafe activities to supervisor and/or Human Resources
.
Knowledge & SkillsExperienceFive or more years' experience with ETL/ELT tools and development. Experience in a Data Warehouse (DWH) environment with data integration of large and complex data sets.Proficient in SQL and scripting languages (e.g., Python).Hands-on experience with Snowflake data warehouse, including performance tuning and optimization.Experience with cloud-based platforms such as AWS and Azure.Knowledge of business intelligence (BI) tools such as Sigma, Power BI, SSRS, or Crystal Reports.Strong understanding of data warehousing concepts, dimensional modeling, and data lake architecture.Experience with SQL Server and Oracle database management systems.Technical proficiency with relational databases: T-SQL, SQL, and stored procedure
s.
Education/Certifications/LicensesBachelor’s degree in Computer Science, Information Systems, or another related field requir
ed.
SkillsCreative thinking, with the ability to evaluate a business scenario and design user-friendly, lowmaintenance software solutions. Excellent written, oral, and interpersonal communication skills.Effective use of multiple reporting tools. Technical expertise. Analytical thinking skills. Problem-solving skills. Leadership skills. The ability to work in a team environment and motivate or influence others is a critical part of the job, requiring a significant level of diplomacy, influence and tr
ust.
Physical RequirementsIs able to bend, sit, and stand in order to perform primarily sedentary work with limited physical exertion and occasional lifting of up to 10 lbs. Must be capable of climbing / descending stairs in an emergency situation. Must be able to operate routine office equipment including computer terminals and keyboards, telephones, copiers, facsimiles, and calculators. Must be able to routinely perform work on computer for an average of 6-8 hours per day, when necessary. Must be able to work extended hour or travel off site whenever required or requested by manage
ment.
Working ConditionsRegular in-office attendance req
uired.
Mental and/or Emotional RequirementsMust be able to perform job functions independently or with limited supervision and work effectivelyeither on own or as part of a team. Must be able to read and carry out various written instructions and follow oral instructions. Must be able to complete basic mathematical calculations, spell accurately, and understand computer basics. Must be able to speak clearly and deliver information in a logical and understandable sequence. Must be capable of dealing calmly and professionally with numerous different personalities from diverse cultures at various levels within and outside of the organization and demonstrate highest levels of customer service and discretion when dealing with the public. Must be able to perform responsibilities with composure under the stress of deadlines / requirements for extreme accuracy and quality and/or fast pace. Must be able to effectively handle multiple, simultaneous, and changing priorities. Must be capable of exercising highest level of discretion on both internal and external confidential m
atters.",https://capfed.wd1.myworkdayjobs.com/CapFed_Careers/job/Topeka-KS/Data-Scientist_R-101056
CareSignal® – Lightbeam's Deviceless Remote Patient Monitoring,https://www.linkedin.com/company/caresignal/life,Data Transformation Engineer,https://www.linkedin.com/jobs/view/4255138278,4255138278,"Horsham, PA",On-site,,2025-06-23 11:53:10,False,,"Job Summary

As a leader in end-to-end population health management solutions and services, we are continuing to grow our talented software development team. We are currently looking for full-time Data Transformation Engineer to join our experienced team. The Data Transformation Engineer will design, configure, implement, operate, and maintain technology systems. The Data Transformation Engineer will be capable of addressing new technology, compliance, and industry standards. They will be fluent in HL7 data management and normalization. Ability to work with teammates to accept support tickets from client partners around specific data transformation requests. This position will review, troubleshoot, analyze, and triage tickets appropriately. Strong personal skills in collaborating with partner resources Ideal candidate will have experience in:

Healthcare ITJavaJavaScriptGroovyXMLHL7/CCD knowledgeXSLTJSONSQL/Postgres

Job Responsibilities

Provide expertise on receiving and transforming data from disparate sourcesWork cross functionally within the company and externally with partners to meet data transformation, interface mapping, configuration, and validation needsPerform risk management pertaining to interfaces and data transformation to maximize client implementation successTrack any/all open interface issues through resolutionActively work with internal and external project team members to resolve open interface implementation itemsMonitor and report on ongoing data transmission for errors or issues at all stages of data ingestion and transformation, including communication to CustomerReport and escalate support issues as appropriateCommunicate and collaborate with development engineers for knowledge and assistanceEnsure interfaces and data transformation work are completed within agreed upon project schedule timelines provided by OperationsReview validation reports and address any discrepancies with the Customer

Qualifying Job Knowledge, Skills & Abilities

Proven working experience in interface implementation and/or working with claims/clinical data within the Healthcare IndustryPayor/Provider Background preferredKnowledge of value-based care, population health a plusAccustomed to Customer/Partner-facing rolesAbility to speak to all levels within an organizationOutstanding communication skillsSolid organizational skills, including attention to detail and multi-tasking skillsStrong working knowledge of Microsoft Office SuiteStrong working knowledge of SQLAbility to embrace change and company growthAnalytical and problem-solving skillsProven experience managing relationships with Customers technical and analytical team members

Key Competencies

Critical thinking with problem solving skillsDecision-makingCommunication skillsInfluencing and leadingTeamwork and positivityConflict managementAdaptabilityStress toleranceStrong computer skills are required

Education

Required: BS Degree (or higher) or equivalent work experience

Lightbeam Internal Use Only

Position Security Classification

CRITERIA: POSITION RISK LEVEL

High Risk (if any of these are true)

Access to Restricted information, including PHI and PII

Elevated access privileges

Access to the Security Infrastructure

Medium Risk

Access to Confidential, but not to Restricted information

Low Risk

Access to Unclassified information only

Data Transformation Engineer: High

CRITERIA: SECURITY ROLES & RESPONIBILITIES

Security Roles & Responsibilities

 Approve security-related changes Execute security-related changes Not applicable

Policy/procedures for Lightbeam

Cyber Security Incident Response (CSIRT) team

Data Transformation Engineer: Not Applicable

SECURITY ROLE-SPECIFIC TRAINING

Annual Incident Response Exercises",https://www.ziprecruiter.com/k/l/AAIjdZCbe76K8Mcov7Mw2nTsbaBfxfnqJC8pyB7TXj1eq2XxsFwLgVRBTnQQ_tDwgGbX6jDQAUEhQs5fEmX7BEQP6PDXFntPllAm9qDoLK5T4wZ7fgi3zgIMhiV_ecOtcHSAnmbENX9QpePIy-jWOwxmy7-9-w7w52pO15qdz6NwGi1LYFkS_6i4CLs7OxB6AW843Ex93Tfe6JGN_MRCP4hcMUh_PtQBND4s9MQii1N9EK7-I52okN_wmKb50JeGNDxs_8pj3k2PEeqTno28HyOUm9-MI3Zsvpg87K1F69CYGeY0wSDZNJ1b61v54uD0e1WFoKpII_NC_V3w
Jobot,https://www.linkedin.com/company/jobot/life,Data Platform Engineer,https://www.linkedin.com/jobs/view/4255099236,4255099236,"Midland, TX",Remote,$130K/yr - $160K/yr,2025-06-23 15:17:41,True,28 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Established company, HQ in Austin, that is innovating the preventive healthcare space seeks a Data Platform Engineer to join their team!

This Jobot Job is hosted by Rachel Hilton Berry

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $130,000 - $160,000 per year

A Bit About Us

We are seeking a dynamic and highly skilled Data Platform Engineer to join our rapidly growing team in the healthcare industry. This is a permanent, full-time position where you'll be at the forefront of revolutionizing healthcare data management. You will be working on cutting-edge technologies and will have the opportunity to make a significant impact in a fast-paced, innovative AI environment.



Why join us?


Innovative health screening company.Looking to innovative with AI.Stable company with 25+ years in the making

Job Details

Responsibilities

 Designing, developing, and maintaining scalable data platforms using Azure Fabric, PowerBI, Spark, Databricks, and other advanced technologies. Implementing robust data governance strategies to ensure data integrity, accessibility, and protection. Ensuring compliance with all relevant data security regulations and standards. Leveraging Python, Java, or Scala to develop data processing algorithms and ETL processes. Collaborating with cross-functional teams to understand data needs and deliver comprehensive solutions. Troubleshooting and resolving data-related issues in a timely manner. Continually monitoring the latest trends in data engineering and implementing innovative solutions to enhance data operations.

Qualifications

The ideal candidate for this role should have

 A bachelor's degree in Computer Science, Data Science, or a related field. A master's degree is a plus. A minimum of 5 years of experience in data platform engineering, preferably in the healthcare sector. Proficiency in Azure Fabric, PowerBI, Spark, Databricks, and other data platform technologies. Strong knowledge of data governance principles and practices. Experience in ensuring data security and compliance in a highly regulated industry. Excellent programming skills in Python, Java, or Scala. Exceptional problem-solving skills, attention to detail, and the ability to handle multiple tasks simultaneously. Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.

This is an exciting opportunity for a seasoned Data Platform Engineer who is passionate about leveraging data to drive healthcare innovation. If you're ready to take your career to the next level and make a real difference in the healthcare industry, we'd love to hear from you.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255099236
