company,company_url,job_title,job_url,job_id,location,work_type,salary,posted_at,is_easy_apply,applicant_count,description,apply_url
Lucas James Talent Partners,https://www.linkedin.com/company/lucas-james-talent-partners/life,Lead Data Engineer,https://www.linkedin.com/jobs/view/4256565545,4256565545,"Chicago, IL",Hybrid,,2025-06-26 23:39:45,False,,"Lead Data Engineer / Engineering Manager (onsite Tues, Wed, and Thurs)
 We're a dynamic marketing startup seeking a versatile and hands-on Lead Data Engineer to drive our data operations forward. As we grow rapidly, you'll wear multiple hats, leading a small but mighty team of engineers while staying deeply involved in technical work. 

 What You'll Do: 
- Spearhead our data engineering efforts, building and scaling our infrastructure from the ground up
 - Lead, mentor, and grow a lean team of talented engineers
 - Get your hands dirty with coding, troubleshooting, and system design
 - Rapidly prototype and implement data solutions to meet evolving business needs
 - Collaborate closely with cross-functional teams, from marketing to product
 - Make critical technical decisions that will shape our company's future 

 What You'll Bring:
 - 5+ years of data engineering experience, with 2+ years in a leadership role 
- Mastery of Python (Object-oriented & Functional Programming) and a strong command of AWS services
 - Proficiency with Git, Docker, Kubernetes, and Jenkins
 - Shell scripting skills and familiarity with CI/CD processes
 - Experience building and optimizing data pipelines, architectures, and data sets
 - Ability to thrive in ambiguity and adapt quickly to changing priorities
 - Passionate about staying current with the latest data engineering trends and technologies
 - Strong communication skills and ability to explain complex concepts to non-technical stakeholders
 - We are on a hybrid schedule, in the office on Tuesdays, Wednesdays, and Thursdays.
 Nice-to-Haves:
 - Experience in a high-growth startup environment
 - Background in marketing analytics or ad tech
 - Familiarity with machine learning workflows
 - Contributions to open-source projects

 The X-Factor:
 - You're a problem solver at heart, energized by challenges and continuous learning
 - You have a bias for action and can balance speed with quality in a fast-paced environment
 - You're comfortable with ambiguity and excited about building something from scratch
 - Growth Mindset: you're a learn-it-all, not a know-it-all
 - Pragmatic: You deliver practical solutions that provide quick value 

 We offer the opportunity to make a significant impact in a rapidly growing company. If you're excited about rolling up your sleeves, wearing multiple hats, and shaping the future of data-driven marketing, we want to hear from you! We're more interested in your skills and passion than formal qualifications. If you don't tick all the boxes but think you'd be great for this role, we would like to encourage you to apply. All applicants must be authorized to work in the United States for any employer without the need for current or future sponsorship. 
 



Benefits

Medical
Dental
401k match



",https://recruit.zoho.com/recruit/ViewJob.na?digest=Nyi9VFWIsbMXNaN0qju4E6KhkDs6rYG9j3qdOVinULQ-&embedsource=LinkedIn%2BLimited%2BListings
Vertex Elite LLC,https://www.linkedin.com/company/vertex-elite-llc/life,Data Engineer with Ataccama,https://www.linkedin.com/jobs/view/4256505777,4256505777,United States,Remote,,2025-06-26 15:27:00,True,over 100 applicants,"Hello, ""Data Engineer"" with Ataccama We have an urgent requirement for a ""Data Engineer"" with one of our direct clients, please share your updated resume if interested. 
Role: ""Data Engineer"" (Ataccama)Location: RemoteDuration: Long-term 
Description:1. Configuration Management:Configure and optimize Ataccama ONE modules to meet organizational requirements Manage configurations for different modules in multiple environments 2. Data Integration:Establish and maintain data connections for effective data processing and integration.Ensure seamless integration of data across various systems.3. Customization and Enhancement:Enhance user experience through customized Ataccama configurations and solutions Modify security and logging options to suit specific environments.
With Best Regards, Srikanth Nakka | Technical RecruiterEmail: srikanth@vertexelites.comContact : +1 (832) 626-0589 Ext 108 | Direct : +1 (832) 353-3407linkedin.com/in/srikanth-n-750884324Vertex Elite LLC | E-Verified Company | www.vertexelites.com",https://www.linkedin.com/job-apply/4256505777
Stratacent,https://www.linkedin.com/company/stratacent/life,MS Fabric Data Engineer,https://www.linkedin.com/jobs/view/4255445954,4255445954,United States,Remote,,2025-06-26 23:47:24,True,47 applicants,"Title: Data EngineerLocation: RemoteDuration: Fulltime (W2) 
Job Summary:The ideal candidate will possess deep technical skills in Microsoft Fabric data engineering includingPowerBI, with proven ability in end-to-end integrations, custom connectors, real-time data processing, and secure API management using OAuth 2.0. The resource will be responsible for architectural strategy, solution delivery, governance, security setup and administration.
Key Responsibilities:Lead the design and architecture of Microsoft Fabric data engineering solutions and Power BI visualizations that are scalable, secure, and aligned with business goals.Proficiency in Power BI development, data modeling, and visualization concepts with expertise in building PBI dashboardsExperience with SQL and DAX, along with familiarity with data warehouse conceptsBuild custom connectors, integrate with third-party services and internal APIs, and enable real-time data integrations across systems.Integrate with 3rd party systems like SAP, Salesforce, IBM OpenPages, Azure Data Factory, SQL,and other enterprise data systems for analytics and data-driven applications.Drive OAuth 2.0-based authorization flows for secure and compliant system integrations.Design and maintain Azure API Management (APIM) interfaces with version control, policyenforcement, and monitoring.Ensure best practices for application lifecycle management (ALM) using tools like Azure DevOps or GitHub.Guide and mentor development teams, enforce coding standards, and ensure architectural alignment across multiple projects.Collaborate closely with business users, IT leaders, and project managers to translate requirements into technical solutions.Experience in configuring Power BI Service for enterprise-wide access, sharing, and collaborationEstablish governance frameworks for user roles, permissions, and data access
Required Skills and Experience:10+ years in software/application development and enterprise architecture.5+ years in Data Engineering with at least 1.5 years in Microsoft Fabric.Proven experience in custom connectors, REST APIs, OAuth 2.0, webhooks, and API-based integrations.Strong knowledge of front-end and back-end integration, including UI/UX development principles.Experience integrating with Databricks, Azure Services, and cloud/on-prem systems.Excellent understanding of real-time data integration and event-driven architecture.Experience with CI/CD pipelines for Power Platform and automation using Azure DevOps and GitHub.Excellent communication, stakeholder engagement, and documentation skills.
Preferred Certifications:Microsoft Certified: DP 700
About Us:Stratacent is an IT Managed Services firm, headquartered in Jersey City, NJ, with two global delivery centres in New York City area and New Delhi area plus offices in London, Canada and Pune, India.We are a leading IT services provider focusing in Financial Services, Insurance, Healthcare and Life Sciences. We help our customers in their digital transformation journey and provide solutions around cloud infrastructure, containers, AI, Client & Analytics and Automation.We have partnerships with SAS, Automation Anywhere, Snowflake, Azure, AWS and GCP.(To learn more: www.stratacent.com ).
 BenefitsHealth Insurance (Co Pay)Vision PlanDental PlanPaid Time Off401k (No Match)Continuous Learning Program",https://www.linkedin.com/job-apply/4255445954
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256394016,4256394016,"Springfield, IL",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:40,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/36fea2bc72e144279079d5f59c4a10eatjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
TikTok,https://www.linkedin.com/company/tiktok/life,Data Engineer - Data Platform,https://www.linkedin.com/jobs/view/4256562163,4256562163,"San Jose, CA",On-site,$194K/yr - $410K/yr,2025-06-26 22:19:10,True,over 100 applicants,"Responsibilities
As a data engineer in the data platform team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.

Responsibilities - What You'll Do
• Design and build data transformations efficiently and reliably for different purposes (e.g. reporting, growth analysis, multi-dimensional analysis);
• Design and implement reliable, scalable, robust and extensible big data systems that support core products and business;
• Establish solid design and best engineering practice for engineers as well as non-technical people.

Qualifications
Minimum Qualifications: 
• BS or MS degree in Computer Science or related technical field or equivalent practical experience;
• Experience in the Big Data technologies(Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc.);
• Experience with performing data analysis, data ingestion and data integration;
• Experience with ETL(Extraction, Transformation & Loading) and architecting data systems;
• Experience with schema design, data modeling and SQL queries;
• Passionate and self-motivated about technologies in the Big Data area.

About TikTok
TikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.


Why Join Us
Inspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect – and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.
We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an ""Always Day 1"" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.

Diversity & Inclusion
TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.


TikTok Accommodation
TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://tinyurl.com/RA-request


Job Information
【For Pay Transparency】Compensation Description (Annually)
The base salary range for this position in the selected city is $194000 - $410000 annually.
Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.
Benefits may vary depending on the nature of employment and the country work location. Employees have day one access to medical, dental, and vision insurance, a 401(k) savings plan with company match, paid parental leave, short-term and long-term disability coverage, life insurance, wellbeing benefits, among others. Employees also receive 10 paid holidays per year, 10 paid sick days per year and 17 days of Paid Personal Time (prorated upon hire with increasing accruals by tenure).
The Company reserves the right to modify or change these benefits programs at any time, with or without notice.
For Los Angeles County (unincorporated) Candidates:
Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state, and local laws including the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act. Our company believes that criminal history may have a direct, adverse and negative relationship on the following job duties, potentially resulting in the withdrawal of the conditional offer of employment:
1. Interacting and occasionally having unsupervised contact with internal/external clients and/or colleagues;
2. Appropriately handling and managing confidential information including proprietary and trade secret information and access to information technology systems; and
3. Exercising sound judgment.",https://www.linkedin.com/job-apply/4256562163
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer [remote work],https://www.linkedin.com/jobs/view/4256387618,4256387618,"Fort Walton Beach, FL",Remote,$75.5K/yr - $128.3K/yr,2025-06-26 13:54:54,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for BAE Systems USA.

Job Description

We are seeking a skilled Data Engineer with expertise in Artificial Intelligence (AI) and Machine Learning (ML) to join our team. The successful candidate will build and maintain large-scale data systems that support AI/ML applications, develop and deploy AI/ML models to drive business growth and improve decision-making.

100% Remote or Hybrid work arrangement, with occasional visits to the office. US Citizenship or Green Card required 

Key Responsibilities

Design and implement data pipelines to ingest, process, and store large datasets from various sources Develop and deploy AI/ML models using techniques such as supervised and unsupervised learning, deep learning, and natural language processing Build and maintain data warehouses, including data modeling, data governance, and data quality Develop data architectures to support AI/ML applications, including data lakes, data marts, and data hubs Collaborate with data scientists to develop and deploy AI/ML models, including data preparation, feature engineering, and model evaluation Ensure data quality and integrity by developing and implementing data validation, cleansing, and normalization processes Optimize data systems for performance, scalability, and reliability Stay up to date with industry trends in AI/ML, data engineering, and data science, and apply this knowledge to develop innovative solutions 

Required Education, Experience, & Skills

Bachelor's or Master's degree in Computer Science or a related field, with at least 6 years of experience in Information Technology

6 years expereience developing and deploying AI/ML models using techniques such as supervised and unsupervised learning, deep learning, and natural language processing

Programming skills in languages such as Python, Java, Scala, or C Experience with data storage solutions such as relational databases (Oracle, SQL Server), NoSQL databases, or cloud-based data warehouses (Redshift) Experience with data processing frameworks such as Apache Kafka, Fivetran. Experience in building ETL pipelines using AWS Glue, Apache Airflow, and programming languages including Python and PySpark. Experience with AI/ML frameworks and tools such as TensorFlow, PyTorch, scikit-learn, or Keras Experience with cloud platforms including AWS and AWS Services or Azure. Experience with containerization using Docker and orchestration using Kubernetes Experience with machine learning engineering tools such as MLflow, TensorFlow Extended, or AWS Sage Maker Experience with agile development methodologies such as Scrum or Kanban Certifications in data engineering, AI/ML, or related fields 

Pay Information

Full-Time Salary Range: $75510 - $128340

Please note: This range is based on our market pay structures. However, individual salaries are determined by a variety of factors including, but not limited to: business considerations, local market conditions, and internal equity, as well as candidate qualifications, such as skills, education, and experience.

Employee Benefits: At BAE Systems, we support our employees in all aspects of their life, including their health and financial well-being. Regular employees scheduled to work 20 hours per week are offered: health, dental, and vision insurance; health savings accounts; a 401(k) savings plan; disability coverage; and life and accident insurance. We also have an employee assistance program, a legal plan, and other perks including discounts on things like home, auto, and pet insurance. Our leave programs include paid time off, paid holidays, as well as other types of leave, including paid parental, military, bereavement, and any applicable federal and state sick leave. Employees may participate in the company recognition program to receive monetary or non-monetary recognition awards. Other incentives may be available based on position level and/or job specifics.

Data Engineer [remote work]

112020BR

EEO Career Site Equal Opportunity Employer. Minorities . females . veterans . individuals with disabilities . sexual orientation . gender identity . gender expression

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/b98bc75c86df4d18a770c6d65206db88tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Apexon,https://www.linkedin.com/company/apexon/life,Data Engineer,https://www.linkedin.com/jobs/view/4256509646,4256509646,"Dallas, TX",On-site,,2025-06-26 15:42:29,True,over 100 applicants,"Must Have :Develop/Maintain programs in SSIS/ Spark/ SQL Stored Procedures as part of ELT/ETL pipelinesExperience with BI Tools like TableauProficient in writing SQL queries and data analysisKnowledge of relational database management systems like SQL Server/SnowflakeExperience in programming languages like Java/Scala
Great to Have:Experience with big data technologies like Hadoop, Spark, and KafkaImplement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache SparkPrior experience working in Banking/Finance domain
Experience with Big Data Technologies - Spark (Java) for handling large-scale data processingProficiency in SQL and Database - querying, managing, and manipulating data setsKnowledge of Cloud Platforms - data storage, processing, and deployment in a scalable environment (Azure)",https://www.linkedin.com/job-apply/4256509646
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Senior AWS Data Engineer,https://www.linkedin.com/jobs/view/4256539804,4256539804,United States,Remote,,2025-06-26 20:08:51,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Raas Infotek LLC, is seeking the following. Apply via Dice today!

Job Role: Senior AWS Data Engineer

Location: Remote

Duration: 12+ Months (Long-Term Contract)

Experience: 12+ Years

Job Description

Experience with tools like AWS Glue, Step Functions, Airflow, or cron jobs; ability to handle batch and streaming data.Experience with AWS Lambda, ECS, Fargate, S3, and IAM; understanding of cost optimization and containerization using Docker.Usage of Snowflake, SingleStore, Redshift, Kafka, Glue, and ability to design low-latency pipelines.Strong command over Python (Pandas, NumPy, Flask), SQL, and optionally JavaScript or Shell scripting. Use of unit testing frameworks like Pytest is a bonus.Understanding of IAM policies, role-based access control, Snyk/code scanning, and CI/CD practices with Git, Jenkins, or similar tools.AWS Certification preferred",https://click.appcast.io/t/HktMLZ-qbKHRFLqAfBpN-_eVNlj_29_RqMnZ7eztMwo=
Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Analytics Engineer, New Grad & Entry Level",https://www.linkedin.com/jobs/view/4256546619,4256546619,United States,Remote,,2025-06-26 21:17:46,False,,"Who we areAdelaide is the leader in one of the fastest-growing areas of digital advertising: attention metrics. Since 2020, we’ve been a trusted measurement partner for 40% of Fortune 50 companies. They rely on our metric, AU, to maximize the effectiveness of media spend. AU is “the attention economy's most widely recognized metric,” according to Adweek, and we swept the measurement category in the 2024 Adexchanger awards.
Position OverviewThis position reports to the Senior Data Analytics Engineer; it will behave cross functionally across Data teams with an emphasis on supporting the Analytics team and their workflows.In this role, you will be joining a team of data scientists and engineers. You'll help build and maintain the semantic layer of our data pipeline, ensuring clean, consistent, and reusable data models. Day-to-day activities range from developing and testing data models, editing LookML, managing a data catalog, automating data analysis workflows, and working closely with stakeholders to understand and support their data needs.
What you'll learnAn important part of our culture is continuing education and the sharing of ideas. We offer:A large network of investors and advisors for you to access that will help your team succeedMentorship from executives with decades of experience in adtech and mediaRegular internal knowledge-sharing sessionsEducation budget to accelerate your team’s development
Core responsibilities:Assist in building, testing, and maintaining transformational data models (dbt, Redshift)Help create and manage a clean, reliable reporting layer in Looker using LookMLWork cross-functionally with emphasized support to the Analytics TeamHelp identify and automate manual data analysis processes (Python, dbt)Contribute to maintaining a well-organized data catalog with accurate, accessible metadata for both technical and non-technical audiencesEnsure metric consistency across dashboards and data toolsCollaborate with analysts and PMs to document key metrics and definitionsMonitor adoption and identify opportunities for improved data usability
What you'll bring:SQL Proficiency – Strong ability to write, optimize, and debug SQL queries. You also understand data modeling and warehouse best practices, and you’re committed to writing clean, readable, and well-documented code.BI & Visualization Tools – Hands-on experience with BI/dashboarding platforms (e.g., Looker, Tableau).Data Modeling Tools – Familiarity with semantic modeling tools (e.g., LookML) and dbt; understanding of ETL concepts. Experience with orchestration tools (e.g., Airflow) is a plus.Data Quality & Testing – Strong attention to detail in building data validation, profiling routines, and root cause analysis workflows.Programming Skills – Experience with Python for data transformation, scripting, and automation; experience with associated libraries (e.g., pandas, openpyxl) is a plus.Communication & Collaboration – Excellent interpersonal skills with experience working cross-functionally; ability to translate technical concepts for non-technical audiences and deliver training/support.Educational Background & Experience – Bachelor’s degree in a quantitative, technical, or analytical field (e.g., Computer Science, Math, Physics, Engineering) or a rigorous coding bootcamp with a portfolio demonstrating the above skills.",https://jobright.ai/jobs/info/6859f854d3b885d69bbfe235?utm_source=1124
Recruiting from Scratch,https://www.linkedin.com/company/recruiting-from-scratch/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4256565174,4256565174,United States,Remote,$135K/yr - $150K/yr,2025-06-26 23:19:30,True,over 100 applicants,"Who is Recruiting from Scratch: Recruiting from Scratch is a talent firm that focuses on placing the best candidate for our clients. Our team is 100% remote and we work with teams across North America, South America, and Europe to help them hire.https://www.recruitingfromscratch.com/
Title of Role: Senior Analytics EngineerLocation: Remote - United StatesCompany Stage of Funding: Public CompanyOffice Type: RemoteSalary: $135,000 - $150,000 base + bonus + full benefits
Company DescriptionWe are partnering with a pioneering education technology company that has been shaping the future of K–12 learning for over two decades. This organization builds immersive digital experiences in reading, math, and science—serving over 15 million students across all 50 states. Their programs integrate formative assessments, next-generation curriculum, and powerful educator tools to improve student outcomes and instructional insight at scale.This company is widely recognized for combining the agility of a modern tech company with the rigor and mission of educational excellence. They are actively expanding their data engineering team to enhance how students, teachers, and administrators engage with and leverage data in the learning process.
What You Will DoAs a Senior Analytics Engineer, you will join a multidisciplinary data team that powers the company’s insights infrastructure. You’ll be responsible for developing scalable, well-documented, and testable data pipelines and models that support product teams, educators, administrators, and internal operations.
Key Responsibilities:Build and maintain ELT pipelines using modern tools like dbt, Fivetran, Airflow, and Snowflake.Partner with data scientists, analysts, and business users to model clean, modular, and reusable datasets for internal reporting and product analytics.Support K–12 administrators and educators by building metrics and dimensional models that visualize teaching practices and student learning progress.Work with product teams to express student learning data through flexible and rich schemas for adaptive educational experiences.Collaborate with marketing, sales, and finance to unify data across ERP and CRM systems (e.g., Salesforce, HubSpot, Netsuite) and enable actionable reporting.Analyze performance and debug tricky data issues using SQL, Python, and visualization tools like Tableau and Looker.Contribute to a data-driven culture by creating documentation, participating in cross-functional learning sessions, and mentoring junior team members.Ideal Candidate Background5+ years of experience in data engineering, analytics engineering, or software development focused on data systems.Expert-level proficiency with SQL and strong understanding of ETL/ELT frameworks, preferably using dbt.Solid grasp of analytical data modeling principles, especially Kimball-style dimensional modeling.Demonstrated experience building reliable and efficient pipelines, with a focus on business logic, maintainability, and performance.Strong communication skills—capable of translating complex data concepts for both technical and non-technical audiences.Passion for education or other mission-driven fields is a strong plus.PreferredFluency in Python for scripting and data analysis.Experience with data orchestration tools like Airflow, Matillion, or Fivetran.Familiarity with cloud data platforms such as Snowflake and AWS (S3, RDS, DynamoDB).Proficiency with BI tools such as Tableau or Looker.Exposure to metadata management tools like Atlan.Prior work in education or ed-tech, or contribution to data modeling standards like Caliper Analytics or EdFi.Compensation and BenefitsBase Salary: $135,000 - $150,000Bonus: Annual discretionary bonus based on individual and company performanceEquity: N/A (Public company)Benefits Include:Comprehensive health, dental, vision, and mental health coverage401(k) retirement planGenerous PTO and parental leaveBasic life insuranceAccess to top-tier professional development programsThis role offers a unique opportunity to work at the forefront of educational transformation, delivering data solutions that empower millions of learners and educators. If you're passionate about analytics engineering and education, we encourage you to apply.https://www.recruitingfromscratch.com/",https://www.linkedin.com/job-apply/4256565174
Bonhill Partners,https://www.linkedin.com/company/bonhill-partners/life,Databricks Data Architect / Engineer - Commodities Trading,https://www.linkedin.com/jobs/view/4258097817,4258097817,"Houston, TX",Hybrid,$135/hr - $150/hr,2025-06-26 22:17:07,True,72 applicants,"We are currently supporting a global commodities trading company as they expand their leading Front Office Technology team. 
We are currently seeking a hands-on Data Architect/Engineer with extensive experience in Databricks. The ideal candidate will have a strong background data migration from RDBMS to Databricks, and a deep understanding of modern data architecture principles. 
Your role will be focused on designing, developing and implementing a robust, scalable data architecture, utilising Databricks, to deliver multi-asset-class commodity systems, with a focus on automation, user experience, optimization, innovation and control.
Key SkillsDatabricks - ETL processes, data migrationPython - OOPAWSSQL
This is a contract role paying up to $150 per hour, for an initial 12 month period.
The role requires 4 days a week in their Houston office.
If you believe you are a good match for the role please apply. We look forward to speaking with you!",https://www.linkedin.com/job-apply/4258097817
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4258019483,4258019483,"Salem, OR",On-site,$220.7K/yr - $235.4K/yr,2025-06-26 14:10:28,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/5420de6c082444989f5088c29a4b9ae7tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Zensar Technologies,https://www.linkedin.com/company/zensar/life,OpenSearch Database Engineer,https://www.linkedin.com/jobs/view/4255450132,4255450132,"Minneapolis, MN",On-site,,2025-06-26 21:21:29,True,23 applicants,"About the Role:We are seeking a skilled and passionate OpenSearch Engineer to join our growing Data team. In this role, you will be responsible for designing, deploying, managing, and optimizing OpenSearch clusters to support search, logging, and analytics use cases across our platform. You will collaborate with developers, DevOps, and data teams to ensure high availability, performance, and security of OpenSearch in production environments.
Responsibilities:· Design, implement, and maintain scalable OpenSearch clusters across environments.· Optimize indexing, querying, and storage strategies for performance and cost-efficiency.· Set up and manage OpenSearch Dashboards (formerly Kibana) for log analytics and search interfaces.· Monitor and troubleshoot performance and availability issues with OpenSearch services.· Collaborate with engineering teams to integrate OpenSearch with Micro-Services , application logs, metrics, and events.· Define and enforce security best practices for data access and cluster management.· Stay current with OpenSearch developments, plugins, and the broader observability ecosystem.Qualifications:· 10+ years of experience in managing Elasticsearch or OpenSearch clusters in production.· Managing data ingestion and Database knowledge on Kafka Glue and EMR· Strong understanding of distributed systems and search engine internals.· Proficiency with scripting and automation tools (e.g., Bash, Python, JavaScript,).· Familiarity with containerized environments (Docker, Kubernetes) and CI/CD pipelines.· Experience with cloud platforms (AWS, GCP, or Azure) and managed services.· Excellent communication and collaboration skills.Preferred Qualifications:· Hands -On Experience with ETL tools like Informatica and Glue.· Scheduling Tools (TWS or TIDAL).",https://www.linkedin.com/job-apply/4255450132
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256387785,4256387785,"Columbia, SC",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:40,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/7f31a55cd916415397b04465bb8252d5tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Data and Analytics Support Engineer,https://www.linkedin.com/jobs/view/4258019328,4258019328,United States,Remote,,2025-06-26 14:10:44,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Norstella.

Data and Analytics Support Engineer

Company: Norstella

Location: Remote, India

Date Posted: May 2, 2025

Employment Type: Full Time

Job ID: R-1154

Description

About Norstella

At Norstella, our mission is simple: to help our clients bring life-saving therapies to market quicker—and help patients in need.

Founded in 2022, but with history going back to 1939, Norstella unites best-in-class brands to help clients navigate the complexities at each step of the drug development life cycle —and get the right treatments to the right patients at the right time.

Each Organization (Citeline, Evaluate, MMIT, Panalgo, The Dedham Group) Delivers Must-have Answers For Critical Strategic And Commercial Decision-making. Together, Via Our Market-leading Brands, We Help Our Clients

Citeline – accelerate the drug development cycle Evaluate – bring the right drugs to market MMIT – identify barrier to patient access Panalgo – turn data into insight faster The Dedham Group – think strategically for specialty therapeutics 

By combining the efforts of each organization under Norstella, we can offer an even wider breadth of expertise, cutting-edge data solutions and expert advisory services alongside advanced technologies such as real-world data, machine learning and predictive analytics.

As one of the largest global pharma intelligence solution providers, Norstella has a footprint across the globe with teams of experts delivering world class solutions in the USA, UK, The Netherlands, Japan, China and India.

Job Description

As we expand our data team, we are looking to hire a super-person to become our Scrum Master to coordinate and coach our data & insights team. You will be our go-to person for applying scrum to produce high-quality work. Scrum Master duties include managing timelines, resolving problems, and coaching team members on Agile methodologies. You will help create self-organizing teams that are flexible and fully productive during sprints. You should have excellent knowledge of the scrum framework, with all its artifacts and techniques. You will also need the ability to coordinate people and projects (occasionally facilitating changes) with your mind set on deliverables. If you are a strong communicator, a capable leader and you're invested in Agile frameworks, we would like to meet you.

Key Duties And Responsibilities

Providing support, i. e., incident, problem, service request management to the internal clients through the helpdesk ServiceNow, email or teams chat Identifying and accelerating priority issues per business requirements Interacting with teams to deliver and process data in response to inquiries, issues, and needs for software, procedures, and services Triaging on new issues and ensure proper information is provided to Development teams to replicate the issue Logging issues, documenting and assigning to proper team members for development if needed, providing root cause analysis of issues Performing the automated data quality checks and reports issues Overseeing and maintaining analytical data assets in either data warehouse, databases, BI applications Monitoring & administering analytical applications like Power BI & Tableau Supporting end users with their data and analytics related questions, performing demos of functionalities and educating end users if needed Assisting and supporting Data Hub & Spoke teams in company-wide projects 

Key Requirements

Fluency in written and spoken English Experience with SQL databases, writing own queries, Python programming, AWS cloud deployment Knowledge of Snowflake SQL, Tableau, Power BI systems is a must Experience of data management lifecycles (collection, cataloguing, ETL design, database management). Experience of working independently and to demanding timescales High level of enthusiasm and a positive attitude Ability to work within teams or independently and scoping/executing high quality deliverable Ability to handle multitasking Proven track record of reliability Great communication and customer relationship management experience Ability to work with clients under pressure. Willing to work in 24*7 support model 

Our guiding principles for success at Norstella

01: Bold, Passionate, Mission-First

We have a lofty mission to Smooth Access to Life Saving Therapies and we will get there by being bold and passionate about the mission and our clients. Our clients and the mission in what we are trying to accomplish must be in the forefront of our minds in everything we do.

02: Integrity, Truth, Reality

We make promises that we can keep, and goals that push us to new heights. Our integrity offers us the opportunity to learn and improve by being honest about what works and what doesn’t. By being true to the data and producing realistic metrics, we are able to create plans and resources to achieve our goals.

03: Kindness, Empathy, Grace

We will empathize with everyone's situation, provide positive and constructive feedback with kindness, and accept opportunities for improvement with grace and gratitude. We use this principle across the organization to collaborate and build lines of open communication.

04: Resilience, Mettle, Perseverance

We will persevere – even in difficult and challenging situations. Our ability to recover from missteps and failures in a positive way will help us to be successful in our mission.

05: Humility, Gratitude, Learning

We will be true learners by showing humility and gratitude in our work. We recognize that the smartest person in the room is the one who is always listening, learning, and willing to shift their thinking.

Benefits

Health Insurance Provident Fund Reimbursement of Certification Expenses Gratuity 24x7 Health Desk 

Norstella is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people’s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual’s abilities, skills, performance and behavior and our business requirements. Norstella operates a zero tolerance policy to any form of discrimination, abuse or harassment.

Sometimes the best opportunities are hidden by self-doubt. We disqualify ourselves before we have the opportunity to be considered. Regardless of where you came from, how you identify, or the path that led you here- you are welcome. If you read this job description and feel passion and excitement, we’re just as excited about you.

Norstella is an equal opportunity employer. All job applicants will receive equal treatment regardless of race, creed, color, religion, alienage or national origin, ancestry, citizenship status, age, physical or mental disability or handicap, medical condition, sex (including pregnancy and pregnancy-related conditions), marital or domestic partner status, military or veteran status, gender, gender identity or expression, sexual orientation, genetic information, reproductive health decision making, or any other protected characteristic as established by federal, state, or local law.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/93767cea1a0e424588400d893e09cb8dtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=None&utm_medium=slot&utm_source=linkedin&utm_term=jse
Eliassen Group,https://www.linkedin.com/company/eliassen-group/life,AWS Data Engineer (Onsite interview),https://www.linkedin.com/jobs/view/4256510876,4256510876,"Nashville, TN",Hybrid,$55/hr - $65/hr,2025-06-26 16:24:22,True,over 100 applicants,"Client DescriptionInterviews will take place in person in Nashville (there is NO initial virtual interview and travel expenses will not be reimbursed).Our client looking for a Senior Data Engineer with a strong background in AWS, current projects will focus on AWS Data Engineering with some level of cloud engineering (Terraform). Eventually migrating to Azure.
We can facilitate w2 and corp-to-corp consultants. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.
ResponsibilitiesWork on current cloud Data Warehouse implementation and later working on migrating to AZURE.
Experience RequirementsRequired skills are 8+ years of Data Engineering, with strong Databricks, AWS, SQL and Azure in order of importance.Integrating data from multiple sourcesImplementing DatabricksMany applications in AWS cloud currently, need to be moved overNeed strong AWS experience to work on applications (Glue, EC2, EMR, Lamba Functions).Will be building out infra in azure environment from scratch in addition to migrating stuff overIAC- Terraform used now, bicep would be nice to haveNeed good comms to help team learn new azure services
Education RequirementsBachelors in CS or equivalent work experience.
Skills, experience, and other compensable factors will be considered when determining pay rate. The pay range provided in this posting reflects a W2 hourly rate; other employment options may be available that may result in pay outside of the provided range.
W2 employees of Eliassen Group who are regularly scheduled to work 30 or more hours per week are eligible for the following benefits: medical (choice of 3 plans), dental, vision, pre-tax accounts, other voluntary benefits including life and disability insurance, 401(k) with match, and sick time if required by law in the worked-in state/locality.
Please be advised- If anyone reaches out to you about an open position connected with Eliassen Group, please confirm that they have an Eliassen.com email address and never provide personal or financial information to anyone who is not clearly associated with Eliassen Group. If you have any indication of fraudulent activity, please contact InfoSec@eliassen.com.",https://www.linkedin.com/job-apply/4256510876
Pennant,https://www.linkedin.com/company/thepennantgroup/life,Senior Analytics Engineer - Marketing,https://www.linkedin.com/jobs/view/4255451217,4255451217,United States,Remote,,2025-06-26 21:50:34,False,,"We are seeking a talented Sr. Analytics Engineer to join our team. This role will be instrumental in transforming raw data into actionable insights that drive operational efficiency and enhance patient and resident experiences within the senior living, home health, hospice, and home care industries.
Key Responsibilities:Enterprise Data Architecture & Strategy:Design scalable, secure data systems to serve needs of internal and external stakeholders.Balance analytical and operational use cases.Implement and enhance operational data store, data warehouse and champion master data management.Data Engineering:Design, develop, and maintain efficient data pipelines to extract, transform, and load (ETL) data from various sources, including electronic health records (EHRs), HRIS systems, and financial systems.Optimize data models for performance and scalability.Ensure data quality and integrity by implementing robust data validation and cleansing processes.Data Analysis and Modeling:Collaborate with cross-functional teams to understand business objectives and translate them into data-driven solutions.Translate business requirements and data structures into well-designed and scalable Star Schemas.Data Visualization:Create interactive dashboards and reports using Tableau to provide insights into key performance indicators (KPIs) such as occupancy rates, revenue, and resident satisfaction.Customize visualizations to meet the specific needs of different user groups, including executive leadership, operations teams, and clinical staff.Collaboration and Communication:Work closely with cross-functional teams to understand their data needs and provide tailored solutions.Effectively communicate technical concepts to both technical and non-technical stakeholders, including senior living executives and frontline staff.Document data pipelines, models, and visualizations to ensure knowledge sharing and maintainability.
Required Skills and Qualifications:Strong proficiency in SQL for data extraction, transformation, and analysis.Solid understanding of data warehousing and data modeling concepts, including data lakes, operational data stores, Kimball Methodologies, and more.Experience with modern data stack tools such as dbt and Snowflake.Experience with data visualization tools like Tableau.Strong analytical and problem-solving skills.Excellent communication and collaboration skills.
Preferred Skills and Qualifications:Understanding of and experience with marketing domain data.Passion for data warehouse design and architecture, balancing multiple use cases both internal and external.Proficiency in Python, dlt, and/or Azure Function Apps for data extraction.
Why Join Us?Something else that sets us apart from other companies is the quality of our most valuable resources – our people! We are dedicated to living out our culture as defined by our core values, “CAPLICO”:Customer Second – We prioritize and support our team so they can deliver exceptional care.Accountability – Own your work and outcomes.Passion for Learning – Grow continuously with curiosity and culture.Love One Another – Build authentic, respectful, and trusting relationships.Intelligent Risk Taking – Innovate and challenge the status quo.Celebrate – Recognize the small wins, they add up!Ownership – Be the CEO of your role.
Additional Benefits:True Work-Life balance – We believe in taking care of yourself before you take care of others!Full benefits package (medical, dental, vision, 401(k) with match)Paid time off, holiday pay, and professional developmentYour voice matters! - Work with other passion and high-achieving leaders who care deeply about patient outcomes and team success.
Location: Remote in Utah or Idaho
About Pennant ServicesWe support over 180 home health, hospice, senior living, and home care operations across 14 states. Our Service Center model empowers local leadership while providing centralized clinical, legal, HR, IT, and compliance support to help ensure high-quality care.
www.pennantgroup.com
Pennant Service Center1675 E. Riverside Drive, #150Eagle, ID 83616",https://pennant.wd1.myworkdayjobs.com/PennantServicesCareerSite/job/Utah/Senior-Analytics-Engineer---Marketing_JR53486
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256387962,4256387962,"Sacramento, CA",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:31,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/8f366b93ae054f4fa3a284853921d074tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
GSTV,https://www.linkedin.com/company/fueled-by-gstv/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4256567920,4256567920,United States,Remote,,2025-06-27 00:58:59,False,,"GSTV is dedicated to building an inclusive team and culture that reflects the communities we serve every day. Being part of the GSTV team means that we are always encouraged and challenged to grow personally and professionally. More importantly, we are accountable for our actions towards one another as the foundation for a strong and accepting workplace.

GSTV Values:

Growth FocusedSocial AccountabilityTenacious BehaviorValued Actions

Benefits Day One! Medical, Dental, Vision, Paternal Leave, Life Insurance, Accident, Critical Illness, Hospital Indemnity, STD/LTD + Vol Plans., Paid Holidays, 20 PTO days + Sick time, Perks, HSA and FSA and Lifestyle Spending Account (1 st of mo after 30 days), 401K Match (90 days).

GSTV offers both hybrid and remote work situations. Candidates located within commutable distance to our New York Office will be tagged to that office and are not currently considered 100% remote.

SENIOR DATA ENGINEER

Summary:

At GSTV, we take pride in the data that powers our media platform and are committed to unlocking its full potential to drive meaningful business outcomes. That’s why we’re expanding our Data Engineering team and looking for a talented Senior Data Engineer to join us.

As a Senior Data Engineer, you'll play a key role in building and maintaining the pipelines and platforms that enable our teams to access trusted, timely, and actionable data. You’ll work closely with Software Engineering, Architecture, IT, and Analytics to deliver scalable, high-quality data solutions that support our advertising, hardware, and retail partners.

Your primary focus will be on building efficient and reliable data pipelines, ensuring data quality, supporting analytics and reporting, and helping evolve GSTV’s data platform to meet future business needs. Along the way, you’ll mentor other engineers, help establish best practices, and collaborate across the organization to deliver measurable impact.

We’re passionate about being a data-driven organization, and we need someone who shares that passion. If you're excited to solve complex challenges, thrive in a fast-paced environment, and want to shape the future of data at GSTV, we’d love to hear from you.

Your success will be measured by:

Your ability to build and maintain robust, scalable data pipelines that meet business needs.The reliability and accuracy of the data systems you deliver—ensuring high data quality and visibility of any system issues.Your contributions to the technical design and implementation of data infrastructure and pipelines.Your ability to mentor and coach other members of the engineering team.Your ability to collaborate effectively with engineering, architecture, product, and analytics groups.Your ability to stay current with modern data engineering trends and apply them effectively at GSTV.

Technologies We Use:

Data & Visualization: Snowflake, Domo, MongoDB, DynamoCloud & Infrastructure: AWS (Kinesis, Lambda, S3, SQS, CDK)Languages: Python, SQLIntegrations: SaaS APIs (e.g., Salesforce)Development Practices: Agile, Git, CI/CD

Responsibilities:

Responsibilities include, but are not limited to:

Deliver high-quality data solutions on time and within scope, while meeting the highest quality standards.Design, develop, and maintain secure, scalable ETL pipelines that process structured and unstructured data from various sources.Build and optimize data models and processing workflows to support business intelligence, analytics, and reporting needs.Develop monitoring and alerting solutions to ensure pipeline performance and data quality, with quick visibility into issues.Collaborate closely with software engineering, architecture, product management, analytics, and IT teams.Translate business requirements into robust technical solutions and provide input on feasibility, scalability, and design.Conduct code reviews and provide thoughtful feedback to other members of the engineering team.Contribute to the continuous improvement of data development processes through automation, tooling, and best practices.Own the delivery and ongoing support of critical data flows and reporting systems.Develop insightful dashboards, reports, and datasets in visualization tools like Domo.Mentor other data engineers and contribute to onboarding efforts by teaching the GSTV approach to data development.Participate in agile ceremonies including standups, retrospectives, and planning.Escalate blockers or risks to appropriate leadership promptly.Other duties as assigned.

Requirements:

You are

Detail-oriented with the ability to drill down into tactical considerations.Able to act autonomously while following team philosophy and guidelines.Resourceful—able to find solutions even when they are not obvious—and know when to ask for help.Comfortable with ambiguity and rapid change.A team player with strong communication and collaboration skills in a matrixed environment.Business and outcome-focused with a bias toward action.Skilled at identifying and resolving data quality issues.Passionate about building well-structured, maintainable, and scalable data systems.A self-starter and lifelong learner, always eager to experiment with new tools and technologies.

You have, ideally,

Strong proficiency in SQL and data modeling techniques.Deep experience with Snowflake or comparable data platforms.Hands-on experience with Python and familiarity with other languages.Expertise in integrating data from external APIs and third-party systems.Familiarity with infrastructure-as-code and cloud-native data services (especially AWS).Strong analytical thinking, problem-solving, and debugging skills.Effective and impactful oral, written, and presentation communication skills.Experience in a fast-paced, Agile environment.Strong influencing skills—you can achieve goals without direct authority.Excellent organization and time management skills.

The requirements listed are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Education and/or Experience:

Experience: 

Ideal candidates will have 5+ years of experience developing and maintaining ELT systems, and a high-level understanding of different data processing systems.Must HaveExperience with the data warehouses (Snowflake or BigQuery), data ELT/ETL pipelinesProficiency in data modeling, advanced SQL, stored procedures, and query optimizationProficiency in one or more programming languages such as Python, Java, or ScalaNice To HaveCreating datasets, dataflows, and dashboards in DomoExperience building integrations with SaaS APIs like Salesforce
Education:

Bachelor’s degree in Computer Science or a related fieldExtra considerations given for certifications such as AWS Data Analytics Certification or Snowflake Data Engineering Certification

Of course, this is just a sample of the kinds of work this role will require! You should assume that your role will encompass other tasks, too, and that your job duties and responsibilities may change from time to time at GSTV's discretion, or otherwise applicable with local law.",https://recruitingbypaycor.com/career/JobIntroduction.action?clientId=8a7883d07ed6004a017edc47c7bd0a4d&id=8a7887ac97a8f3470197ae0ade7b4aa4&source=&lang=en
Evolve,https://www.linkedin.com/company/evolve-vacation-rental/life,Senior Data Analytics Engineer,https://www.linkedin.com/jobs/view/4258207865,4258207865,United States,Remote,$150K/yr - $160K/yr,2025-06-27 00:47:39,False,,"At Evolve, we’re on a mission to make vacation rental easy for everyone. Our high-performing, customer-obsessed team runs on curiosity, communication, and accountability—working together to create exceptional experiences for our owners and guests. Whether solving big challenges, delivering outstanding results, or celebrating wins, we approach every day with purpose and passion. If you’re ready to join a mission-driven company where every teammate has the opportunity to thrive, Evolve might just be the place for you.

Why This Role

At Evolve, data powers every aspect of how we transform the vacation rental industry. From optimizing guest experiences to enabling smarter homeowner engagement, our data platform supports scalable decision-making, operational excellence, and future-forward innovation.

As a Senior Data Engineer, you'll be a key technical leader responsible for building and evolving our data ecosystem. You’ll help design and optimize pipelines and architecture that support core reporting, analytics, and machine learning workloads, while also leading initiatives focused on cost efficiency, performance, and data accessibility.

This role is ideal for an engineer who is passionate about modern data architecture, cloud-native infrastructure, and mentoring others while delivering high-impact systems that make data a true asset.

What You’ll Do


Serve as a senior technical contributor helping shape the design of scalable, modular, and performant data systems.Partner with cross-functional teams to design and deliver reliable data pipelines for analytics, reporting, and data science use cases.Develop and optimize ELT workflows using modern tools such as Airbyte, Fivetran, dbt, and Python.Design and maintain performant data models in Snowflake, including star schemas and other dimensional modeling techniques that support clean, well-structured, and easily consumable datasets—including those optimized for AI tools and text-to-SQL interfaces.Lead efforts to improve data observability, including proactive alerting, monitoring, and incident response capabilities.Explore and implement open table formats (e.g., Apache Iceberg) to enable flexibility, scalability, and interoperability across data systems.Collaborate with DevOps and Cloud teams to ensure our AWS-based infrastructure is optimized for performance, cost, and reliability.Establish and maintain engineering best practices, including version control, code reviews, CI/CD pipelines, and automated testing.Act as a mentor to other engineers and analytics professionals, fostering knowledge sharing and high standards for quality and consistency.Stay current on emerging technologies and bring forward recommendations for improving platform capabilities, performance, or efficiency.


What Makes You a Great Fit


8+ years in data engineering, architecture, or related technical roles working with large-scale data systems.Deep experience with SQL and Python for data transformation and automation.Hands-on experience with modern data stack tools such as dbt, Airbyte, Fivetran, and Snowflake.Strong understanding of cloud technologies, particularly AWS (e.g., S3, Lambda, EC2, IAM).Familiarity with open data formats and scalable data lakehouse principles (e.g., Iceberg or similar).Experience building and operating workflow orchestration tools such as Airflow.Knowledge of observability and monitoring frameworks to support data platform health and alerting.Comfortable working in Git-based environments with strong CI/CD practices.Ability to communicate technical concepts effectively to both technical and non-technical audiences.Demonstrated ability to mentor, coach, and collaborate across a multidisciplinary data team.


Compensation

Annual base salary range: $150,000-$160,000 depending on experience and qualifications. This role is also eligible for a variable annual bonus based on both company and individual performance.

Location 

All Evolve team members must live in one of our approved locations by their first day. We can hire from anywhere in the U.S. except D.C. and Hawaii. Some positions may also have restrictions based on compensation in the following states: California, Maryland, New York, Pennsylvania, Rhode Island, and Washington. If you live in Colorado, you can work remotely anywhere in the state, at our downtown Denver office, or a hybrid of both! If you're planning to move soon, please let us know, and we'll be happy to review your application again.

California Applicant Privacy Policy | Evolve

How We Reward Evolvers

We’re intentional about offering benefits that empower every Evolver to thrive both professionally and personally because they’re more than perks—they’re investments in our customer-obsessed, high-performing team.

We believe in treating others as they want to be treated, providing benefits that deliver real value, and challenging the status quo to meet the diverse needs of our team. Whether it’s helping you take care of your health, plan for the future, or celebrate life’s milestones, our offerings are designed to support you every step of the way.


Financial Wellness: Industry-competitive pay, equity in the company, and a 401(k) with a 4% immediate vesting match.Family Support: 16-18 weeks of paid parental leave for birthing parents and 10 weeks for non-birthing parents, plus infertility coverage.Health & Wellness: Comprehensive medical, dental, and vision plans (100% employer-paid for individual enrollment), 10 free mental health visits, and pet insurance.Time to Recharge: Generous PTO, RTO (for full-time exempt employees), sick leave, holidays, and a personal holiday to celebrate what matters most to you.Travel Perks: Annual Evolve travel credit after one year and discounts on stays at all Evolve properties.Growth Opportunities: World-class onboarding programs, learning, and development resources to help you grow your impact.Connection: Employee Resource Groups celebrating our diverse communities at Evolve.


How We Work Together

With our core values as our guide, every Evolver helps shape the company we want to work for and the people we want to be. We’ve cultivated a culture of collaboration, care, and responsibility that we can all be proud of, and we’re excited to see what you’ll bring as your authentic self.

Still curious about who we are and what we do? Read more about our business and our culture at evolve.com.

EEO 

At Evolve, we are committed to diversity and inclusion. As an equal opportunity employer, all qualified candidates will be considered for employment without regard to race, color, creed, religion, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, national origin, ancestry, citizenship status, military service or veteran status, physical or mental disability, or any other legally protected characteristic. Evolve participates in e-Verify for all positions.

If you have a disability or special need that requires accommodation at any point in the hiring process, please let your recruiter know.",https://job-boards.greenhouse.io/evolvevacationrental/jobs/6618182003?gh_src=2550933e3us
Tietoevry,https://www.linkedin.com/company/tietoevry/life,Power BI & Data Engineer - Tietoevry Create ( m/f/d),https://www.linkedin.com/jobs/view/4256555886,4256555886,"Dallas, TX",On-site,,2025-06-26 22:21:37,False,,"Job Description

Around 8 years of experience as a Business Intelligence and Data Engineer developer using the MicrosoftPower BI and good experience as a data engineerExperience in Building dashboards, reports and cubes using SQL, MDX, DAX or Power BIExperience in designing effective layouts using themes and report gridsHands on Experience in publishing and scheduling Power BI reports as per the business requirementsGood experience and analytical skill - Data Analysis & Data profilingGood experience as a data engineer in any of the databases preferably using SnowflakeExperience in Data modelling including designing effective BI data modelsExperience in Workspace and Capacity administration, User ManagementVery strong SQL skills for verifying and creation of views/tables, functions and stored proceduresWorking experience in Azure environment.Critical thinker and problem-solving skillsGood Team player

Optional Skills

Experience in ADF and Logic Apps will be a added advantageExperience in designing ETL processes including knowledge of data warehousing strategies and theories will be added advantageKnowledge in Insurance domain, especially in Agency management will be an added advantage

Additional Information

All your information will be kept confidential according to EEO guidelines.

At Tietoevry, we believe in the power of diversity, equity, and inclusion. We encourage applicants of all backgrounds, genders (m/f/d), and walks of life to join our team, as we believe that this fosters an inspiring workplace and fuels innovation. Our commitment to openness, trust, and diversity is at the heart of our mission to create digital futures that benefit businesses, societies, and humanity.

Diversity, equity and inclusion (tietoevry.com)",https://careers.tietoevry.com/job/power-bi-and-data-engineer-tietoevry-create-m-f-d-in-dallas-us-jid-362?_atxsrc=AttraxLinkedin&utm_source=AttraxLinkedin
PwC,https://www.linkedin.com/company/pwc/life,Data Engineer - Senior Manager,https://www.linkedin.com/jobs/view/4256541006,4256541006,"California, United States",Hybrid,$124K/yr - $280K/yr,2025-06-26 19:54:44,False,,"Specialty/Competency: Data, Analytics & AI

Industry/Sector: Not Applicable

Time Type: Full time

At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth. In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.

Growing as a strategic advisor, you leverage your influence, expertise, and network to deliver quality results. You motivate and coach others, coming together to solve complex problems. As you increase in autonomy, you apply sound judgment, recognising when to take action and when to escalate. You are expected to solve through complexity, ask thoughtful questions, and clearly communicate how things fit together. Your ability to develop and sustain high performing, diverse, and inclusive teams, and your commitment to excellence, contributes to the success of our Firm.

Skills

Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:

Craft and convey clear, impactful and engaging messages that tell a holistic story.Apply systems thinking to identify underlying problems and/or opportunities.Validate outcomes with clients, share alternative perspectives, and act on client feedback.Direct the team through complexity, demonstrating composure through ambiguous, challenging and uncertain situations.Deepen and evolve your expertise with a focus on staying relevant.Initiate open and honest coaching conversations at all levels.Make difficult decisions and take action to resolve issues hindering team effectiveness.Model and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.

As part of the Data and Analytics Engineering team you design and implement thorough data architecture strategies that meet current and future business needs. As a Senior Manager you lead large projects, innovate processes, and maintain operational excellence while interacting with clients at a strategic level to drive project success. You also be responsible for developing and documenting data models, data flow diagrams, and data architecture guidelines, maintaining compliance with data governance and data security policies, and collaborating with business stakeholders to translate their data requirements into technical solutions.

Responsibilities

 Design and implement thorough data architecture strategies Lead large-scale projects and innovate processes Maintain operational excellence and client interactions Develop and document data models and data flow diagrams Adhere to data governance and security policies Collaborate with business stakeholders to translate data requirements into technical solutions Drive project success through strategic advising and problem-solving Foster a diverse and inclusive team environment

What You Must Have

 Bachelor's Degree 8 years of experience

What Sets You Apart

 Certification in Cloud Platforms [e.g., AWS Solutions Architect, AWS Data Engineer, Google Professional Cloud Architect, GCP Data Engineer Microsoft Azure Solutions Architect, Azure Data Engineer Associate, Snowflake Core, Snowflake Architect, Databricks Data Engineer Associate] is a plus Designing and implementing thorough data architecture strategies Developing and documenting data models and data flow diagrams Maintaining data architecture compliance with governance and security policies Collaborating with stakeholders to translate data requirements into solutions Evaluating and recommending new data technologies and tools Leading data strategy engagements providing thought leadership Developing leading practices for Data Engineering, Data Science, and Data Governance Architecting and implementing cloud-based solutions meeting industry standards

Learn more about how we work: https://pwc.to/how-we-work

PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.

As PwC is an equal opportunity employer, all qualified applicants will receive consideration for employment at PwC without regard to race; color; religion; national origin; sex (including pregnancy, sexual orientation, and gender identity); age; disability; genetic information (including family medical history); veteran, marital, or citizenship status; or, any other status protected by law. 

For only those qualified applicants that are impacted by the Los Angeles County Fair Chance Ordinance for Employers, the Los Angeles' Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, San Diego County Fair Chance Ordinance, and the California Fair Chance Act, where applicable, arrest or conviction records will be considered for Employment in accordance with these laws. At PwC, we recognize that conviction records may have a direct, adverse, and negative relationship to responsibilities such as accessing sensitive company or customer information, handling proprietary assets, or collaborating closely with team members. We evaluate these factors thoughtfully to establish a secure and trusted workplace for all.

Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines

The salary range for this position is: $124,000 - $280,000, plus individuals may be eligible for an annual discretionary bonus. For roles that are based in Maryland, this is the listed salary range for this position. Actual compensation within the range will be dependent upon the individual's skills, experience, qualifications and location, and applicable employment laws. PwC offers a wide range of benefits, including medical, dental, vision, 401k, holiday pay, vacation, personal and family sick leave, and more. To view our benefits at a glance, please visit the following link: https://pwc.to/benefits-at-a-glance

",https://ad.doubleclick.net/ddm/clk/438405794;254302458;m?https://jobs.us.pwc.com/job/-/-/932/81216676768?utm_source=linkedin.com&utm_campaign=core_media&utm_medium=social_media&utm_content=job_posting&ss=paid
Centraprise,https://www.linkedin.com/company/centraprise/life,"Sr. Data Engineer (GCP, Vertex AI)",https://www.linkedin.com/jobs/view/4256564831,4256564831,"Cincinnati, OH",Remote,,2025-06-27 00:34:19,True,42 applicants,"Sr. Data Engineer (GCP, Vertex AI)Cincinnati, OH - Remote12+ Months Contract
Job Description:
As a Sr. Data Engineer, you will have the opportunity to lead the development of innovative data solutions, enabling the effective use of data across the organization. You will be responsible for designing, building, and maintaining robust data pipelines and platforms to meet business objectives, focusing on data as a strategic asset. Your role will involve collaboration with cross-functional teams, leveraging cutting-edge technologies, and ensuring scalable, efficient, and secure data engineering practices. A strong emphasis will be placed on expertise in GCP, Vertex AI, and advanced feature engineering techniques.
Key Responsibilities:Provide Technical Leadership: Offer technical leadership to ensure clarity between ongoing projects and facilitate collaboration across teams to solve complex data engineering challenges.Build and Maintain Data Pipelines: Design, build, and maintain scalable, efficient, and reliable data pipelines to support data ingestion, transformation, and integration across diverse sources and destinations, using tools such as Kafka, Databricks, and similar toolsets.Drive Digital Innovation: Leverage innovative technologies and approaches to modernize and extend core data assets, including SQL-based, NoSQL-based, cloud-based, and real-time streaming data platforms.Implement Feature Engineering: Develop and manage feature engineering pipelines for machine learning workflows, utilizing tools like Vertex AI, BigQuery ML, and custom Python libraries.Implement Automated Testing: Design and implement automated unit, integration, and performance testing frameworks to ensure data quality, reliability, and compliance with organizational standards.Optimize Data Workflows: Optimize data workflows for performance, cost efficiency, and scalability across large datasets and complex environments.Mentor Team Members: Mentor team members in data principles, patterns, processes, and practices to promote best practices and improve team capabilities.Draft and Review Documentation: Draft and review architectural diagrams, interface specifications, and other design documents to ensure clear communication of data solutions and technical requirements.Cost/Benefit Analysis: Present opportunities with cost/benefit analysis to leadership, guiding sound architectural decisions for scalable and efficient data solutions.",https://www.linkedin.com/job-apply/4256564831
Lemonlight,https://www.linkedin.com/company/lemonlight-media/life,Senior Engineer: Data & AI,https://www.linkedin.com/jobs/view/4256519459,4256519459,"Los Angeles, CA",Remote,$130K/yr - $220K/yr,2025-06-26 17:41:28,True,over 100 applicants,"Senior Software EngineerData & AI Squad | Hybrid - Los AngelesIC 4-5
🍋 About UsLemonlight is setting the standard for what the future of film production looks like with our AI-first internal platform, Hero. We combine the best of white-glove, hands on creative teammates with the future of AI video production, scheduling, and data management to empower the world’s best brands and creatives. We’ve likely had a hand in some of your favorite commercials, and we’re expanding to match our constant growth. If you want a chance to work inside the studio and not on the sidelines, this is it. 
Our product team is located in Los Angeles and Europe, and we’ve been rated one of the best companies to work for in Los Angeles by BuiltIn. 
📽️ Why This Role ExistsHero powers every shoot, script, storyboard, call-sheet, client interaction… you name it. Our Data & AI squad is at the center of this, developing systems that streamline how our 100+ creatives, producers, editors, and crew work around the world, centralizing every point of data so everyone is on the same page. Think of how many questions, challenges, issues, and disconnected bits of data come up from a film shoot. Now picture all of that in one place, giving you real-time feedback, insights, instructions, and creative outputs. When people use Hero, they will measure in minutes what used to take them months. 
You’ll be joining a team at a pivotal moment—working on high-leverage projects like universal chat editing, knowledge streaming, data enrichment, talent scheduling optimization, automated scheduling, call sheet creation, and much, much more to empower everyone to do more, creatively. You will interface with producers, editors, and many more people who use our product daily and sit in the office with us. 
Together we’ll radically improve production speed and accuracy so creatives can spend more time creating. 
🛠️ What You’ll Work On🚛 Build and ship production ready projects from end-to-end with a team that prioritizes speed of learning using React.js, React Query, Tailwind CSS, MUI, and more. At Lemonlight, “everyone builds”, regardless of title. 🙏 Collaborate with product experience managers and other engineers on a small, autonomous squad focused on improving capacity and saving time through AI tools. 📐 Architect solutions that centralize knowledge, empower faster editing, and provide better AI outputs for the creative team. 🧱 Own services like prompting, knowledge collection, knowledge centralization, fact management, AI training, and more.⚡️ Rapidly prototype to go from idea to POC in collaboration with the PXM team.🤔 Hold yourself accountable for key outcomes for your squad, performance metrics for the products, and the customer experience.🗻 Elevate standards according to our motto of being 1% better every day. 
🏆 What Winning Looks LikeProducts increase project capacity per teammateFeatures save teammates measurable hours per weekSquad OKRs are surpassed, raising the bar for next timeAI models improve themselves over time, and more data is centralized into HeroHero’s product satisfaction score goes through the roofSpeed of learning is prioritized over perfectionPOCs are brought to life in hours, not months
☑️ Who We’re Looking For6+ years of direct, full-stack engineering experienceStrong experience in JavaScript and React ecosystemsDirect experience integrating AI tooling into outcome focused featuresExperience driving features from 0 to 1Strong customer empathy and a willingness to learn rather than proveCan execute beautiful front end projects and complex API integrationsProven background of engineering collaboration, Git management, and shipping cross-functionally facing products
➕ Bonus Points ForYou have a library of agents and personal AI projects you’ve builtYou have deep, direct experience with our tech stackYou can take Figma files and rapidly transform them into prototypesYou have SaaS and B2B or marketplace experienceThe idea of organizing complex challenges into clear experiences lights your soul on fire
⚙️ Our StackBackend: Node.js, NestJS, MongoDB, AWSFrontend: React 19, Tanstack Query, MUI, React-hook-form, TipTap, AmplitudeAI: OpenAI, Fal, ElevenLabs, Langchain
💰 Compensation and Interview ExpectationsThe base salary range for this role is IC4-IC5:🇺🇸 Tier 1 USA: $130,000 to $220,000
You'll be given a step by step guide for our interview process, tips, and expectations at each stage to help you put your best foot forward.
🎁 PerksUnlimited vacation policyCollaborative team training & learning programsHealth, dental, visionInternational offsitesMonthly socials on “First Fridays”In-person team collaboration",
PwC,https://www.linkedin.com/company/pwc/life,Data Engineer- Manager,https://www.linkedin.com/jobs/view/4256539020,4256539020,"California, United States",Hybrid,$99K/yr - $232K/yr,2025-06-26 19:54:44,False,,"Specialty/Competency: Data, Analytics & AI

Industry/Sector: EUR X-Sector

Time Type: Full time

Travel Requirements: Up to 80%

At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth. In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.

Enhancing your leadership style, you motivate, develop and inspire others to deliver quality. You are responsible for coaching, leveraging team member’s unique strengths, and managing performance to deliver on client expectations. With your growing knowledge of how business works, you play an important role in identifying opportunities that contribute to the success of our Firm. You are expected to lead with integrity and authenticity, articulating our purpose and values in a meaningful way. You embrace technology and innovation to enhance your delivery and encourage others to do the same.

Skills

Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:

Analyse and identify the linkages and interactions between the component parts of an entire system.Take ownership of projects, ensuring their successful planning, budgeting, execution, and completion.Partner with team leadership to ensure collective ownership of quality, timelines, and deliverables.Develop skills outside your comfort zone, and encourage others to do the same.Effectively mentor others.Use the review of work as an opportunity to deepen the expertise of team members.Address conflicts or issues, engaging in difficult conversations with clients, team members and other stakeholders, escalating where appropriate.Uphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.

Minimum Degree Required

Bachelor's Degree

Required Field(s) Of Study

Management Information Systems, Computer and Information Science, Systems Engineering,Electrical Engineering,Chemical Engineering,Industrial Engineering,Mathematics,Statistics,Mathematical Statistics

Minimum Year(s) Of Experience

5 year(s)

Preferred Qualifications

Demonstrates abilities and/or success in one or many of the following areas:

Design and implement comprehensive data architecture strategies that meet the current and future business needs; Develop and document data models, data flow diagrams, and data architecture guidelines; Ensure data architecture is compliant with data governance and data security policies; Collaborate with business stakeholders to understand their data requirements and translate them into technical solutions; Evaluate and recommend new data technologies and tools to enhance data architecture; Build, maintain, and optimize ETL/ELT pipelines for data ingestion, processing, and storage across batch and real-time data processing; Build, maintain, and optimize Data Quality rules leveraging DQ tools and/or other ETL/ELT tools; Develop and deploy scalable data storage solutions using AWS, Azure and GCP services such as S3, Redshift, RDS, DynamoDB, Azure Data Lake Storage, Azure Cosmos DB, Azure SQL DB, GCP Cloud Storage etc.; Implement data integration solutions using AWS Glue, AWS Lambda, Azure Data Factory, Azure Functions, GCP Functions, GCP Dataproc, Dataflow and other relevant services; Design and manage data warehouses and data lakes, ensuring data is organized and accessible; Monitor and troubleshoot data pipelines, data warehouses and workflows to ensure data quality, system reliability, performance and cost management; Implement IAM roles and policies to manage access and permissions within AWS, Azure, GCP; Use AWS CloudFormation, Azure Resource Manager templates, Terraform for infrastructure as code (IaC) deployments; Use AWS, Azure and GCP DevOps services to build and deploy DevOps pipelines; Implement data security best practices using AWS, Azure, GCP, Snowflake or Databricks; Optimize Cloud resources for cost, performance, and scalability; Strong proficiency in SQL and experience with relational databases; Proficient in programming languages such as Python, Java, or Scala; Familiarity with big data technologies like Hadoop, Spark, or Kafka is a plus; Experience with machine learning and data science workflows is a plus; Knowledge of data governance and data security best practices; Strong analytical, problem-solving, and communication skill; and, Ability to work independently and as part of a team in a fast-paced environment. 

Demonstrates thought leader-level abilities with, and/or a proven record of success directing efforts in the following areas:

Applying modern, cloud-based technology skills, ability to research emerging trends, analyst publications, and adoption of modern technologies in solution architectures; Collaborating and contributing as a team member: understanding personal and team roles, contributing to a positive working environment by building proven relationships with team members, proactively seeking guidance, clarification and feedback; Prioritizing and handling multiple tasks, researching and analyzing pertinent client, industry and technical matters, utilizing problem-solving skills, and communicating effectively in written and verbal formats to various audiences (including various levels of management and external clients) in a professional business environment; and, Coaching and collaborating with associates who assist with this work, including providing coaching, feedback and guidance on work performance. 

Learn more about how we work: https://pwc.to/how-we-work

PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.

As PwC is an equal opportunity employer, all qualified applicants will receive consideration for employment at PwC without regard to race; color; religion; national origin; sex (including pregnancy, sexual orientation, and gender identity); age; disability; genetic information (including family medical history); veteran, marital, or citizenship status; or, any other status protected by law. 

For only those qualified applicants that are impacted by the Los Angeles County Fair Chance Ordinance for Employers, the Los Angeles' Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, San Diego County Fair Chance Ordinance, and the California Fair Chance Act, where applicable, arrest or conviction records will be considered for Employment in accordance with these laws. At PwC, we recognize that conviction records may have a direct, adverse, and negative relationship to responsibilities such as accessing sensitive company or customer information, handling proprietary assets, or collaborating closely with team members. We evaluate these factors thoughtfully to establish a secure and trusted workplace for all.

Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines

The salary range for this position is: $99,000 - $232,000, plus individuals may be eligible for an annual discretionary bonus. For roles that are based in Maryland, this is the listed salary range for this position. Actual compensation within the range will be dependent upon the individual's skills, experience, qualifications and location, and applicable employment laws. PwC offers a wide range of benefits, including medical, dental, vision, 401k, holiday pay, vacation, personal and family sick leave, and more. To view our benefits at a glance, please visit the following link: https://pwc.to/benefits-at-a-glance

",https://ad.doubleclick.net/ddm/clk/438405794;254302458;m?https://jobs.us.pwc.com/job/-/-/932/81216676944?utm_source=linkedin.com&utm_campaign=core_media&utm_medium=social_media&utm_content=job_posting&ss=paid
Lensa,https://www.linkedin.com/company/lensa/life,Sr Storage Engineer and Admin (Remote),https://www.linkedin.com/jobs/view/4256387675,4256387675,"Atlanta, GA",Remote,$145K/yr - $172K/yr,2025-06-26 13:54:49,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is hiring a Network Storage Engineer for designing, implementing, and maintaining storage solutions for the Department of Veterans Affairs. The candidate will monitor and maintain storage environments, ensuring optimal performance. This position is fully remote within the United States.

Responsibilities

Develop and deploy scalable storage solutions tailored to enterprise needs, ensuring efficiency and reliability. Oversee the monitoring and maintenance of storage infrastructure, proactively identifying and resolving performance issues. Implement and manage data backup and recovery strategies to safeguard critical information and minimize downtime. Collaborate cross-functionally with network engineers and system administrators to optimize storage performance and integration. Enforce robust security measures to protect data integrity and ensure compliance with industry regulations. 

Qualifications

Required Skills and Experience:

Bachelor's degree in Computer Science, Information Technology with 12+ years of experience or commensurate experience Expertise in Storage Area Networks (SAN) and Network Attached Storage (NAS). Knowledge of cloud storage solutions (AWS, Azure, Google Cloud).  Ability to troubleshoot storage performance issues. 

Preferred Skills and Experience:  

Proven experience in designing and implementing architecture to support scalable and efficient system development. Familiarity with Agile methodologies and project management tools.   Experience with technologies like VMware vSAN, or Hyper-V. Familiarity with AWS S3, or Azure Blob Storage. Ability to fine-tune storage environments for efficiency. Understanding of fiber channel, iSCSI, and other storage networking protocols. 

**Clearance Required   **

Ability to obtain and maintain a suitability/Public Trust   

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $145,000.00 - USD $172,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6045/sr-storage-engineer-and-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6045

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/6fcfb8a2fc834a2c80459a8b1f015ddbtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer,https://www.linkedin.com/jobs/view/4258017420,4258017420,United States,Remote,,2025-06-26 14:10:43,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Zeelo.

Data Engineer

Remote - UK based (Occasional travel to London - likely quarterly for Kickoffs)

As a Data Engineer, you will lay the groundwork for extracting value from data by developing and maintaining powerful and efficient data infrastructure. In addition, you will help Zeelo's Data Team become the center of excellence on data and responsible for educating other teams, establishing best practices, and facilitating knowledge sharing on data-related matters across the company.

Zeelo and its clients have many data analytics needs all supported by Zeelo's data team. You will support data analysts by engineering ETL pipelines to deliver useful, clean, clear, and timely data ready for analytics use. Zeelo's data maturity is growing, and you will help incorporate streaming and AI tools into the business effectively. You will also maintain Zeelo's serverless data architecture and improve its functionality and efficiency.

What We Want You To Know About Zeelo

Zeelo is on a mission to make shared transportation more accessible, efficient, and sustainable. We're scaling fast, and this is a chance to help shape the future of our technology in a role where you'll have real ownership and impact.

Zeelo is a transit-tech company powering bus operators, employers and schools to provide highly efficient, sustainable, and affordable transport programs. Our mission is to empower opportunity through sustainable transportation. Our vision is to build the category leader for employers and schools offering transportation as a benefit. Our culture strives to match a high performing sports team. We are inspired by the “Ubuntu” mindset: I am, because we are. Our model is asset light, we do not own vehicles or employee drivers, instead we routinely procure bus operator partners to provide ground transportation We're a team of 130+ across 3 offices (London, Barcelona & Boston) and our transit services are live in 2 markets (UK & US) Our values are Trust, Efficiency, and Drive. 

What will I be doing?

Design, build, and maintain scalable and reliable data pipelines. Manage Zeelo's serverless centralized data architecture (Fivetran, BigQuery, dbt, and other tools) that supports analytical functions across the business. Design, build, and maintain ETL, ELT and other data pipelines for purposes to support analytics use cases. Identify improvements in how Zeelo collects, manages and leverages internal and external data sources. Identify and deliver improvements to scalability and cost. Optimize queries and pipelines for cost and performance. Develop data infrastructure to power accurate and efficient analytics. Be a champion of data and analytics within the company, educating team members and supporting data use throughout the business. Work with transportation data including location data, scheduling data, ridership data, and financial data. Write clear documentation on the mechanics of the data architecture. 

Skills And Experience We're Looking For

Bachelor's degree in a quantitative field. Advanced degrees are a plus. Min 3+ years data engineering experience in a commercial environment. Proficiency in SQL. Experience building SQL-based transformation flows in dbt or similar tools. Good understanding of cloud platforms such as GCP, AWS or Azure. Experience configuring orchestration of SQL and Python via Airflow or similar tools. Experience working with data pipelines, defining problems, crafting and launching solutions, and practicing continuous improvement. Experience with process improvement frameworks and/or project management frameworks is a plus. Experience maintaining a data warehouse including adding features to improve utility and refactoring to reduce costs. Knowledge of data modeling best practices. Experience with REST APIs. Experience building unit tests or working with testing frameworks. Experience with data governance and security. A passion for sustainability, technology, and improving mobility. Experience developing in Python (optional). Experience with transportation systems (optional). 

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/207950b13daf4d8c8b9264ef5a1febactjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Lorven Technologies Inc.,https://www.linkedin.com/company/lorventech/life,"GCP Data Engineer - Geopatial Data Specialist - Houston, TX - 12+ Months Contract",https://www.linkedin.com/jobs/view/4255445996,4255445996,"Houston, TX",On-site,,2025-06-27 00:14:20,True,20 applicants,"Job Title: GCP Data Engineer – Geospatial Data Specialist

Location: Houston, TX (Onsite)

Duration: 12+ Months Contract 

Required Qualifications

 3+ years of hands-on experience as a Data Engineer on Google Cloud Platform (GCP). Strong proficiency with SQL (especially BigQuery GIS) and Python or Java. Experience handling geospatial data formats (GeoJSON, KML, shapefiles, raster, tiles) and spatial indexing techniques. Familiarity with PostGIS, GDAL, GeoPandas, or other GIS libraries and tools. Experience with streaming and batch data processing (Kafka/PubSub, Apache Beam, Dataflow). Solid understanding of data warehousing and data lake architectures. Experience with Airflow or Cloud Composer for orchestration.",https://www.linkedin.com/job-apply/4255445996
Lensa,https://www.linkedin.com/company/lensa/life,HR Data Engineer,https://www.linkedin.com/jobs/view/4258015916,4258015916,United States,Remote,$94.8K/yr - $151.4K/yr,2025-06-26 14:10:24,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for General Motors.

Job Description

Work Arrangement:

This role is based remotely, but if you live within a 50-mile radius of Atlanta (GA), Austin (TX), Detroit (MI), Warren (MI), or Mountain View (CA) you are expected to report to that location three times a week, at minimum.

The Role

The People Analytics CoE is seeking an HR Data Engineer to support data engineering, data automation, and data analytics solutions development, enhancements and management. In this role, you’ll be responsible for enhancing our existing HR data foundation (Databricks) by designing, building, and maintaining scalable data pipelines and integrations that enable accurate, secure, and timely access to HR data across the organization. You will build data models and optimize our databases to improve decision making, and evaluate the integrity, quality, and reliability of data outputs and outcomes. You’ll work closely with the broader PA team, IT and HR COEs to empower data-driven decisions around workforce strategy, talent acquisition, performance management, and employee engagement. This role reports to a People Analytics Solutions Lead and requires strong HR data engineering technical expertise working with large HR datasets and demonstrated knowledge of different areas of HR (data, analytics, and processes)

What's in it for you? You will have a chance to influence our talent strategy, design the PA data infrastructure stack, develop insights that matter, and be part of a great team that puts innovation and curiosity at the center of everything we do.

Responsibilities

Design, develop, and maintain ETL/ELT processes for HR data from multiple systems including Workday to empower data-driven decision-making Drive implementation of robust HR data models and pipelines optimized for reporting and analytics, ensuring data quality, reliability, and security for on-prem and Azure cloud solutions. Develop pipelines and testing automation to ensure HR data quality and integrity across multiple data sources Collaborate with People Analytics and HR business partners to understand data requirements and deliver reliable solutions. Collaborate with technical teams to build the best-in-class data environment and technology stack for People Analytics teams. Ensure data integrity, quality, consistency, security, and compliance (e.g., GDPR, CCPA, HIPAA where applicable). Design and implement secure processes for handling sensitive information in our data tech stack while maintainingappropriate access controls and confidentiality Automate manual HR reporting and improve data accessibility through scalable data pipelines across the entire HR employee lifecycle Troubleshoot and resolve data-related issues quickly and efficiently. Contribute to HR tech stack evaluations and migrations, especially around data capabilities and API integrations. Incorporate external data sources into internal datasets for comprehensive analysis Manage and optimize platform architecture including Databricks environment configuration and performance optimization Stay up to date with emerging trends and advancements in data engineering – both technically and in the HR and People Analytics/sciences domain 

Additional Job Description

Requirements :

5+ years of experience in HR Data Engineer role leading HR data engineering transformation and implementing data pipelines and data solutions in the People Analytics/HR domain Very good understanding of HR data and HR employee lifecycle processes (talent acquisition, talent development, workforce planning, engagement, employee listening, external benchmarking etc.) Very good understanding of HCM data architecture , models and data pipelines and experience designing and implementing data integrations and ETLs with Workday (RaaS, APIs) Experience designing and automating data and analytics solutions that can provide insights and recommendations at scale Proficiency in SQL, R/Python and ETL tools Deep expertise in modern data platforms (particularly Databricks ) and end-to-end data architecture (DLT Streaming Pipelines, Workflows, Notebooks, DeltaLake, Unity Catalog) Experience with different authentication (Basic Auth, Oauth, etc.) and encryption methods and tools (GPG, Voltage, etc.) Very strong data analytics skills and ability to leverage multiple internal and external data sources to enable data-driven insights and inform strategic talent decisions Knowledge of compliance and regulatory requirements associated with data management Experience working in environments requiring strict confidentiality and handling of sensitive data Great communication skills and ability to explain complex technical concepts to non-technical stakeholders. Degree with quantitative focus (e.g., Mathematics, Statistics) and/or degree in Human Resources is a plus 

Relocation:

This job is not eligible for relocation benefits.

Compensation

The salary range for this role is $94,800 - $151,400. The actual base salary a successful candidate will be offered within this range will vary based on factors relevant to the position. Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance. Benefits: GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more. 

GM DOES NOT PROVIDE IMMIGRATION-RELATED SPONSORSHIP FOR THIS ROLE. DO NOT APPLY FOR THIS ROLE IF YOU WILL NEED GM IMMIGRATION SPONSORSHIP (e.g., H-1B, TN, STEM OPT, etc.) NOW OR IN THE FUTURE.

About GM

Our vision is a world with Zero Crashes, Zero Emissions and Zero Congestion and we embrace the responsibility to lead the change that will make our world better, safer and more equitable for all.

Why Join Us

We believe we all must make a choice every day – individually and collectively – to drive meaningful change through our words, our deeds and our culture. Every day, we want every employee to feel they belong to one General Motors team.

Benefits Overview

From day one, we're looking out for your well-being–at work and at home–so you can focus on realizing your ambitions. Learn how GM supports a rewarding career that rewards you personally by visiting Total Rewards Resources (https://search-careers.gm.com/en/working-at-gm/total-rewards) .

Non-Discrimination and Equal Employment Opportunities (U.S.)

General Motors is committed to being a workplace that is not only free of unlawful discrimination, but one that genuinely fosters inclusion and belonging. We strongly believe that providing an inclusive workplace creates an environment in which our employees can thrive and develop better products for our customers.

All employment decisions are made on a non-discriminatory basis without regard to sex, race, color, national origin, citizenship status, religion, age, disability, pregnancy or maternity status, sexual orientation, gender identity, status as a veteran or protected veteran, or any other similarly protected status in accordance with federal, state and local laws.

We encourage interested candidates to review the key responsibilities and qualifications for each role and apply for any positions that match their skills and capabilities. Applicants in the recruitment process may be required, where applicable, to successfully complete a role-related assessment(s) and/or a pre-employment screening prior to beginning employment. To learn more, visit How we Hire (https://search-careers.gm.com/en/how-we-hire) .

Accommodations

General Motors offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email (Careers.Accommodations@GM.com) us or call us at 800-865-7580. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.

About

We are leading the change to make our world better, safer and more equitable for all through our actions and how we behave. Learn more about:

Our Company (https://search-careers.gm.com/en/working-at-gm/)

Our Culture

How we hire (https://search-careers.gm.com/en/how-we-hire/)

Our diverse team of employees bring their collective passion for engineering, technology and design to deliver on our vision of a world with Zero Crashes, Zero Emissions and Zero Congestion. We are looking for adventure-seekers and imaginative thought leaders to help us transform mobility.

Explore our global location s

The policy of General Motors is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity/expression or veteran status. Additionally, General Motors is committed to being an Equal Employment Opportunity Employer and offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email us at Careers.Accommodations@GM.com .In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.

If you have questions about this posting, please contact support@lensa.com

",https://lensa.com/cgw/0b42c3eced854d41b6e07a7b792c06c0tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Millennium Software and Staffing,https://www.linkedin.com/company/millennium-software-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4248175921,4248175921,"Cincinnati, OH",Remote,,2025-06-12 17:02:30,True,over 100 applicants,"Hi Professional,
We have a W2 job opportunity, and we are currently looking for Data Engineer ,
Job Description: Requirements:• PL/SQL• Python• AWS Redshift",https://www.linkedin.com/job-apply/4248175921
Lensa,https://www.linkedin.com/company/lensa/life,Jr. Data Engineer,https://www.linkedin.com/jobs/view/4258015977,4258015977,"San Antonio, TX",On-site,,2025-06-26 14:10:18,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

This is a unique opportunity to join a high-impact team working on critical data pipelines and transformations that support information security, fraud detection, and access management. Youll work with a modern data stack and gain exposure to cloud technologies, data reliability practices, and business-facing analytics. Data is generally clean, but candidate may handle some transformations, error handling, and sensitive data scrubbing. Role involves working with information security data sets (member and employee access management, external/internal fraud). Some knowledge of fraud models and machine learning is a plus, but not mandatory.

Build and maintain data pipelines using Snowflake, DBT, and other tools in our stack (e.g., DataStage, Talend, Kafka). Support data transformations for reporting (DL4), error handling, and sensitive data scrubbing. Collaborate with cross-functional teams to deliver clean, reliable data for internal and external fraud detection and access management. Contribute to monitoring and reliability efforts, with opportunities to learn tools like Grafana, Fortuna, and Datadog. Work in a cloud-first environment, applying best practices in data engineering and security. 

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

Technical Requirements

Core Technologies: Proficiency with Snowflake and DBT.

Experience with DataStage, Talend, and Kafka (receiving side).

Monitoring & Reliability: Proficient in Grafana, Fortuna, or Datadog for SLO/SLA monitoring.

Familiarity with Google SRE or data reliability engineering principles.

Project Focus Areas: experience in information security datasets (e.g., access management, fraud detection).

Handling clean but sensitive data, including transformations, error handling, and scrubbing.

Supporting business-facing data transformations (DL4/data for reporting).

Cloud Experience: Required, especially in the context of Snowflake/DBT deployments. AI + Machine Learning: Some knowledge of fraud models or ML concepts is a plus, but not required.

Certifications in tools like Snowflake or DBT null

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/c5afa41a29964b69a75a8381530b8195tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
eClinical Solutions,https://www.linkedin.com/company/eclinical-solutions/life,Data Engineer,https://www.linkedin.com/jobs/view/4258201123,4258201123,United States,Remote,,2025-06-26 21:35:24,False,,"eClinical Solutions helps life sciences organizations around the world accelerate clinical development initiatives with expert data services and the elluminate Clinical Data Cloud – the foundation of digital trials. Together, the elluminate platform and digital data services give clients self-service access to all their data from one centralized location plus advanced analytics that help them make smarter, faster business decisions.

You Will Make An Impact

The Data Engineer will work closely with clients and provide technical consulting services, configuration of the elluminate platform, development for specific projects that include trial configuration, quality control, process improvements, system validation, custom analytics development, clinical software implementations and integrations. platform configuration, ETL and custom analytics development. The Data Engineer will engage in technical development and implementation of various software service delivery related activities.

Accelerate your skills and career within a fast-growing company while impacting the future of healthcare. 

Your Day To Day

Design, develop, test, and deploy highly efficient SQL code and data mapping code according to specificationsDevelop ETL code in support of analytic software applications and related analysis projectsWork with Analytics developers, other team members and clients to review the business requirements and translate them into database objects and visualizations Build any analytics reports and visualizations using tools like JReview, Qlik Provide diagnostic support and fix defects as needed Ensure compliance with eClinical Solutions/industry quality standards, regulations, guidelines, and procedures Other duties as assigned 

Take the first step towards your dream career. Here is what we are looking for in this role. 

Qualifications

3+ years of professional experience preferred Bachelor's degree or equivalent experience preferredExperience developing back end, database/warehouse architecture, design and development preferredKnowledge of variety of data platforms including SQL Server, DB2, Teradata, (Cloud based DB a plus)Understanding of Cloud / Hybrid data architecture concepts is a plusKnowledge of clinical trial data is a plus - CDISC ODM, SDTM, or ADAM standardsExperience in Pharmaceutical/Biotechnology/Life Science industry is a plusProficient in SQL, T-SQL, PL/SQL programing Experience in Microsoft Office Applications, specifically MS Project and MS Excel Familiarity with multiple Database Platforms: Oracle, SQL Server, Teradata, DB2 Oracle Familiarity with Data Reporting Tools: QlikSense, QlikView, Spotfire, Tableau, JReview, Business Objects, Cognos, MicroStrategy, IBM DataStage, Informatica, Spark or related Familiarity with other languages and concepts: .NET, C#, Python, R, Java, HTML, SSRS, AWS, Azure, Spark, REST APIs, Big Data, ETL, Data Pipelines, Data Modelling, Data Analytics, BI, Data Warehouse, Data Lake or relate 

Accelerate your skills and career within a fast-growing company while impacting the future of healthcare. We have shared our story, now we look forward to learning yours!

eClinical is a winner of the 2023 Top Workplaces USA national award! We have also received numerous Culture Excellence Awards celebrating our exceptional company vision, values, and employee experience. See all the details here: https://topworkplaces.com/company/eclinical-solutions/

eClinical Solutions is a people first organization. Our inclusive culture values the contribution that diversity brings to our business. We celebrate individual experiences that connect us and that inspire innovation in our community. Our team seeks out opportunities to learn, grow and continuously improve. Bring your authentic self, you are welcome here!

We are proud to be an equal opportunity employer that values diversity. Our management team is committed to the principle that employment decisions are based on qualifications, merit, culture fit and business need.",https://job-boards.greenhouse.io/eclinicalsolutions/jobs/4774577007?gh_src=44f4ebe87us
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256393003,4256393003,"Jackson, MS",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:53,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/005aacc6e4de484193e84c9bc6017a21tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256387932,4256387932,"Denver, CO",Remote,$118K/yr - $122K/yr,2025-06-26 13:54:32,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/efc9ecbf68b7412ebb17aa0f9da501b3tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Big Data Systems Engineer (Remote),https://www.linkedin.com/jobs/view/4256392335,4256392335,"Niceville, FL",Remote,$150K/yr,2025-06-26 13:54:42,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for KBR.

Title

Big Data Systems Engineer (Remote)

Belong, Connect, Grow, with KBR!

KBR’s National Security Solutions (NSS) team provides high-end engineering and advanced technology solutions to our customers in the intelligence and national security communities. In this position, your work will have a profound impact on the country’s most critical role – protecting our national security.

KBR is seeking a Big Data Systems Engineer to join our team. The successful candidate will be part of the KBR team supporting the Test Resource Management Center’s (TRMC) Big Data (BD) and Knowledge Management (KM) Team deploying BD and KM systems for DoD testing Ranges and various acquisition programs

Big Data Systems Engineer – Job Summary

This is being hired nationwide as it is a remote work capable position.

The candidate can either work in one of KBR’s facilities or work from home, assuming the candidate has a stable internet connection.

The Big Data Systems Engineer will work on the deployment and integration of a highly visible data analytic project called Cloud Hybrid Edge-to-Enterprise Evaluation Test & Analysis Suite (CHEETAS) at multiple DoD ranges and labs. As a Big Data Systems Engineer, you will be a critical part of our technical team responsible for deploying CHEETAS within customer environments. You will be the frontline interface that customers will have when first experiencing CHEETAS within their DoD Range and lab environments. This position will require you to work closely with system administrators and software developers to communicate, document and ultimately resolve deployment issues as they arise. You will deploy CHEETAS within disparate DoD testing Ranges and acquisition programs environments (on different non-standard hardware stacks and integrated into different existing ecosystems) sometimes located within DoD vaults with no outside internet connectivity. Come join the KBR BDKM team and be a part of the award-winning team responsible for revolutionizing how data analysis is performed across the entire Department of Defense! 

Roles And Responsibilities

Work on the deployment and integration of a highly visible data analytic project called Cloud Hybrid Edge-to-Enterprise Evaluation Test & Analysis Suite (CHEETAS) at multiple DoD ranges and labs Deploy CHEETAS within customer environments Work closely with system administrators and software developers to communicate, document and ultimately resolve deployment issues as they arise Deploy CHEETAS within disparate DoD testing Ranges and acquisition programs environments (on different non-standard hardware stacks and integrated into different existing ecosystems) sometimes located within DoD vaults with no outside internet connectivity 

Basic Qualifications

Must have an active U.S. government TS/SCI security clearance to be considered for this position This position requires a bachelor's degree in a STEM Computer Science, Data Science, Statistics or related, technical field, and 10 years of DoD experience. Entry level Integration Engineers will NOT be considered due to the breadth of knowledge necessary to be successful in the position. Previous experience must include integration with and configuration of: Hadoop, SQL Server Big Data Cluster, Kubernetes, CentOS, Ubuntu, RedHat, Windows Server, VMWare, etc.) Previous experience must include five (5) years of hands-on experience in big data environments. 

Knowledge / Skills / Abilities

Must be adept at deploying and configuring Big Data and Knowledge Management tools in an enterprise environment. Must have extensive technical expertise in the configuration and troubleshooting of big data ecosystems. Must have excellent written and verbal communication skills and be comfortable assisting customers with installation and configuration of their big data infrastructure. Must have strong troubleshooting skills and the ability to become a CHEETAS deployment subject matter expert. Must be comfortable working with a wide range of stakeholders and functional teams at various levels of experience. Excellent interpersonal skills, oral and written communication skills, and strong personal motivation are necessary to succeed within this position. Experience with installation, configuration, integration with and usage of the following tools and technologies: Helms Charts, YAML, Kubernetes, Kubectl, Kubernetes IDE, NFS, SMB, S3, SQL Server, Windows Server, Windows 10/11, Linux (CentOS, Ubuntu, RedHat), Hadoop. Must be prepared to learn new business processes or CHEETAS application nuances every Agile sprint release (roughly every 6 weeks) prior to deploying to customer sites. Ability to problem solve, debug, and troubleshoot while under pressure and time constraints is required. Ability to communicate effectively about technical topics to both experts and non-experts at both the management and technical level is required. Ability to work independently and provide appropriate recommendations for optimal design, analysis, and development. Excellent verbal communications skills are required, as the Integration Engineer will be in frequent contact with the project technical lead, be taking direction from various government leads, and will frequently be interacting with end users to gather requirements and implement solutions while away from other team members. Excellent testing, debugging and problem-solving skills are required to be successful in this position. Experience designing, building, integrating with and maintaining both new and existing big data systems and solutions. Ability to speak and present findings in front of large groups. Ability to document and repeat procedures. This position is anticipated to require travel of 25% with surges possible up to 50% to support end users located at various DoD Ranges and Labs across the United States. 

Preferred Qualifications

10+ years of experience working in government / defense labs and within their computing restrictions. Experience working in government/defense labs and their computing restrictions. Experience working with major DoD acquisition programs, such as Joint Strike Fighter (JSF). Knowledge of the Test and Training Enabling Architecture (TENA), the Joint Mission Environment Testing Capability (JMETC) and distributed testing and training. Experience with working in distributed team environment is preferred. Ability to teach and mentor engineers with a variety of skill levels and backgrounds is a plus. Knowledge of DoD cybersecurity policies. 

KBR Benefits

KBR offers a selection of competitive lifestyle benefits which could include 401K plan with company match, medical, dental, vision, life insurance, AD&D, flexible spending account, disability, paid time off, or flexible work schedule. We support career advancement through professional training and development.

Basic Compensation

$150,000-200,00

Belong, Connect and Grow at KBRAt KBR, we are passionate about our people and our Zero Harm culture. These inform all that we do and are at the heart of our commitment to, and ongoing journey toward being a People First company. That commitment is central to our team of team’s philosophy and fosters an environment where everyone can Belong, Connect and Grow. We Deliver – Together.

KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/a4b0f389a99f4de78b356da5dbcedae3tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=None&utm_medium=slot&utm_source=linkedin&utm_term=manual
VBeyond Corporation,https://www.linkedin.com/company/vbeyond-corporation/life,Snowflake Data Engineer,https://www.linkedin.com/jobs/view/4256522549,4256522549,United States,Remote,,2025-06-26 18:19:37,True,over 100 applicants,"Job Description 
Job Title : - Snowflake Data Engineer with HealthcareLocation : - RemoteType of Employment : - Fulltime
Minimum : - 12+ Years
Must Have : Snowflake, ETL, IBM data warehousing, Healthcare (HL7, HIPAA, FHIR, EDI, Payer, or provider domain) 
What is in it for you?
As a Lead Data Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards .
Job Summary: -
We are seeking a Lead Data Engineer with deep expertise in Data Warehousing, Cloud Technologies, and Snowflake, along with extensive experience in the Healthcare domain, particularly in Payer and Provider systems.This role requires a combination of technical leadership, solution design, and hands-on expertise to develop scalable and compliant data architectures for enterprise healthcare solutions.Experience with IBM data warehousing technologies, including IBM NPS (Netezza Performance Server), IBM DataStage, and IBM Workload Scheduler (IWS) Jobs, is highly desirable. Key Responsibilities:
Solution Architecture & Design:Design and implement enterprise-scale data warehouse solutions using Snowflake and IBM Netezza (NPS) for healthcare data processing and analytics.Develop secure, scalable, and HIPAA-compliant cloud-based architectures.Define best practices, frameworks, and governance for healthcare data integration, interoperability, and analytics.Architect solutions for payer and provider data management, including claims processing, eligibility verification, risk adjustment, and quality reporting.
Healthcare Data & Cloud Technologies Expertise:
Deep understanding of payer-provider ecosystems, including claims, billing, member enrollment, care management, and EDI transactions (837, 835, 270/271, etc.).Design and optimize large-scale ETL/ELT pipelines using IBM DataStage and Snowflake for structured and unstructured healthcare data.Implement FHIR, HL7, EDI, HIPAA, and SOC2-compliant data architectures.Expertise in Snowflake’s features like Snowpipe, Streams, Secure Data Sharing, and Data Cloning.Experience in cloud migration strategies (AWS, Azure, GCP) and performance tuning of cloud-based healthcare data platforms.Experience with IBM Workload Scheduler (IWS) Jobs for automating and orchestrating healthcare data workflows.
Collaboration & Leadership:
Work closely with healthcare business leaders, product managers, and IT teams to translate complex business needs into scalable solutions.Lead technical discussions, architectural reviews, and strategic roadmaps for payer and provider data ecosystems.Mentor and guide junior architects, engineers, and data professionals in healthcare-specific data management best practices.
Onsite Client Engagement:
To engage with healthcare clients, payers, providers, and IT teams. Act as a trusted advisor to clients, driving value-based care initiatives, population health analytics, and claims data modernization.Bridge communication gaps between business, compliance, and technical teams. Experience: -
12 Years+ Location: -
Remote Educational Qualifications: -
Engineering Degree – BE/ME/BTech/MTech/BSc/MSc.Technical certification in multiple technologies is desirable.
Mandatory skills:-
10+ years of experience in Data Architecture, Cloud Technologies, and Solution Design.Expert in Snowflake and IBM NPS (Netezza), including performance tuning, security, and healthcare data optimization.Strong experience in cloud platforms – AWS, Azure, or GCP.Extensive hands-on experience in IBM DataStage and IBM Workload Scheduler (IWS) Jobs for ETL and job automation.Extensive experience in US healthcare, payer-provider data models, and regulatory frameworks (HIPAA, CMS, NCQA, ONC). Proficiency in SQL, Python, ETL/ELT tools (e.g., IBM DataStage, Matillion, Talend, Informatica).Experience with healthcare analytics, predictive modeling, and reporting solutions.Strong understanding of value-based care models, HEDIS, risk adjustment, and quality reporting.
Good to have skills: -
Snowflake Architect Certification.Experience with FHIR, HL7, EDI transactions, and interoperability solutions.Knowledge of IBM Netezza (NPS), DataStage, and IWS Job scheduling in a healthcare data environment.Experience with real-time data streaming (Kafka, Spark Streaming, Databricks) in healthcare.Experience in AI/ML-driven healthcare analytics.
Recruiter Details : -
Name : - Prashant PalEmail I'd : - PrashantP@vbeyond.com",https://www.linkedin.com/job-apply/4256522549
Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer / Fabric Admin (Remote),https://www.linkedin.com/jobs/view/4256393044,4256393044,"Raleigh, NC",Remote,$140K/yr - $150K/yr,2025-06-26 13:54:49,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currentlyseekinganexperienced and proactivePower BI Developer /Fabric Admin (Remote)to manage and support the enterprise-wide Power BI environment, focusing onPower BI Service administration, Fabric platform oversight, user support across tiers, and end-to-end report lifecycle management. This role requires technical depth in Power BI tools and infrastructure, combined witheffective communicationand stakeholder engagement to work across theCenter of Excellence (COE), business units, and IT support.This position will be fully remote located within the United States.

Responsibilities

Serve as aPower BI Fabric Administrator, overseeing workspace management, deployment pipelines, permissions, and performance monitoring. Develop aPower BI Center of Excellence (COE)to enforce standards, best practices, and governance for enterprise Power BI usage. Conducts PBI Training (Instructor-led Class Development, Delivery). ProvideTier 1 and Tier 2 supportfor Power BI issues—including report access, refresh failures, dataset issues, and workspace configuration. Assist withSelf-Help resources, including FAQs, templates, training materials, and troubleshooting documentation to empower end users. ManagePower BI licensing and desktop software distribution, working with ITassetsand licensing teams to track usage and entitlements. Support integration withexisting data sourcesandfacilitatethedesign and onboarding of new data sources, ensuring data quality, refresh schedules, and secure access. Handle incomingPower BI reporting and workspace requeststhrough a managed queue or mailbox, ensuringtimelyprioritization, response, and resolution. Design, publish, andmaintainPower BI reports and dashboardsaligned with business requirements andoptimizedfor usability and performance. Create andmaintainrobusttechnical documentation, including data dictionaries, architecture diagrams, workspace catalogs, and refresh schedules. Engage in proactive system health checks and performance tuning of Power BI Service, datasets, and gateway configurations. 

Qualifications

Required Skills and Experience

Bachelorswith 12+ years (orcommensurateexperience). Strong experience withPower BI Service Administration, Fabric workspace management, and deployment pipelines. Familiarity withPower BI governance, tenant settings, and organizational policies. Experience supporting and escalating issues acrossTier 1 and Tier 2 support models, including Service Desk and End-User Enablement. Knowledge ofPower BI licensing models(Free, Pro, Premium, PPU) and integration with Microsoft 365 administration tools. 

Preferred Skills And Experience

Solid understanding ofdata connectivity, gateway setup, refresh scheduling, and source control for bothexisting and new data sources.

Clearance Requirements

Ability to obtain and maintain a suitability/public trust clearance

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $140,000.00 - USD $150,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6140/power-bi-developer---fabric-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6140

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/cd4b6ef804da4ce2a511eddfff5ffe38tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Kforce Inc,https://www.linkedin.com/company/kforce/life,Data Engineer,https://www.linkedin.com/jobs/view/4225513532,4225513532,"Charlotte, NC",On-site,$51.75/hr - $61.13/hr,2025-05-29 11:55:27,True,over 100 applicants,"Responsibilities

Kforce has a client that is seeking a Data Engineer in Charlotte, NC. This is a contract, potential to hire role. Overview: We are seeking a seasoned Data Engineer to spearhead our data modernization initiatives within the AWS ecosystem. This pivotal role demands a multifaceted professional adept in AWS architecture, data warehousing, and modern data management practices. The ideal candidate will possess a deep understanding of AWS best practices, data modeling, and have hands-on experience with tools like Apache Iceberg, S3, Python, and PySpark. This position offers the potential for full-time employment based on performance and organizational fit. Key Responsibilities:

 Data Engineer will design and implement scalable, secure, and efficient data architectures on AWS, adhering to industry best practices Assist with the modernization of data warehousing solutions, with a focus on integrating Apache Iceberg as the primary Data Lakehouse format Develop and optimize data pipelines using Python and PySpark to facilitate seamless data ingestion, transformation, and loading processes

Requirements

 Bachelor's or Master's degree in Computer Science, Information Systems, or a related field Extensive experience with AWS services, including but not limited to S3, Glue, EMR, and Redshift Hands-on experience with Apache Iceberg, or a strong willingness and ability to learn and implement it effectively Proficiency in Python and PySpark for data processing tasks Strong understanding of data warehousing concepts and experience with platforms like Redshift, Hive, or Snowflake Experience in designing and managing MDM systems and OLTP databases Excellent problem-solving skills and the ability to work independently Strong communication skills, both verbal and written

Preferred Qualifications

 Experience with infrastructure-as-code tools like Terraform or CloudFormation Familiarity with CI/CD pipelines and DevOps practices Knowledge of data governance and security best practices

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.

By clicking “Apply Today” you agree to receive calls, AI-generated calls, text messages or emails from Kforce and its affiliates, and service providers. Note that if you choose to communicate with Kforce via text messaging the frequency may vary, and message and data rates may apply. Carriers are not liable for delayed or undelivered messages. You will always have the right to cease communicating via text by using key words such as STOP.",
Engtal,https://www.linkedin.com/company/engtal/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4256547666,4256547666,"Chicago, IL",Hybrid,$300K/yr - $450K/yr,2025-06-26 20:45:38,True,over 100 applicants,"Join a forward-thinking technology organization at the heart of global financial markets. We are seeking a Senior Data Engineer who thrives in dynamic environments and is passionate about building scalable data solutions that drive real-time decision-making.
Key ResponsibilitiesDesign and implement robust data infrastructure utilizing tools such as Kafka, Hadoop, and Dremio.Develop and maintain data processing pipelines using languages and frameworks like Java, Python, Spark, and Flink.Collaborate with cross-functional teams to optimize data models, ingestion processes, and storage solutions.Ensure data accuracy, availability, and integrity across various platforms.Serve as a subject matter expert in big data technologies, providing guidance and support to development teams.
QualificationsMinimum of 5 years in a data engineering role within a complex, data-intensive environment.At least 3 years of hands-on experience with Kafka, including developing streaming applications and managing clusters.Proficiency in building data pipelines with backends such as S3, HDFS, Databricks, or Iceberg.Strong programming skills in Java, Python, and SQL.Familiarity with data science tools, particularly those based in Python.Experience with containerization and orchestration tools like Docker and Kubernetes.Knowledge of monitoring and alerting systems such as Prometheus, Grafana, Alert Manager, Alerta, and OpsGenie.Solid understanding of statistical analysis and root-cause troubleshooting.Competence in Unix scripting (e.g., Bash, Python).
About the CompanyWe are a global leader in technology-driven solutions, dedicated to innovation and excellence in the financial sector. Our teams operate across North America, Europe, Asia-Pacific, and India, fostering a collaborative culture that emphasizes continuous improvement and impactful contributions to the industry.",https://www.linkedin.com/job-apply/4256547666
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4258018532,4258018532,"Bellevue, WA",On-site,$210K/yr - $235.4K/yr,2025-06-26 14:10:27,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Bachelor's degree in Computer Science, Engineering, Information Systems, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field, followed by five years of progressive, post baccalaureate work experience in the job offered or in a computer-related occupation. Requires five years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP systemPythonGathering and understanding business requirements for complex systems and processesAnalyzing and optimizing performance of complex workflows
Public Compensation

$210,035/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/cb344e9eb4c641bf810226f7f0ee6d83tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Lensa,https://www.linkedin.com/company/lensa/life,Sr. Data Engineer - Remote,https://www.linkedin.com/jobs/view/4256394151,4256394151,"Atlanta, GA",Remote,,2025-06-26 13:54:30,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for UnitedHealth Group.

Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.

Information is the lifeblood of the healthcare industry-everything depends on it. At Optum Insight Technology, you’ll help us work on streamlining the flow of information between payers, healthcare providers and various other stakeholders to deliver the right insights to the right places at the right times, driving better outcomes for patients, reducing friction in the health system and lowering costs. Every day our work directly impacts the world for the better, in meaningful and profound ways.

We live in a time of unprecedented technical capability and possibility. Health care is at a pivotal point in this journey where even small gains can lead to major transformation. You could be a part of that - you have tremendous skill and the potential to make a lasting impact. Optum Insight Technology is uniquely positioned to bring your skills to bear on these pressing and life-changing technical challenges. The health care industry has an immediate need for your drive, innovation, passion, and technical insight. Help us help the millions of people we serve each day.

Functions may include database architecture, engineering, design, optimization, security, and administration; as well as data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning and other similar roles. Responsibilities may include Platform-as-a-Service and Cloud solution with a focus on data stores and associated eco systems. Duties may include management of design services, providing sizing and configuration assistance, ensuring strict data quality, and performing needs assessments. Analyzes current business practices, processes and procedures as well as identifying future business opportunities for leveraging data storage and retrieval system capabilities. Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. May design schemas, write SQL or other data markup scripting and helps to support development of Analytics and Applications that build on top of data. Selects, develops and evaluates personnel to ensure the efficient operation of the function.

You’ll enjoy the flexibility to work remotely* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities

Research, evaluate, identify alternative approaches, recommend, design and code efficient and effective solutions for challenging problems ranging from small to large work efforts for low to high complexity problems Develop, test, deploy and schedule complex ETL/ELT solutions to integrate multiple data asset across the organization Comply with standards and guidelines related to the design, construction, testing and deployment activities as established by departmental and organizational standards Demonstrate collaborative skills working within a project team of diverse skills and will bring communication skills including oral, written and presentation skills, creativity, and problem-solving skills to a challenging environment Partner with other competency leads/ developers and support project planning, technical design, development, and solution deployment functions Identify opportunities in business processes, system capabilities and delivery methodologies for continuous improvement as applicable Lead, mentor and develop development resources and provide project related directions Act in a technical SME role, support development, QA, and production support teams in SDLC and operational activities 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

Undergraduate degree or equivalent experience 5+ years of experience with modern relational databases 5+ years of experience optimizing SQL statements 5+ years of experience working on commercially available software and / or healthcare platforms as a Data Engineer 3+ years of experience building data pipelines in Azure Data factory, Databricks, App services, Az Functions 3+ years of experience working with Synapse, Cosmos 3+ years of experience designing and building Enterprise Data solutions on Cloud 1+ years of experience/knowledge of Power BI 

Preferred Qualifications

Experience building Big Data solutions on public cloud (Azure) Experience with Data warehousing services, preferably synapse and SQL Experience developing RESTful Services in .NET, Java, or any other language Experience with DevOps in Data engineering Experience with Microservices architecture Experience in using modern software engineering and product development tools including Agile/SAFE, Continuous Integration, Continuous Delivery, DevOps etc. 

Technology Careers with Optum . Information and technology have amazing power to transform the health care industry and improve people's lives. This is where it's happening. This is where you'll help solve the problems that have never been solved. We're freeing information so it can be used safely and securely wherever it's needed. We're creating the very best ideas that can most easily be put into action to help our clients improve the quality of care and lower costs for millions. This is where the best and the brightest work together to make positive change a reality. This is the place to do your life's best work.(sm)

California, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only : The salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $85,000 to $167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/41bcc08cb28b483e95603bac53257728tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Addison Group,https://www.linkedin.com/company/addisongroup/life,Data Engineer,https://www.linkedin.com/jobs/view/4256565636,4256565636,"Houston, TX",On-site,,2025-06-26 23:57:47,True,over 100 applicants,"Key Responsibilities:Design, develop, and implement robust data solutions using platforms like DatabricksBuild scalable, reliable, and secure data environments for advanced analyticsLead data migration from traditional RDBMS systems to DatabricksDesign and maintain scalable data pipelines and infrastructureDevelop and manage ETL processes using DatabricksOptimize ETL workflows for performance and data integrityEnsure seamless data transition with minimal business disruptionMonitor, optimize, and scale Databricks environments for reliability and cost-effectivenessCollaborate with cross-functional teams (data engineers, data scientists, analysts, product managers)Define best practices and standards for data engineering and ensure adherenceEvaluate and implement new technologies and toolsProvide technical leadership and mentorship to junior team membersWork closely with DevOps and infrastructure to manage production deploymentsEnsure compliance with organizational, industry, and regulatory standardsAlign data architecture with overall IT strategy in partnership with architects and IT leaders
Required Skills & Experience:BS or MS in Computer Science or relevant fieldExtensive hands-on experience with Databricks, including ETL and data migrationDatabricks certifications strongly preferredExperience with cloud platforms such as AWS, Azure, or GCPStrong understanding of big data technologies and frameworksSolid grasp of data warehousing, data modeling, and SQLExperience transitioning data from RDBMS (e.g., SQL Server) to modern cloud platformsProficiency in Python, SQL, and scripting languagesFamiliarity with data governance frameworks, quality management, and security best practicesExpertise in database technologies and designBonus: Familiarity with Docker, Kubernetes",https://www.linkedin.com/job-apply/4256565636
University of Maryland Medical System,https://www.linkedin.com/company/ummedicalsystem/life,Data Engineer,https://www.linkedin.com/jobs/view/4258065515,4258065515,"Linthicum Heights, MD",Remote,$47/hr - $70.54/hr,2025-06-26 19:47:53,True,over 100 applicants,"Company Description

The University of Maryland Medical System (UMMS) is an academic private health system, focused on delivering compassionate, high quality care and putting discovery and innovation into practice at the bedside. Partnering with the University of Maryland School of Medicine, University of Maryland School of Nursing and University of Maryland, Baltimore who educate the state’s future health care professionals, UMMS is an integrated network of care, delivering 25 percent of all hospital care in urban, suburban and rural communities across the state of Maryland. UMMS puts academic medicine within reach through primary and specialty care delivered at 11 hospitals, including the flagship University of Maryland Medical Center, the System’s anchor institution in downtown Baltimore, as well as through a network of University of Maryland Urgent Care centers and more than 150 other locations in 13 counties. For more information, visit www.umms.org.

Job Description

Ensure analytics infrastructure and associated systems meet business requirements and industry best practices.Gather and process raw data from multiple disparate sources (including writing scripts, calling APIs, write SQL queries, etc.) into a form suitable for analysis.Gathers, analyzes, documents and translates application requirements into data models.Builds data models.Enables big data, batch and real-time analytical processing solutions leveraging emerging technologies.


Qualifications

Bachelor’s degree in computer science, mathematics, information systems, engineering, physical sciences, life sciences or closely related field is required. Four (4) years of equivalent related professional experience may be substituted for education requirement. Additional certifications are preferred.Minimum two (2) years’ experience designing, implementing and supporting systems in a large scale analytics or data engineering environment containing many disparate application systems and multiple data sources is required.


Additional Information

All your information will be kept confidential according to EEO guidelines.

Compensation:

Pay Range: $47- $70.54

Other Compensation (if applicable):

Review the 2024-2025 UMMS Benefits Guide

Like many employers, UMMS is being targeted by cybercriminals impersonating our recruiters and offering fake job opportunities. We will never ask for banking details, personal identification, or payment via email or text. If you suspect fraud, please contact us at careers@umms.edu.",https://www.linkedin.com/job-apply/4258065515
Lensa,https://www.linkedin.com/company/lensa/life,Remote Data Engineer,https://www.linkedin.com/jobs/view/4256393263,4256393263,"Irving, TX",Remote,,2025-06-26 13:54:34,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

Insight Global is seeking a Data Engineer to join a Fortune 100 Healthcare Organization and work remotely. This is a short term project projected to last 2 months, and this individual will be responsible for the development of a new reusable data pipeline using Python and SQL to support various PBM clients.

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

3+ years of data engineering experience Extensive experience with Python and SQL Experience building data pipelines from scratch using Python and SQL Experience with Azure as cloud platform null 

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/c39e34c08626440d9aa331c38b9a99f2tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256388504,4256388504,"Hartford, CT",Remote,$118K/yr - $122K/yr,2025-06-26 13:54:49,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/9f7478ea9b6f40d594ae8b2aca9a3221tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Aspirion ,https://www.linkedin.com/company/aspirion-health-resources-llc/life,Senior Azure SQL Data Engineer,https://www.linkedin.com/jobs/view/4250056733,4250056733,"Atlanta, GA",Remote,,2025-06-13 02:24:17,False,,"Position Overview:

Aspirion is seeking a detail-oriented and performance-driven Senior Azure SQL Data Engineer to join our growing Data Engineering team. This role will focus on designing, building, and optimizing scalable data infrastructure and services that support real-time business operations and enable advanced analytics, including AI/ML initiatives. The engineer will be responsible for driving excellence in database architecture, SQL performance tuning, and Azure resource management, while also contributing to the buildout of our Azure Lakehouse and Microsoft Fabric environment.

Key Responsibilities:

 Database Engineering & Optimization

– Write and optimize advanced T-SQL queries, stored procedures, views, and triggers.

– Analyze execution plans, tune indexes, and resolve deadlocks to maximize performance in OLTP environments.

– Design scalable and reliable schema changes for transactional and analytical systems.

 Azure SQL Infrastructure

– Provision, configure, and manage Azure SQL resources (Azure SQL Database, Managed Instances, SQL on Azure VMs).

– Implement capacity planning for compute and storage across OLTP and OLAP environments.

 Lakehouse Architecture & Fabric Integration

– Support and enhance Azure Data Lakehouse environments using Microsoft Fabric and OneLake.

– Develop ELT pipelines using Data Factory and Synapse Analytics.

– Integrate data governance practices using Microsoft Purview.

 AI & BI Readiness

– Design and prepare data layers optimized for AI/ML use cases and business intelligence consumption.

– Collaborate with analysts and data scientists to ensure performant and accessible datasets.

 Monitoring & Maintenance

– Implement proactive monitoring, logging, and alerting for SQL and data pipeline operations.

– Participate in incident response and continuous improvement of data reliability and performance.

Required Skills and Experience:

 Core SQL Expertise

– Advanced T-SQL development (queries, procedures, triggers)

– OLTP performance analysis and optimization

– Execution plan review, index tuning, deadlock resolution

 Azure Cloud Experience

– Hands-on experience with Azure SQL (all service models)

– Capacity planning for Azure SQL workloads

– Familiarity with Azure networking, access, and security for data services

 Data Engineering Tools

– Proficient with Azure Data Factory, Synapse Analytics, and Data Lake Storage

– Exposure to Microsoft Fabric and OneLake environments

– Understanding of Delta Lake and parquet formats

 Collaboration and Communication

– Proven experience working with cross-functional teams including BI, AI/ML, and DevOps

– Strong documentation and communication skills

 Experience

– 7+ years in data engineering with a focus on SQL and cloud data platforms

– at least 4 years of hands-on Azure SQL development and deployment

Preferred Qualifications:

 Microsoft Certified: Azure Data Engineer Associate or related Azure certifications Experience with Microsoft Fabric or early adoption of Lakehouse platforms Familiarity with data governance and cataloging via Microsoft Purview Exposure to tools supporting AI/ML pipelines in Azure

Educational Requirements:

 Bachelor’s degree in computer science, Data Engineering, Information Systems, or equivalent experience

About Aspirion:

For over two decades, Aspirion has delivered market-leading revenue cycle services, specializing in complex reimbursement challenges. With over 1,400 teammates, our culture is rooted in innovation, excellence, and a shared mission to deliver exceptional outcomes for our healthcare clients. We embrace flexibility, personal growth, and the intelligent use of technology—especially AI—to drive results.

Benefits:

 Competitive compensation and performance-based incentives Health, dental, vision, and life insurance from day one 401(k) with company match Flexible work environment (remote/hybrid) Professional development and certification support Mission-driven, people-first culture",https://aspirion-health-resources-llc.primepay-recruit.com/job/930067/senior-azure-sql-data-engineer?d=2025-06-12+19%3A36%3A54+UTC&s=lif
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Data Engineer,https://www.linkedin.com/jobs/view/4256540781,4256540781,"Needham, MA",Remote,,2025-06-26 20:08:51,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, SeaGlass IT, is seeking the following. Apply via Dice today!

Must have: DBT, SnowFlake, Python, Dagster, AWS, S3

Experience with OCI (Oracle Cloud Infrastructure) and Oracle Cloud services, including Oracle Fusion, is a big plus.

Responsibilities:

Writing and building Python-based data pipelinesScheduling and parameterizing those pipelines using DagsterParameterizing means making things easily adjustable so the data pipelines can adapt to different needs instead of having to be completely redone every time there s a new business need.Working in an AWS + Snowflake environmentHandling data stored in S3Moving data into Snowflake from various sources

Nice to Have Skills: 

Ability to model up through medallion architecture using DBT. Which means:

This means the person knows how to organize and transform data in a structured way using DBT (a data transformation tool), following something called the medallion architecture.

Medallion architecture: A layered approach to data transformation that separates data into:

Bronze (raw data),

Silver (cleaned and filtered),

Gold (final, business-ready data).",https://click.appcast.io/t/Muvz0wc1DXDQ5qKG5oqN0YYgCXOJTGNWgnomeinHtDg=
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4258021314,4258021314,"Olympia, WA",On-site,$220.7K/yr - $235.4K/yr,2025-06-26 14:10:27,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/aef14fb24bba44c19e118582a7a5eee5tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Radiansys Inc.,https://www.linkedin.com/company/radiansys-inc/life,"SAP HANA Data Engineer Lead(Azure, Pyspark, sql and migration data from HANA to Databricks ) Remote",https://www.linkedin.com/jobs/view/4258206114,4258206114,"California, United States",Remote,,2025-06-26 22:52:19,True,30 applicants,"HiWe are looking for SAP HANA Data Engineer Lead(Azure, Pyspark, sql and migration data from HANA to Databricks ) Remote. Anyone interested can share your resume atpkumar@radiansys.com 
Title: Technical Lead/San Jose, CA (Onsite)/Remote Location: San Jose, CA 95110 (First Preference) / Santa Clara, CA 95054 (Onsite)Direct-Hire / Contract (C2C/W2) Open for Remote 
CLIENT UPDATE – In terms of skills, core expectation is Data Engineering, Azure skills – mainly PySpark and SQL. Experience of SAP HANA as they migrate data from HANA to Databricks.
We are looking for a skilled Technical Lead to oversee the design and implementation of large-scale data engineering andmigration projectsDatabricks & Delta Lake — Evaluate good understanding of Hands-on experience with Databricks, including development using PySpark or Spark SQL, Efficient use of Delta Lake for scalable data pipelines, and data lineage in DatabricksAzure Data Factory — Evaluate good understanding of Building and managing ETL pipelines using ADF, Using ADF for orchestration with Databricks, Blob Storage, SAP sources, Monitoring, error handling, and pipeline performance tuningPerformance Optimization Evaluate good understanding of handling Performance optimization on DatabricksPython & PySpark — Evaluate good understanding of Writing robust, maintainable data processing scripts, Using Python/Spark for custom transformations and integration logicSAP HANA Experience/knowledge — Integrating SAP data with other platforms, Handling large-scale SAP data extraction, transformation, and migration; Good understanding of SAP HANA architecture and data modelingETL Tools, SAP Data Services — Evaluate good understanding of Creating, deploying, and optimizing data jobs in SAP BODS/Data Services, Working with complex mappings and SAP-specific data types, Handling change data capture (CDC) scenariosData Profiling & Validation — Evaluate good understanding of Experience in data profiling, validation, and reconciliation during migrationsAzure Cloud & Networking — Evaluate good understanding of Azure services related to compute, storage, networking, and security, Experience resolving firewall, VPN, and VNet issues impacting data pipelines, Familiarity with IAM, RBAC, and secure credential storage

Regards,PinkuTalent Acquisition – Radiansys Inc.39510 Paseo Padre Pkwy #110, Fremont, CA 94538Direct: 510 790 2000 Ext 1006Email: pkumar@radiansys.com",https://www.linkedin.com/job-apply/4258206114
Lensa,https://www.linkedin.com/company/lensa/life,Sr Storage Engineer and Admin (Remote),https://www.linkedin.com/jobs/view/4256392282,4256392282,"Olympia, WA",Remote,$145K/yr - $172K/yr,2025-06-26 13:54:46,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is hiring a Network Storage Engineer for designing, implementing, and maintaining storage solutions for the Department of Veterans Affairs. The candidate will monitor and maintain storage environments, ensuring optimal performance. This position is fully remote within the United States.

Responsibilities

Develop and deploy scalable storage solutions tailored to enterprise needs, ensuring efficiency and reliability. Oversee the monitoring and maintenance of storage infrastructure, proactively identifying and resolving performance issues. Implement and manage data backup and recovery strategies to safeguard critical information and minimize downtime. Collaborate cross-functionally with network engineers and system administrators to optimize storage performance and integration. Enforce robust security measures to protect data integrity and ensure compliance with industry regulations. 

Qualifications

Required Skills and Experience:

Bachelor's degree in Computer Science, Information Technology with 12+ years of experience or commensurate experience Expertise in Storage Area Networks (SAN) and Network Attached Storage (NAS). Knowledge of cloud storage solutions (AWS, Azure, Google Cloud).  Ability to troubleshoot storage performance issues. 

Preferred Skills and Experience:  

Proven experience in designing and implementing architecture to support scalable and efficient system development. Familiarity with Agile methodologies and project management tools.   Experience with technologies like VMware vSAN, or Hyper-V. Familiarity with AWS S3, or Azure Blob Storage. Ability to fine-tune storage environments for efficiency. Understanding of fiber channel, iSCSI, and other storage networking protocols. 

**Clearance Required   **

Ability to obtain and maintain a suitability/Public Trust   

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $145,000.00 - USD $172,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6045/sr-storage-engineer-and-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6045

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/04eda5c745894b9091fef192d475642dtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Foodsmart,https://www.linkedin.com/company/hellofoodsmart/life,Data Engineer,https://www.linkedin.com/jobs/view/4226046067,4226046067,United States,Remote,$130K/yr - $155K/yr,2025-06-25 14:45:23,False,,"About Us

Foodsmart is the leading telenutrition and foodcare solution, backed by a robust network of Registered Dietitians. Our platform is designed to foster healthier food choices, drive lasting behavior change, and deliver long-term health outcomes. Through our highly personalized, digital platform, we guide our 2.2 million members—including those in employer-sponsored health plans, regional and national Medicaid managed care organizations, Medicare Advantage plans, and commercial insurers—on a tailored journey to eating well while saving time and money.

Foodsmart seamlessly integrates dietary assessments and nutrition counseling with online food ordering and cost-effective meal planning for the entire family, optimizing ingredients both at home and on the go. We partner with national and regional retailers across the U.S., many of whom accept SNAP/EBT, making healthier food more accessible. Additionally, we assist members with SNAP enrollment and management, providing tangible access to nutritious food.In 2024, Foodsmart secured a $200 million investment from TPG’s Rise Fund, which supports entrepreneurs dedicated to achieving the United Nations’ Sustainable Development Goals. This investment will help us expand our reach, particularly to low-income workers who are disproportionately affected by diet-related diseases.

At Foodsmart, Our Mission Is To Make Nutritious Food Accessible And Affordable For Everyone, Regardless Of Economic Status. We Are Committed To a Set Of Core Values That Shape Our Culture And Work Environment

Measured: We make data-driven, truth-seeking decisions.

Impactful: We are fueled by achieving our mission and vision.

Collaborative: We help each other be better and create a positive environment.

Hungry: We maintain a healthy growth mindset, seeking to overcome challenges with courage.

Joyful: We take joy in each other, our work, and the privilege of doing this work.

Whether you're a dietitian, a commercial leader, or a technologist, working at Foodsmart means being part of a team that is passionate, supportive, and driven by a shared purpose. Join us in transforming the way people access and enjoy healthy food.

About The Role

The Data Engineer is a critical role responsible for constructing and optimizing our data pipeline architecture, collaborating closely with data scientists and analysts to facilitate data-related functionalities. The Data Engineer will be pivotal in designing, building, and maintaining highly scalable data pipelines, optimizing data delivery, and automating data processes. They will work closely with cross-functional teams to ensure efficient data flow and contribute to the success of our data-driven initiatives.

You Will

Own the optimization of data delivery for various cross-functional teams.Design, construct, install, test, and maintain highly scalable data pipelines.Collaborate closely with data architects, data scientists, and analysts to fulfill data requirements.Develop automated data processes for cleaning, validation, correction, and data mining.Identify, implement, and enhance internal process improvements, automating manual processes, and enhancing scalability.

You Are

Proactive and act as a driving force for efficient data delivery and infrastructure.Focused on quality and approach every data-related project with enthusiasm.Diligent in ensuring secure and compliant handling of data in accordance with relevant regulations.Collaborative and adept at addressing data-related technical issues and supporting stakeholders' data infrastructure needs.An expert in data warehouse architecture, data modeling, and automated data pipelines.

You Have

A minimum of 2 years of experience in a Data Engineering role.Hands-on experience with data warehouse solutions such as Snowflake or RedshiftExperience with cloud platforms such as AWS, GCP, or AzureAdvanced SQL knowledge and proficiency in working with relational databases.Familiarity with data pipeline and workflow management tools like Apache Airflow or Luigi.Strong analytical skills and the ability to thrive in a fast-paced environment.Familiarity with healthcare data standards like FHIR and HL7 is advantageous but not mandatory.Bachelor’s degree in Computer Science, Engineering, Mathematics, or related field; Master’s degree is a plus.

$130,000 - $154,999 a year

Role: Senior Data Engineer

Location: Remote

Base Salary Range: $130,000/yr to $155,000/yr + equity + benefits

Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries at our headquarters in San Francisco, California. Individual pay is determined by work location, job-related skills, experience, and relevant education or training.

About Our Benefits And Perks

Remote-First Company

Unlimited PTO

Healthcare Coverage (Medical, Dental, Vision)

401k, bonus, & stock options

Gym reimbursement

Foodsmart is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other protected class.",https://jobs.lever.co/foodsmart/3864c246-5eda-45e9-92e9-f6d804fa96b0/apply?source=LinkedIn
Kforce Inc,https://www.linkedin.com/company/kforce/life,Data Center Technical Operations Engineer II,https://www.linkedin.com/jobs/view/4256535236,4256535236,"Arlington, VA",On-site,$38/hr - $42/hr,2025-06-26 21:04:02,True,13 applicants,"Responsibilities

Kforce has an enterprise client seeking a Data Center Technical Operations Engineer II in Arlington, VA. Summary: The Data Center Technical Operations Engineer, Facility will be responsible for Data Center Engineering Operations within a Data Center including risk management and mitigation, corrective and preventative maintenance of critical infrastructure, vendor management and metric reporting. Responsibilities:

 Responsible for the on-site management of shift technicians, senior shift technicians, sub-contractors and vendors, ensuring that all work performed is in accordance with established practices and procedures Establish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance Work with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency Operate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc. May assist in the design and build out of new facilities May assist in projects to increase current facility efficiency Responsible for asset and inventory management Assist in recruiting efforts Deliver quality service and ensure all customer demands are met

Requirements

 Bachelor's degree or Technical (Military/Trade School) degree and relevant experience 2-4 years of relevant work experience 2-4 years of management experience Strong verbal and written communication skills Strong leadership and organizational skills Strong attention to detail Ability to prioritize in complex, fast-paced environment

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.

By clicking “Apply Today” you agree to receive calls, AI-generated calls, text messages or emails from Kforce and its affiliates, and service providers. Note that if you choose to communicate with Kforce via text messaging the frequency may vary, and message and data rates may apply. Carriers are not liable for delayed or undelivered messages. You will always have the right to cease communicating via text by using key words such as STOP.",
Lensa,https://www.linkedin.com/company/lensa/life,Principal Engineer Data Engineering - US Remote,https://www.linkedin.com/jobs/view/4256394169,4256394169,"Virginia Beach, VA",Remote,,2025-06-26 13:54:29,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Anywhere Real Estate.

Anywhere is at the forefront of driving the digital transformation and building best-in-class products that help our agents and brokers sell more homes, make more money, and work more efficiently.

Data & Analytics (DNA) is Anywhere's data arm. We create innovative analytics, data science, and robust data foundation capabilities to generate data-driven insights that serve the heart of Anywhere Advisor and Anywhere Brand business. Together with our business counterparts in the real estate business, we work daily to deliver differentiating insights (AI & BI) for Strategy and AA & AB Operations.

We're seeking a Principal Engineer to join our Data Platform Team. In this critical position, you'll be responsible for designing, implementing, and managing the data infrastructure. You will work closely with data scientists, software engineers, and other stakeholders to ensure the Data Platform's availability, usability, and integrity.

Data Infrastructure Design And Implementation

Evaluate, select, and implement new tools and frameworks to expand our data platform capabilities. Design, build, and maintain robust, scalable, and reliable data pipelines and ETL processes. Develop and maintain data infrastructure and platforms using various technologies (e.g., AWS, Snowflake Cloud Platforms, databases, Kafka streaming platforms). Ensure data quality, consistency, and integrity across the organization. Architect and optimize Data Ingestion and Snowflake ETLs. Production Support and enhancements to the observability of the Data Platform. 

Team Leadership And Mentorship

Lead and mentor data engineers, providing guidance and support to junior engineers. Foster a culture of technical excellence and continuous learning. Collaborate with other teams (e.g., data scientists, software engineers, and product managers) to ensure data solutions meet business needs. 

Data Security And Compliance

Implement and maintain data security measures to protect sensitive data. Ensure compliance with data protection regulations and industry standards. 

Problem Solving And Innovation

Identify and solve complex data-related problems. Stay abreast of industry trends and emerging technologies and identify opportunities to enhance data capabilities. Proactively address performance, scale, complexity, and security considerations. 

Skills And Qualifications

Technical Expertise:

10+ years’ experience with a strong understanding of data engineering principles and technologies. 10+ years’ experience with data pipelines, ETL processes, and data warehousing. 5+ years’ experience building data pipelines using Kafka, Kafka Connect, Airflow, and Snowflake. 5+ years’ experience with Snowflake Data Platform. 5+ years’ experience with AWS Data Services such as DMS, EMR, Glue, Athena, S3, CloudWatch, Lambda, or IAM. 5+ years’ experience with Data Quality, Data Reconciliation 5+ years’ experience managing production data platforms. 5+ years’ experience building observability (Monitoring & Alerting) using tools such as Data Dog and M. Proficiency in programming languages (e.g., Java, Python, SQL). Knowledge of data governance, data modeling, and security best practices. Proficiency in CI/CD, IAC, and Agile Development. 

Leadership And Communication

Strong leadership and mentoring skills. Excellent communication and collaboration skills. Ability to explain complex technical concepts to both technical and non-technical audiences. 

Problem-Solving And Analytical Skills

Ability to identify and solve complex problems. Strong analytical skills to identify data quality issues and performance bottlenecks. 

Anywhere Real Estate Inc. (http://www.anywhere.re/)   (NYSE: HOUS) is moving real estate to what's next. Home to some of the most recognized brands in real estate Better Homes and Gardens® Real Estate (https://www.bhgre.com/) , Century 21® (https://www.century21.com/) , Coldwell Banker® (https://www.coldwellbanker.com/) , Coldwell Banker Commercial® (https://www.cbcworldwide.com/) , Corcoran® (https://www.corcoran.com/) , ERA® (https://www.era.com/) , and Sotheby's International Realty® (https://www.sothebysrealty.com/eng) , we fulfill our purpose to empower everyone's next move through our leading integrated services, which include franchise, brokerage, relocation, and title and settlement businesses, as well as mortgage and title insurance underwriter minority owned joint ventures. Anywhere supports nearly 1 million home sale transactions annually and our portfolio of industry-leading brands turns houses into homes in more than 118 countries and territories across the world.

At Anywhere, we are empowering everyone’s next move – your career included. What differentiates us is our scale, expertise, network, and unique business model that positions us as a trusted advisor throughout every stage of the real estate transaction. We pursue talent – strategic thinkers who are eager to always find a better way, relentlessly focus on talent, obsess about growth, and achieve exceptional results. We value our people-first culture, which thrives on empowerment, innovation, and cross-company collaboration as we keep moving the world forward, together. Read more about our company culture and values in our annual Impact Report (https://anywhere.re/wp-content/uploads/2025/03/2024-Impact-Report.pdf) .

We are proud of our award-winning culture and are consistently recognized as an employer of choice by various organizations including:

Great Place to Work Forbes World's Best Employers Newsweek World's Most Trustworthy Companies Ethisphere World's Most Ethical Companies 

EEO Statement: EOE including disability/veteran

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/ee8e766148014973be31e3f4f7bef04atjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
TalentAlly,https://www.linkedin.com/company/talentallyatwork/life,Data Engineer,https://www.linkedin.com/jobs/view/4256523781,4256523781,"Richmond, VA",Hybrid,,2025-06-26 18:41:41,False,,"Senior Data Engineer

Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do:Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systemsUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and SnowflakeShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering communityCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowermentPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:Bachelor's DegreeAt least 3 years of experience in application development (Internship experience does not apply)At least 1 year of experience in big data technologies
Preferred Qualifications:5+ years of experience in application development including Python, SQL, Scala, or Java2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)2+ year experience working on real-time data and streaming applications2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of data warehousing experience (Redshift or Snowflake)3+ years of experience with UNIX/Linux including basic commands and shell scripting2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
McLean, VA: $158,600 - $181,000 for Senior Data Engineer
Richmond, VA: $144,200 - $164,600 for Senior Data Engineer









Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).PDN-9f3e70a9-dd5c-442b-8f31-a1258cff61d0",https://talentally.com/job/senior-data-engineer-3318
Lensa,https://www.linkedin.com/company/lensa/life,"Advisory Sr. Consultant, Cloud Data Engineer - Remote",https://www.linkedin.com/jobs/view/4256391447,4256391447,"Eden Prairie, MN",Remote,$89.8K/yr - $176.7K/yr,2025-06-26 13:54:41,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for UnitedHealth Group.

Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.

We are seeking a highly skilled and experienced Senior Cloud Data Engineer to join our team for a Cloud Data Modernization project. The successful candidate will be responsible for migrating our on-premises Enterprise Data Warehouse to a modern cloud-based data platform utilizing Azure Cloud data tools and Snowflake.

You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities

Lead the migration of the ETLs from on-premises database platform-based data warehouse to Azure Cloud and Snowflake Design, develop, and implement data platform solutions using Azure Data Factory (ADF), Self-hosted Integration Runtime (SHIR), Logic Apps, Azure Data Lake Storage Gen2 (ADLS Gen2), Blob Storage, and Snowflake Review and analyze existing on-premises ETL processes developed in SSIS and T-SQL Implement DevOps practices and CI/CD pipelines using GitActions Collaborate with cross-functional teams to ensure seamless integration and data flow Optimize and troubleshoot data pipelines and workflows Ensure data security and compliance with industry standards 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

5+ years of experience as a Cloud Data Engineer 3+ years of hands-on experience with Azure Cloud data tools (ADF, SHIR, LogicApps, ADLS Gen2, Blob Storage) and Snowflake 3+ years of experience in ETL development using on-premises databases and ETL technologies 3+ years of experience with Python or other scripting languages for data processing 2+ years of experience with development in Databricks for data engineering and analytics workloads Proficiency in DevOps and CI/CD practices using GitActions Experience with Agile methodologies Proven excellent problem-solving skills and ability to work independently Proven solid communication and collaboration skills Proven solid analytical skills and attention to detail Ability to adapt to new technologies and learn quickly Ability to travel up to 10% 

Preferred Qualifications

Certification in Azure or Snowflake Experience with data modeling and database design Proven knowledge of data governance and data quality best practices Familiarity with other cloud platforms (e.g., AWS, Google Cloud) All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

The salary range for this role is $89,800 to $176,700 annually based on full-time employment. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.

UnitedHealth Group is an Equal Employment Opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/c7b32c9ffeaa443eb72b15eb58ee267dtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Franklin Fitch,https://www.linkedin.com/company/franklin-fitch/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4258094751,4258094751,Greater Houston,On-site,$140K/yr - $150K/yr,2025-06-26 21:36:21,True,over 100 applicants,"Data Operations Engineer | $140K–$150K Base + 10–15% Bonus | Houston, TX (Onsite) | Private Sector
Are you a hands-on data engineer with a strong DBA background who’s ready to own the architecture and help a company modernize its entire data stack?
We’re hiring a Data Operations Engineer for a high-growth private company in Houston. The organization is sitting on rich operational data across multiple systems and is now investing heavily in centralizing, cleaning, and transforming that data into a modern, efficient, and scalable environment.This is a brand-new role with executive backing, a clear mandate, and minimal red tape. You’ll be the one to design, build, and maintain the future of data operations, from pipeline automation to warehousing strategy.
What You’ll Do:Manage and maintain SQL environments—performance tuning, backups, health checksBuild and own ETL pipelines using Python and Azure Data FactoryArchitect a modern data lake/data warehouse structure across multiple systemsCollaborate with internal development and analytics teams to ensure clean, reliable dataWork with Snowflake or similar platforms to deliver scalable solutionsBe the go-to engineer for all things data infrastructure and backend data workflows
What You’ll Need:7–10+ years of experience in data engineering and/or operational DBA rolesStrong T-SQL and SQL Server experience (including SSIS and routine maintenance)Python for automation, data workflows, scripting, and transformationAzure Data Factory or equivalent orchestration toolsSnowflake experience preferred (open to Databricks or similar alternatives)Experience in environments with lots of raw or time-series data is a plusA builder mindset—comfortable working independently, creatively, and iteratively

What You’ll Get:$140K–$150K Base SalaryBonus: 10–15%True architectural ownership with executive trustHands-on access to impactful systems and datasetsOnsite role in Houston, TX (no remote/hybrid—policy-based)Full benefits and strong comp package

This is the role for someone who’s ready to get under the hood, clean up legacy systems, and architect the future. If you’ve been waiting for a chance to lead without red tape - this is it.

#DataEngineer #HoustonJobs #SQLServer #ETL #AzureDataFactory #PythonJobs #DataOps #HiringNow #TechJobs #FranklinFitch",https://www.linkedin.com/job-apply/4258094751
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4258020371,4258020371,"Austin, TX",On-site,$220.7K/yr - $235.4K/yr,2025-06-26 14:10:31,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/bdc624d95ba142808994f1f19a57077ftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256390589,4256390589,"Montgomery, AL",Remote,$118K/yr - $122K/yr,2025-06-26 13:54:32,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/734f12a2bdca45ad8a57767da2e0911ftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256392360,4256392360,"Helena, MT",Remote,$118K/yr - $122K/yr,2025-06-26 13:54:40,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/797563421ffd400781500328c1e024aftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4258023003,4258023003,"Tallahassee, FL",On-site,$220.7K/yr - $235.4K/yr,2025-06-26 14:10:24,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/0d210b90b8a94c6c8e8564b3b9a6da98tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
ICF,https://www.linkedin.com/company/icf-international/life,Senior Data Engineer (remote),https://www.linkedin.com/jobs/view/4256531914,4256531914,"Reston, VA",Remote,$89.2K/yr - $151.6K/yr,2025-06-26 19:53:19,False,,"Description

Senior Data Engineer

The Company

ICF is a mission-driven company filled with people who care deeply about improving the lives of others and making the world a better place. Our core values include Embracing Difference; we seek candidates who are enthusiastic about building a culture that encourages, embraces, and hires dimensions of difference.

The Team

Our Health Engineering Solutions (HES) team works side by side with customers to articulate a vision for success, and then make it happen. We know success does not happen by accident. It takes the right team of people, working together on the right solutions for the customer. We are looking for a seasoned Senior Backend Engineer who will be a key driver to make this happen.

The Work

The ICF Senior Data Engineer will support ICF’s Health Engineering Solutions (HES) team in advancing key strategic initiatives for our CMS client. As a core member of the CMS Public Data Platform Team, this role will be instrumental in transforming complex data ecosystems into actionable insights that drive innovation, integration, and operational efficiency across content and data initiatives. This role is ideal for a technically skilled, innovative thinker who thrives on leveraging AI tools, low-code platforms, and custom coding to extract insights from large volumes of structured and unstructured data.

What You’ll Be Doing

Rapidly analyze and synthesize large-scale structured and unstructured datasets using a combination of AI tools, low-code platforms, and custom code.Design and implement data topologies and knowledge graphs that map relationships, hierarchies, and dependencies across disparate data sources.Perform data acquisition through web scraping, API integration, and ingestion of external knowledge bases.Identify patterns, overlaps, and redundancies across datasets to support data harmonization and enrichment.Develop and maintain scalable data pipelines and workflows that support advanced analytics and machine learning initiatives.Collaborate with data scientists, analysts, and domain experts to translate business questions into data-driven solutions.Ensure data quality, consistency, and governance across all stages of the data lifecycle.

Required Qualifications

Bachelor’s degree in Computer Science, Engineering, or a related technical field.5+ years of experience in data engineering, data integration, or related roles.3 + years of experience working with Python.1 years' experience with AI/ML tools and platforms (e.g., SageMaker, AzureML, VertexAI).1 years' experience with low-code/no-code platforms for data processing and automation.1 years' experience with knowledge graph construction, graph databases (e.g., Neo4j, RDF), and graph theory.Strong understanding of data modeling, data warehousing, and ETL/ELT processes.Excellent communication and collaboration skills.Team player with the ability to work in a fast-paced environment

Preferred Qualifications

Experience with Generative AI and LLMs in data transformation and enrichment workflows.Proficiency in web scraping, API development, and external data ingestion.Familiarity with semantic technologies (e.g., SPARQL, OWL, RDF).Experience with data visualization and storytelling.Demonstrated ability to work independently and prioritize tasks in a fast-paced environment.Strong critical thinking and problem-solving skills.

Working At ICF

ICF is a global advisory and technology services provider, but we are not your typical consultants. We combine unmatched expertise with innovative technology to help clients solve their most complex challenges, navigate change, and shape the future.

We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status.

Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.

Read more about workplace discrimination rights, the Pay Transparency Statement, or our benefit offerings which are included in the Transparency in (Benefits) Coverage Act.

Working at ICF

ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.

We can only solve the world's toughest challenges by building a workplace that allows everyone to thrive. We are an equal opportunity employer. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO policy.

Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation, please email Candidateaccommodation@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. 

Read more about workplace discrimination rights or our benefit offerings which are included in the Transparency in (Benefits) Coverage Act.

Candidate AI Usage Policy

At ICF, we are committed to ensuring a fair interview process for all candidates based on their own skills and knowledge. As part of this commitment, the use of artificial intelligence (AI) tools to generate or assist with responses during interviews (whether in-person or virtual) is not permitted. This policy is in place to maintain the integrity and authenticity of the interview process. 

However, we understand that some candidates may require accommodation that involves the use of AI. If such an accommodation is needed, candidates are instructed to contact us in advance at candidateaccommodation@icf.com. We are dedicated to providing the necessary support to ensure that all candidates have an equal opportunity to succeed.  

Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position.

The pay range for this position based on full-time employment is:

$89,203.00 - $151,646.00

Nationwide Remote Office (US99)",https://careers.icf.com/us/en/job/IIIIIIUSR2501743EXTERNALENUS/Senior-Data-Engineer-remote?utm_source=linkedin&utm_medium=phenom-feeds
AARATECH,https://www.linkedin.com/company/aaratechinc/life,Data Engineer,https://www.linkedin.com/jobs/view/4256516005,4256516005,"Washington, United States",On-site,$65K/yr - $80K/yr,2025-06-26 16:09:32,True,over 100 applicants,"💼 Data Engineer (3–4 Years Experience)Employment Type: Full-time (Contract or Contract-to-Hire)Experience Level: Mid-level (3–4 years)Company: Aaratech Inc

🛑 Eligibility: Open to U.S. Citizens and Green Card holders only. We do not offer visa sponsorship.
🔍 About Aaratech Inc
Aaratech Inc is a specialized IT consulting and staffing company that places elite engineering talent into high-impact roles at leading U.S. organizations. We focus on modern technologies across cloud, data, and software disciplines. Our client engagements offer long-term stability, competitive compensation, and the opportunity to work on cutting-edge data projects.
🎯 Position Overview
We are seeking a Data Engineer with 3–4 years of experience to join a client-facing role focused on building and maintaining scalable data pipelines, robust data models, and modern data warehousing solutions. You'll work with a variety of tools and frameworks, including Apache Spark, Snowflake, and Python, to deliver clean, reliable, and timely data for advanced analytics and reporting.
🛠️ Key Responsibilities
Design and develop scalable Data Pipelines to support batch and real-time processingImplement efficient Extract, Transform, Load (ETL) processes using tools like Apache Spark and dbtDevelop and optimize queries using SQL for data analysis and warehousingBuild and maintain Data Warehousing solutions using platforms like Snowflake or BigQueryCollaborate with business and technical teams to gather requirements and create accurate Data ModelsWrite reusable and maintainable code in Python (Programming Language) for data ingestion, processing, and automationEnsure end-to-end Data Processing integrity, scalability, and performanceFollow best practices for data governance, security, and compliance✅ Required Skills & Experience
3–4 years of experience in Data Engineering or a similar roleStrong proficiency in SQL and Python (Programming Language)Experience with Extract, Transform, Load (ETL) frameworks and building data pipelinesSolid understanding of Data Warehousing concepts and architectureHands-on experience with Snowflake, Apache Spark, or similar big data technologiesProven experience in Data Modeling and data schema designExposure to Data Processing frameworks and performance optimization techniquesFamiliarity with cloud platforms like AWS, GCP, or Azure⭐ Nice to Have
Experience with streaming data pipelines (e.g., Kafka, Kinesis)Exposure to CI/CD practices in data developmentPrior work in consulting or multi-client environmentsUnderstanding of data quality frameworks and monitoring strategies",https://www.linkedin.com/job-apply/4256516005
NBCUniversal,https://www.linkedin.com/company/nbcuniversal-inc-/life,Data Engineer II,https://www.linkedin.com/jobs/view/4256531446,4256531446,"Orlando, FL",Hybrid,$120K/yr - $140K/yr,2025-06-26 19:24:27,True,over 100 applicants,"Company Description

SPORTS NEXT

NBC Sports Next is where sports and technology intersect. We’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology. We’re a subdivision of NBC Sports and home to leading technology platforms and digital applications for Youth & Recreational Sports; Golf; and Emerging Media.

At NBC Sports Next, we equip more than 30MM players, coaches, athletes, sports administrators and fans in 40 countries with more than 25 sports solution products, including SportsEngine, the largest youth sports club, league and team management platform; SportsEngine Play, the first ever streaming service for youth and amateur sports, GolfNow, the leading online tee time marketplace and provider of golf course operations technology; and GolfPass the ultimate golf membership that connects golfers to exclusive content, tee time credits, instructional content and more.

FANDANGO

Over the past twenty years, Fandango has built a network of direct-to-consumer digital brands, where 50 million+ fans unite to celebrate their love for movies, TV and streaming. We take pride in serving fans throughout their entertainment journey from content discovery to theatrical moviegoing to watching at home. Our portfolio includes leading online movie ticketer, Fandango, which tickets for more than 31,000 U.S. movie screens; world-renowned entertainment review site, Rotten Tomatoes; and Fandango at Home (previously known as Vudu), the on-demand streaming service offering the industry’s best selection of 4K UHD titles and more than 250,000 new release and catalogue movies and next day TV shows.

Job Description

Fandango is looking for Data Engineer II. As a Data Engineer II, working on our data systems and services, you will work other data engineers to deliver projects and systems critical to Fandango's business. We expect our data engineers to be versatile, display leadership qualities and be enthusiastic about taking on new problems across our businesses as we continue to build great things for fellow movie fans.

Responsibilities

Contribute to and help team in design, build, testing, scaling, and maintaining data pipelines from a variety of source systems and streams (Internal, third party, cloud based, etc.), according to business and technical requirements.Continually work on improving the codebase and have active participation and oversight in all aspects of the team, including agile ceremonies.Collaborate with engineers and product managers to understand data needs.Participate in Agile ceremonies (standups, retros, sprint planning, etc.)Troubleshoot and fix production issues as they occur.

Qualifications

Bachelor's degree in Computer Science, Computer Engineering, or related technical field, or equivalent practical experience.5+ years of applied experience in Data Engineering, including but not limited to: building Data Pipelines, Orchestration, Data Modeling & Data Lake.Programming skills in one or more of the following: Python, Java, Scala, plus SQL and experience in writing reusable/efficient code to automate analysis and data processes.Experience in Data Warehousing (Relational and dimensional data modeling).Strong working experience with a variety of data sources such as APIs, real-time feeds, structured and semi-structured file formats.Experience with processing large datasets and building code using Glue, Apache Airflow / Amazon MWAA, SQL, Python, and pySpark.Experience of near Real Time & Batch Data Pipeline development.Experience with AWS data management (S3, Redshift, DynamoDB) and related tools (Athena, EMR, Glue, Lambda).Experience working in an agile/scrum environment.

Desired Characteristics

Familiar with software build, release, deployment and monitoring tools and practices.Experience with Talend, Informatica, or other off-the-shelf ETL tools.Experience working with Terraform.Strong Test-Driven Development background, with understanding of levels of testing required to continuously deliver value to production.Team-oriented and collaborative approach with a demonstrated aptitude, enthusiasm, and willingness to learn new methods, tools, practices, and skills.Ability to work effectively across functions, disciplines, and levels

Fully Remote: This position has been designated as fully remote, meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence.

Salary Range: $120,000-140,000

Additional Information

As part of our selection process, external candidates may be required to attend an in-person interview with an NBCUniversal employee at one of our locations prior to a hiring decision. NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law.

If you are a qualified individual with a disability or a disabled veteran and require support throughout the application and/or recruitment process as a result of your disability, you have the right to request a reasonable accommodation. You can submit your request to AccessibilitySupport@nbcuni.com.

Although you'll be hired as an NBCU employee, your employment and the responsibilities associated with this job likely will transition to Versant in the future. By joining at this pivotal time, you'll be a part of this exciting company as it takes shape.",https://www.linkedin.com/job-apply/4256531446
Infojini Inc,https://www.linkedin.com/company/infojini-inc/life,GCP Data Engineer (W2),https://www.linkedin.com/jobs/view/4258068298,4258068298,United States,Remote,,2025-06-26 20:02:50,True,over 100 applicants,"Role- Data EngineerLocation-RemoteDuration-6 Months

We are seeking a hands-on Data Engineer to join our team and contribute to critical backend components that are being migrated to a modern cloud environment (AWS or GCP). This role involves close collaboration with the architecture team to design and develop scalable, high-performance data pipelines and backend services to support advanced analytics and data science initiatives.
Key Responsibilities:Support cloud migration of backend and data components (AWS/GCP)Collaborate with architects to design robust systems and integrationsDesign, develop, and maintain data pipelines and ELT workflowsImplement solutions enabling Data Scientists to access, transform, and analyze dataWrite clean, efficient code in Python, with occasional Java involvementWork with SQL/NoSQL databases and build scalable data modelsEngage with CI/CD tools, monitoring systems, and infrastructureEnsure reliability and performance in production environmentsTroubleshoot and debug data pipeline and infrastructure issuesTranslate business needs into reliable datasets and modelsFollow best practices in modern data engineering and cloud development
Must-Have Qualifications:3+ years of experience in data engineering or backend developmentStrong hands-on coding experience in PythonExperience with cloud platforms – GCP preferred, AWS acceptableHands-on experience with BigQueryStrong SQL skills; experience with NoSQL databasesFamiliarity with distributed data processing tools like Apache SparkExperience integrating with backend/data servicesSolid understanding of CI/CD, version control (Git), and testing practicesStrong analytical, communication, and time management skills
Nice-to-Have Skills:Experience with DBT (Data Build Tool) or BigQuery DataformFamiliarity with Java (basic to intermediate level)Experience with Snowflake, Dremio, or similar data platformsKnowledge of Apache Iceberg, Delta Lake, or open table formatsFamiliarity with orchestration tools like Airflow, Prefect, or DagsterExposure to real-time data tools (e.g., Kafka, Spark Streaming)Experience with monitoring and logging tools in cloud environments",https://www.linkedin.com/job-apply/4258068298
SpawGlass,https://www.linkedin.com/company/spawglass/life,Data Engineer,https://www.linkedin.com/jobs/view/4258062937,4258062937,"Houston, TX",On-site,,2025-06-26 20:01:17,True,over 100 applicants,"As our Data Engineer, you will take ownership of designing, building, and maintaining the data infrastructure that powers business insights and operational reporting. You’ll develop robust data pipelines, integrate information from multiple systems, and ensure data is accurate, accessible, and well-structured to support analytics and decision-making. Your work will reflect the unique needs of the construction industry—supporting job cost tracking, schedule performance, workforce data, and subcontractor activity. To be successful in this role, you should be a clear communicator, resourceful problem-solver, and skilled at bridging the gap between data and construction operations.

What You'll Do

Data Architecture & Infrastructure: Design and manage the technology stack used for data storage and processing, including databases, pipelines, data warehouses, data lakes, and APIs, to support scalable and efficient data operationsData Modeling & Documentation: Create and maintain data models, flow diagrams, mappings, and dictionaries to support clear, consistent, and accessible data structures across the organizationData Quality & Governance: Ensure data accuracy, integrity, and availability by identifying and resolving quality issues, collaborating with security teams, and recommending improvements to enhance governance and complianceBusiness Intelligence & Reporting: Translate business needs into actionable data solutions by designing dashboards, reports, and downstream data feeds that drive informed decision-makingData Integration & Interoperability: Support seamless data movement and transformation across systems by enabling integration between applications and ensuring interoperability within data warehouse and analytics environments




What You Bring To The Team

Prior experience in data engineering, including system integrations, API development, data management, and Microsoft SQL ServerBachelor’s degree in Computer Science, Data Science, Information Systems, or a related fieldRelevant certifications such as MTA: SQL Server, MCSA: Database Development, Database Administration, or Business Intelligence Development




Essential Functions

Ability to lift up to 25 pounds unassistedAbility to sit for extended periods, with occasional walking and standing as neededMust be able to operate standard office equipment (e.g., computers, printers, copiers)Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of the position




Work Environment

Corporate office environment with a business casual dress code; occasional business professional attire may be requiredOccasional visits to jobsites, requiring proper use of Personal Protective Equipment (PPE) and exposure to outdoor elements and typical construction site conditionsStandard work hours are Monday–Friday, 8:00 AM to 5:00 PM




SpawGlass is an Equal Opportunity Employer.",https://www.linkedin.com/job-apply/4258062937
Zeektek,https://www.linkedin.com/company/zeektek/life,Senior Data Engineer - Talend - Mongo - Snowflake - AWS/GitLab 100% Remote Contract Job ,https://www.linkedin.com/jobs/view/4256523077,4256523077,United States,Remote,Up to $61/hr,2025-06-26 17:37:08,True,over 100 applicants,"We are seeking a highly skilled Senior Data Engineer with 4–6 years of experience to support our Risk Adjustment Analytics team on a 3+ month contract. This role focuses on end-to-end data pipeline development, from ingestion to transformation and deployment, with a strong emphasis on Talend ETL development. The ideal candidate will have expertise in MongoDB, Snowflake, and AWS technologies, and demonstrate hands-on experience building and deploying solutions using GitLab, Jenkins, ECS, and Kubernetes. This is a fully remote role, contributing to high-impact projects like WHAMM (Whole Health Actionable Member Metrics), where you will collaborate closely with cross-functional teams to drive actionable data insights. Strong Python skills and familiarity with cloud infrastructure and DevOps best practices are essential.
100% Remote.
MUST HAVES:4-6 years experienceTalendMongoDB, SnowflakeAWS, GitLab-------------------------------------Strong ETL development experience using MongoDB, Python.Strong application development experience with Informatica IDMC, Snowflake and AWS S3 bucket.Development experience building DevOps automation pipelines in GitLab and Jenkins.Deployment experience with Amazon ECS and Kubernetes.Experience architecting and developing applications on MongoDB, Snowflake and AWS.Development experience with data intensive AWS Lambda functions, deployment functions to receive and process events from S3 buckets. · Exposure to Snowpipe and Snowflake integration patterns.
NICE TO HAVES:Teradata/SQL ServerSSISReact.js, Node.js, Angular.jsjQuery",https://www.linkedin.com/job-apply/4256523077
Henry Schein One,https://www.linkedin.com/company/henry-schein-one/life,Sr Engineer (Data Platform),https://www.linkedin.com/jobs/view/4254881844,4254881844,United States,Remote,$85K/yr - $135K/yr,2025-06-26 15:12:13,False,,"This position will be remote within the United States.

Job Summary

You will be a key contributor to our Data Platform team, driving impactful solutions to meet business and product goals. You will lead the development and optimization of our Data Platform APIs, ensuring seamless integration, performance, and scalability. By collaborating closely with cross-functional teams and providing architectural guidance, you will help shape our data infrastructure to support evolving healthcare and dental data needs, empowering data-driven decision-making across the organization.

What You Will Do

Data Platform Development: Lead the design, development, and maintenance of the Data Platform APIs, ensuring efficient and scalable solutions.Pipeline Optimization: Continuously monitor, optimize, and improve data pipelines to ensure reliability, performance, and scalability.Collaboration & Best Practices: Work closely with programming and product teams, ensuring that best practices are followed across the development lifecycle.Tool Management: Oversee development tools, including their maintenance, licensing, and team training to ensure they are effectively used and up-to-date.Mentorship: Guide and mentor junior engineers on best practices across multiple programming languages, helping to upskill the team and foster a collaborative environment.Documentation & Support: Assist in creating clear user documentation and offer technical support by providing accurate, concise information about software functionality.Technical Presentations: Communicate technical concepts effectively to senior management, presenting complex ideas in a clear, understandable manner.SDLC Participation: Actively engage in all relevant Software Development Lifecycle (SDLC) meetings, ensuring timely and seamless product delivery.


Travel/Physical Demands

Travel typically less than 10%Office environment with no special physical demands required


Technologies We Use

Big Data: Spark, Flink, Notebooks (Jupyter and Zepplin)Languages & Frameworks: Python, Java, Spring, JavaScript, Node.JSData Streaming: Kafka, Avro, ParquetDatabases: PostgreSQL, SQL databasesContainerization: Docker


Qualifications

What You Will Have

4+ years of engineering experience with a focus on the HS1 tech stack, particularly in data engineering and data-streamingAPI Expertise: Broad understanding of API concepts and best practicesHealthcare Data Experience: Familiarity with Electronic Health Record (EHR) formats, including X12, HL7, FHIR, and industry-specific models (Dental, Orthodontic, and Dental Support Organization data models)Excellent knowledge of database schema modeling toolsDatabase Knowledge: Understanding of database schema modeling tools and enterprise application design patternsTechnical Design & Implementation: Ability to derive and design technical specifications from product requirements and implement code based on those specificationsCommunication Skills: Excellent written and verbal communication skills, with the ability to explain complex concepts to technical and non-technical teamsProblem-Solving Skills: Excellent analytical, problem-solving, and organizational abilities


Nice to Haves

Bachelor’s Degree in Computer Science or a related fieldExperience with API gateways such as Apigee


The posted range for this position is $85,000 - $135,000 which is the expected starting base salary range for an employee who is new to the role to fully proficient in the role. Many factors go into determining employee pay within the posted range including prior experience, current skills, location/labor market, internal equity, etc. This position is eligible for a bonus target not reflected in the range.

What You Get As a Henry Schein One Employee

A great place to work with fantastic peopleA career in the healthcare technology industry, with the ability to grow and realize your full potentialCompetitive compensationExcellent benefits package! Medical, Dental and Vision Coverage, 401K Plan with Company Match, Paid Time Off (PTO), Sick Leave (if applicable), Paid Parental Leave, Short Term Disability, Income Protection, Work Life Assistance Program, Health Savings and Flexible Spending Accounts, Education Benefits, Worldwide Scholarship Program, Volunteer Opportunities, and more


About Henry Schein One

Henry Schein One is the global leader in dental management, analytics, communication, and marketing software. Our company’s products and services work together as one simple solution to provide users with a seamless and integrated experience.

Our company thrives because of our people. We believe in a supportive, diverse workforce, inclusive environments, professional development opportunities, and competitive compensation packages. We value innovation, and teamwork, and encourage work-life balance.

One of many reasons Henry Schein One leads the industry is our products, services, and, most importantly, our people.

In 2022, Henry Schein One was named one of Best Companies to Work for in Utah. Click here for more information: 2022 Best Companies to Work For | Henry Schein One

Henry Schein, Inc. is an Equal Employment Opportunity Employer and does not discriminate against applicants or employees on the basis of race, color, religion, creed, national origin, ancestry, disability that can be reasonably accommodated without undue hardship, sex, sexual orientation, gender identity, age, citizenship, marital or veteran status, or any other legally protected status.

Unfortunately, Henry Schein One is not currently hiring individuals residing in Alaska, Delaware, Hawaii, Iowa, Louisiana, Maryland, Nebraska, North Dakota, Rhode Island, South Dakota, Vermont, West Virginia, Washington DC, Puerto Rico, or other US Territories at this time.",http://henryscheinone.applytojob.com/apply/UsPe3XYnj1?source=LinkedIn
Claritev,https://www.linkedin.com/company/claritev/life,Data Engineer (remote),https://www.linkedin.com/jobs/view/4245580105,4245580105,"Naperville, IL",Remote,$80K/yr - $100K/yr,2025-06-05 21:46:26,False,,"At Claritev, we pride ourselves on being a dynamic team of innovative professionals. Our purpose is simple - we strive to bend the cost curve in healthcare for all. Our dedication to service excellence extends to all our stakeholders - internal and external - driving us to consistently exceed expectations. We are intentionally bold, we foster innovation, we nurture accountability, we champion diversity, and empower each other to illuminate our collective potential.

Be part of our amazing transformational journey as we optimize the opportunity towards becoming a leading technology, data, and innovation voice in healthcare. Onward and Upward!!!

Job Roles And Responsibilities

 Understand business processes and how they are modeled in various systems Work with business users, technology teams, and executives to understand their data needs to create innovative solutions to fulfil them Implement data structures, workflows, and integrations between enterprise platforms to ensure the accurate and timely execution of business processes. Maintain scalable data pipelines to support continuing increases in data volume and complexity. Adhere to established best practices on data integration/engineering, as well as the future of our data infrastructure Managing and improving the performance of our database, queries, tools, and solutions Creating and maintaining data warehouse, databases, tables, SQL queries, and ingestion pipelines to power report, dashboards, predictive models, and downstream analysis Writing complex and efficient queries to transform raw data sources into easily accessible models for our teams and reporting platforms Prepare data for predictive and prescriptive modeling Identify and analyze data patterns Identify ways to improve data reliability, efficiency and quality Work with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure Collaborate, coordinate, and communicate across disciplines and departments. Ensure compliance with HIPAA regulations and requirements. Demonstrate Company's Core Competencies and values held within. Please note due to the exposure of PHI sensitive data -- this role is considered to be a High Risk and priveleged Role. The position responsibilities outlined above are in no way to be construed as all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.

,

JOB REQUIREMENTS (Education, Experience, And Training)

 Minimum high school diploma and four (4) years' related experience, three (3) of which should be inclusive of experience with OOP, SQL, schema designing, data modeling, designing, building, and maintaining data processing systems. Bachelors' degree in computer science, information technology or a similarly relevant field is highly preferred. Experience with advanced analytics tools with Python and PySpark. Experience using SQL, SPARK, and Azure Data Factory (ADF). Experience in triaging data issues, analyzing end-to-end data pipelines and working with business users in resolving issues. Experience in working with data governance/data quality and data security teams and specifically data stewards and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification. Experience with Databricks and SSIS are nice to have. Exposure to Big Data Development using Hive, Impala, Spark, and familiarity with Kafka Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics Excellent communication skills (verbal, listening and written) Ability to build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. Ability to work with both IT and business in integrating analytics and data science output into business processes and workflows. An agile learner who brings strong problem-solving skills and enjoys working as part of a technical, cross functional team to solve complex data problems Able to prioritize and manage multiple projects and requests at any one time Strong attention to detail when identifying data relationships, trends, and anomalies. Thinking through long-term impacts of key design decisions and handling failure scenarios. Ability to effectively share technical information, communicate technical issues and solutions to all levels of business Ability to meet strict deadlines, work on multiple tasks and work well under pressure

Compensation

The salary range for this position is $80-100k. Specific offers take into account a candidate’s education, experience and skills, as well as the candidate’s work location and internal equity. This position is also eligible for health insurance, 401k and bonus opportunity.

Benefits

We realize that our employees are instrumental to our success, and we reward them accordingly with very competitive compensation and benefits packages, an incentive bonus program, as well as recognition and awards programs. Our work environment is friendly and supportive, and we offer flexible schedules whenever possible, as well as a wide range of live and web-based professional development and educational programs to prepare you for advancement opportunities.

Your Benefits Will Include 

Medical, dental and vision coverage with low deductible & copayLife insurance Short and long-term disabilityPaid Parental Leave401(k) + matchEmployee Stock Purchase PlanGenerous Paid Time Off - accrued based on years of serviceWA Candidates: the accrual rate is 4.61 hours every other week for the first two years of tenure before increasing with additional years of service10 paid company holidaysTuition reimbursementFlexible Spending AccountEmployee Assistance ProgramSick time benefits - for eligible employees, one hour of sick time for every 30 hours worked, up to a maximum accrual of 40 hours per calendar year, unless the laws of the state in which the employee is located provide for more generous sick time benefits 
EEO STATEMENT

Claritev is an Equal Opportunity Employer and complies with all applicable laws and regulations. Qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability or protected veteran status. If you would like more information on your EEO rights under the law, please click here.

APPLICATION DEADLINE

We will generally accept applications for at least 15 calendar days from the posting date or as long as the job remains posted.

",https://recruiting.adp.com/srccar/public/RTI.home?r=5001125594906&c=1204301&d=External&rb=LinkedIn
Optomi,https://www.linkedin.com/company/optomi/life,SAP Data Engineer,https://www.linkedin.com/jobs/view/4256549264,4256549264,"New York, United States",Remote,$70/hr - $80/hr,2025-06-26 20:23:42,True,over 100 applicants,"SAP Data Engineer - BW/S4 HANA100% RemoteContract or Contract to HireUSC or GC Holder only - no C2C or sponsorship
Optomi, in partnership with a leader in the family entertainment industry, is seeking a Senior SAP Data Engineer to join their growing team! This team supports a modernization project and working with multiple SAP tools/technologies such as BW/S4 HANA, SAC, ABAP, BPC, etc. 
Develop, maintain, and optimize data models and ETL pipelines across SAP BW, S/4HANA, and BPC systems.Design and implement BW/4HANA and native HANA models including ADSOs, CompositeProviders, and Calculation Views.Build SAC stories and dashboards by integrating data from BW, S/4HANA, and external sources.Perform custom ABAP development and enhancements related to BW data extraction, transformations, and performance tuning.Support SAP BPC (Standard and Embedded) configuration, data loading, planning functions, and reporting.Utilize CDS Views and ODP-based extractors for real-time and batch data provisioning from S/4HANA.",https://www.linkedin.com/job-apply/4256549264
Insight Global,https://www.linkedin.com/company/insight-global/life,"Data Engineer (GCP, SQL, BI Reporting, PySpark)",https://www.linkedin.com/jobs/view/4256539489,4256539489,"San Jose, CA",Hybrid,$55/hr - $62/hr,2025-06-26 19:49:15,True,over 100 applicants,"FINTECH Company - W2 Contract 6 months w/ extension Data Engineer - Sunnyvale, Ca Pay Rate: $55hr-$62hr
*7+ years of experience as a Data Analyst or Data Engineer *Must have worked with (GCP, SQL, BI Reporting, PySpark)*Big tech background 
Job Description:We are seeking a talented and motivated Data Engineer to join our dynamic team in San Jose, CA. As a Data Engineer, you will play a crucial role in designing, building, and maintaining our data infrastructure to support our innovative financial technology solutions.

Key Responsibilities:Data Pipeline Development: Design, develop, and maintain scalable data pipelines to process and analyze large datasets.Data Integration: Integrate data from various sources, ensuring data quality and consistency.Database Management: Optimize and manage databases, ensuring high performance and availability.ETL Processes: Develop and maintain ETL (Extract, Transform, Load) processes to support data warehousing and analytics.Collaboration: Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.Data Security: Implement and maintain data security measures to protect sensitive information.Documentation: Create and maintain comprehensive documentation for data processes and systems.
Qualifications:Education: Bachelor’s degree in Computer Science, Engineering, or a related field.Experience: 7+ years of experience in data engineering or a related role.Technical Skills:Proficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL).Strong programming skills in Python or Java.Experience with big data technologies (e.g., Hadoop, Spark).Familiarity with cloud platforms (e.g., AWS, GCP, Azure).Knowledge of data warehousing concepts and tools (e.g., Snowflake, Redshift).Soft Skills:Excellent problem-solving and analytical skills.Strong communication and collaboration abilities.Ability to work in a fast-paced, dynamic environment.

Benefits:Comprehensive health, dental, and vision insurance.401(k) plan with company match.Flexible work hours and remote work options.Professional development opportunities.",https://www.linkedin.com/job-apply/4256539489
Insight Global,https://www.linkedin.com/company/insight-global/life,Database Engineer,https://www.linkedin.com/jobs/view/4256507842,4256507842,"Ashburn, VA",Hybrid,$95K/yr - $115K/yr,2025-06-26 15:39:08,True,over 100 applicants,"Job Summary:We are seeking a skilled and detail-oriented Database Engineer with strong experience in PostgreSQL to join our growing team. The Database Engineer is responsible for the performance, integrity, and security of our team’s multiple databases. They will be involved in the planning and development of the database, as well as troubleshooting any issues on behalf of the users. They will play a crucial role in managing data, ensuring it remains consistent, is clearly defined, and is easily accessible. 
Key Responsibilities:Database Design and Implementation: Assist in the design, implementation, and management of databases, ensuring they are robust, scalable, and efficient. Performance Tuning: Monitor database performance, implement changes, and apply new patches and versions when required. Backup and Recovery: Develop, implement, and oversee database backup, recovery, and security strategies. Security Management: Manage user access, ensure data integrity and security, and implement encryption techniques to protect sensitive data. Troubleshooting: Diagnose and troubleshoot database errors and other related issues promptly. Documentation: Maintain detailed documentation of database configurations, procedures, and processes. Collaboration: Work closely with developers, system administrators, and other stakeholders to ensure database integrity and performance. Data Integration: Ensure seamless integration of new data with existing databases and support ETL (Extract, Transform, Load) processes. Compliance: Ensure databases comply with relevant data protection regulations and organizational policies. This position may require occasional after-hours work for maintenance and troubleshooting. The role may include on-call responsibilities as part of a team rotation. 
Required Qualifications:Education: Bachelor’s degree Experience: 3-5 years of experience in database design, database development, and management. Proficiency in Postgres",https://www.linkedin.com/job-apply/4256507842
Golden Technology,https://www.linkedin.com/company/golden-technology-inc/life,Azure Data Engineer,https://www.linkedin.com/jobs/view/4256560500,4256560500,United States,Remote,,2025-06-26 22:51:11,True,over 100 applicants,"Are you passionate about building robust, scalable data pipelines in the cloud? Do you enjoy solving complex data problems and collaborating with cross-functional teams? If so, we want to hear from you!We're looking for a Senior Azure Data Engineer to join our growing data team. In this role, you'll play a pivotal part in designing and optimizing end-to-end data solutions that power decision-making across the business.🔧 What You’ll Be Doing:Design and develop scalable data pipelines and ETL processes using Azure Data Factory, Databricks, and PySparkArchitect modern cloud data platforms using Azure Synapse Analytics, Data Lake Storage, and Azure SQLCreate clean, reusable datasets from both structured and unstructured sources for advanced analytics and BIDevelop and optimize complex SQL queries and stored proceduresMonitor, troubleshoot, and enhance data workflows for performance and reliabilityUphold standards for data quality, governance, and securityPartner with data scientists and analysts to support ML/AI use casesShare knowledge and contribute to best practices across the team✅ What You Bring:5+ years of experience in data engineering or related roles, with 2+ years focused on AzureHands-on expertise in Azure Data Factory, Databricks, Synapse, and Data LakeStrong skills in Python and PySparkDeep understanding of SQL and data modelingKnowledge of cloud architecture, data warehousing, and distributed systemsExperience with CI/CD, DevOps, and version control tools like Git (a plus!)Excellent communication and collaboration abilities💡 Why Join Us?Work with a forward-thinking team using the latest Azure technologiesTackle meaningful data challenges that drive real impactEnjoy a supportive culture where your ideas are heard[Add perks like remote flexibility, professional development, etc.]🔗 Ready to level up your data career? Apply now or connect with us to learn more!#Azure #DataEngineering #DataJobs #PySpark #Databricks #DataFactory #Synapse #SQL #Python #CloudCareers #WeAreHiring",https://www.linkedin.com/job-apply/4256560500
Lensa,https://www.linkedin.com/company/lensa/life,Associate Data Engineer,https://www.linkedin.com/jobs/view/4258018647,4258018647,United States,Remote,$40K/yr - $45K/yr,2025-06-26 14:10:17,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Lumerate.

Lumerate is growing rapidly, and we're searching for a UK-based Associate Data Engineer to join our team for the journey!

Who is Lumerate?

Lumerate is a Toronto-based SaaS company that has built game-changing technology to help sales teams accelerate revenue growth. We help our customers achieve the full picture of their industries. We also strive to achieve our own personal full pictures from a career fulfillment and learning perspective. We're in the business of gathering intelligence about industries and delivering it to the right people within those industries through innovative software interfaces. Our vision is to be the world's most useful and trusted source of information for professionals seeking to understand what's happening in their industry.

Our mission: To deliver industry awareness to an ever-increasing number of people, in whatever way helps them to make the most informed decisions, take the most immediate action and be the most awesome at their unique jobs.

What The Role Looks Like

Working closely with the Data Engineer, you will work on a variety of tasks and projects mainly to support the appropriate data team. Some of these projects may include: Automation of manual tasks; extracting data from various sources such as databases and web scraping; evaluating and setting up new data pipelines; data point retrieval experiments and analysis; working with LLMs for inclusion in current and future workflows. This role will really suit someone who enjoys building processes. Due to the data you will be working with, a background in life sciences would be helpful. Python will be the main programming language used in this role.

What You'll Be Responsible For

Collaborating effectively with data team members and business stakeholders to ensure alignment and clear communication. Supporting definition of detailed requirements for projects with appropriate stakeholders. Supporting solution design based on requirements. Developing and maintaining processes primarily using Python. Ensuring appropriate testing has been conducted. Creating detailed documentation to ensure processes can be easily followed (e.g. User guides) Participating in regular meetings. 

Who will be successful in this role?‍

Someone who holds a minimum of a Bachelor's degree in a scientific or technological field, or possesses equivalent industry experience. Experience with at least Python and associated libraries such as Pandas, requests, and selenium. Experience with spreadsheets and databases. A person with strong analytical and problem-solving skills that genuinely enjoys tackling complex challenges. Someone who demonstrates an ability to quickly learn complex concepts and understand systems. Someone who is able to communicate complex data/ processes to a non technical audience. A person who is able to work well remotely, previous experience of this would be preferable. Someone who is detail oriented. You'll see this request to add the name of the establishment you find at these coordinates in your cover letter 43.66484814697388, -79.45482568477735 

What We'd Love To See In Your Work History

Work experience at a software/ technology company AND/OR background in a life sciences data related role. Experience in automating manual processes. Experience using APIs, LLMs, scraping web content and/or version control software would be helpful. Experience writing Standard Operating Procedures (SOPs) and/or user/resource documentation. Experience managing multiple projects or competing priorities simultaneously. 

Reasons You Might Want To Apply

You'll work with a team who encourages continuous learning and supports your growth. You'll gain exposure to many different projects and work collaboratively with colleagues. You'll build foundational skills essential for a successful career path in Data Engineering. 

Why Lumerate? Fancy perks etc.

Grow with an experienced team with skills in machine learning, development, business and organizational culture Earn yourself some equity (employee options make up 20% of the value of the company at all times) Join us for our annual all-company retreat when we reach our goals (past destinations include Bermuda, Iceland, Costa Rica, Portugal and Dominican Republic) 4 weeks paid annual leave + all local Bank Holidays (paid) Earn additional paid vacation days with continued learning ($1000 annual stipend for courses and classes) Take part in our Employee Giving Program (you choose the causes and the company provides the funds) Basic and extended health and dental benefits Paid and topped-up maternal and parental leave 

Salary: 40,000-45,000 GBP

Start Date: July 2025 (flexible based on the successful candidate)

Location: This is a remote position. Our new Associate Data Engineer can live and work anywhere in the United Kingdom, provided they are legally entitled to do so.

Already picturing your first day as Lumerate's first Associate Data Engineer? Apply today by submitting your cover letter and resume. While we thank all candidates for their interest. Please note that applications without a cover letter will not be considered.

At Lumerate we celebrate diverse backgrounds, experiences, and perspectives. We are passionate about fostering an inclusive environment where everyone feels empowered to bring their authentic selves to work. Lumerate is an equal-opportunity employer, and we are committed to working with applicants requesting accommodation at any stage of the hiring process.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/70a53fee35604546a2ff2582a3ace604tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256388522,4256388522,"Washington, DC",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:47,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/9acddb1ee0854e6888c8b4e58f30858ctjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
STAFFWORXS,https://www.linkedin.com/company/staffworxs/life,Senior Data Engineer – Databricks & PySpark Expert,https://www.linkedin.com/jobs/view/4258044824,4258044824,"Irving, TX",Hybrid,$50/hr - $55/hr,2025-06-26 15:47:40,True,over 100 applicants,"🚀 Hiring Now: Senior Data Engineer – Databricks & PySpark Expert📍 Location: Irving, TX (Hybrid – 3 days onsite)⏳ Duration: 6+ Months Contract🎯 Interview: In-Person (Single Round)We’re seeking an experienced Data Engineer who excels in Databricks, PySpark, and big data technologies. This is a hands-on role for someone who can design and deliver scalable data solutions and enjoys working in a fast-paced Agile environment.🔍 Key Responsibilities:Design, develop, and test scalable data solutions.Work on large and complex stories across multiple technology stacks.Collaborate in requirement gathering, design sessions, and sprint ceremonies.Write clean, maintainable code and conduct thorough code reviews.Ensure timely delivery of project milestones.Promote coding best practices and participate in knowledge-sharing sessions.Develop solutions with millions of records and optimize data pipelines.Contribute to CI/CD pipeline development and infrastructure automation.✅ Must-Have Qualifications:8+ years of hands-on experience in Python, MS SQL Server, and T-SQL.Strong experience with Databricks, PySpark, and Azure Data Factory.Proficiency with big data tech: Hadoop, Spark, Kafka.Solid understanding of data lake/delta lake architectures.Experience with CI/CD pipelines and Infrastructure as Code (Terraform, Pulumi).Agile development background with strong communication skills.➕ Nice to Have:Exposure to or knowledge of .NET technologies.📩 Ready to build enterprise-grade data pipelines?Apply now or DM for more details!#DataEngineer #Databricks #PySpark #Azure #BigData #SQL #DataLake #Kafka #Spark #HiringNow #IrvingJobs #HybridJobs #ContractJobs",https://www.linkedin.com/job-apply/4258044824
Lensa,https://www.linkedin.com/company/lensa/life,Snowflake Data Engineer (Remote),https://www.linkedin.com/jobs/view/4256391647,4256391647,United States,Remote,,2025-06-26 13:54:28,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for NTT America.

Req ID: 327683

NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.

We are currently seeking a Snowflake Data Engineer (Remote) to join our team in Phoenix, Arizona (US-AZ), United States (US).

Role Overview

We are seeking a Snowflake Data Engineer with strong experience working on large EDW programs. This role requires expertise in Snowflake, Informatica, Control-M and github. This role will be responsible for building scalable EDW solutions.

Key Responsibilities

Develop Snowflake data pipelines for migration of data from one EDW to another EDW Design and implement datawarehouse models required to support the project requirements. Execute data migration scripts and Informatica ETL jobs to migrate data for various enviroments. Schedule jobs using Control-M Support end to end testing activities. Maintain solution versioning using Github Deploy solution in production using approved release proceedures. 

Qualifications & Skills

5+ years of Snowflake developer experience specifically working on large EDW programs 3+ years of Snowflake query tuning, ETL, external file handling, CTM skills. 2+ years of experience building and executing Informatica ETL jobs. Knowledge of Control-M and Github Must be US Citizen due to data restrictions 

Preferred Skills

Understanding of Agile methodologies. Knowledge of Salesforce and SAP data Problem-Solving: Strong analytical and troubleshooting skills. 

About NTT DATA

NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com (http://us.nttdata.com/en)

NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here (http://us.nttdata.com/en/compliance#eeos) . If you'd like more information on your EEO rights under the law, please click here (http://us.nttdata.com/en/compliance#know-your-rights) . For Pay Transparency information, please click here (http://us.nttdata.com/en/compliance#ppnp) .

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/547c40b81b894c1f9ef41b90160ad81etjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Sword Health,https://www.linkedin.com/company/swordhealth/life,Senior Data Engineer - Predict,https://www.linkedin.com/jobs/view/4258008984,4258008984,United States,Remote,$138.8K/yr - $218.2K/yr,2025-06-26 13:40:48,False,,"Sword Health is on a mission to free two billion people from pain.

With 67% of members achieving a pain-free life and a 70% reduction in surgery intent, at Sword, we are using AI Care to change lives, and save millions for our 25,000+ enterprise clients across three continents. Today, we hold the majority of industry patents, win 70% of competitive evaluations, and have raised more than $300 million from top venture firms like Founders Fund, Sapphire Ventures, General Catalyst, and Khosla Ventures.

Recognized as a Forbes Best Startup Employer in 2025, this award highlights our focus on being a destination for the best and brightest talent. Not only have we experienced unprecedented growth since our market debut in 2020, but we’ve also created a remarkable mission and value-driven environment that is loved by our growing team. With a recent valuation of $3 billion, we are in a phase of hyper growth and expansion, and we’re looking for individuals with passion, commitment, and energy to help us scale our global impact.

Joining Sword means committing to a set of core values, chief amongst them to “do it for the patients” every day, and to always “deliver more than expected” on behalf of our members and clients.

This is an opportunity for you to make a significant difference on a massive scale as you work alongside 900+ (and growing!) talented colleagues, spanning three continents. Your charge? To help us build a pain-free world, powered by AI, enhanced by people — accessible to all.

A Senior Data Engineer on the Predict team will lead all data initiatives while collaborating closely with both the broader Predict team and the wider data engineering organization.

At Sword Predict, we’re building a platform that does more than just forecast healthcare trends - we intervene where it matters most. Our mission is to identify individuals at high risk of escalating musculoskeletal (MSK) costs and guide them toward conservative, AI-powered care pathways. By preventing unnecessary surgeries and costly interventions, we reduce healthcare waste and improve lives. It's high-impact work with real human outcomes and we’re just getting started. If you’re driven by purpose, inspired by innovation, and excited to shape the future of proactive care, we’d love to have you on the team.

What you’ll be doing: 

Will be helping Predict (one of Sword’s most important solutions) design, build, and maintain ETL/ELT data pipelines to efficiently ingest and process data from multiple sourcesHelp us understand and create data products to be used by our external clientsLead thought around data architecture, data ingestion to make sure we are building robust processes and systemsCollaborate with engineering/product/analyst teams


What you need to have:

Experience in data engineering, with a deep understanding of data pipelines, cloud infrastructure, and big data technologiesExpertise in supporting AI/ML models - familiarity with LLMs and their integration into healthcare applications is a huge plus!A passion for creating data-driven solutions that enhance the healthcare experience and deliver real-world impactA collaborative mindset, able to work across interdisciplinary teams including AI researchers, data scientists, and healthcare professionalsExperience with cloud platforms (AWS, GCP, Azure) and scalable data frameworks (Spark, Kafka, etc.)Experience in designing and implementing scalable, efficient data pipelines, with a focus on SQL and modern data modeling techniquesExperience in developing data engineering solutions using Python, and PySpark, with particular experience in applying tools to AI and ML tasks


What we would love to see:

Proven track record of creating data platforms for production-ready ML models, including model deployment, monitoring, and retrainingUnderstanding of data ingestion systems and tradeoffs to make when ingesting Petabytes of data vs MBs of data


US - Sword Benefits & Perks: 

 Comprehensive health, dental and vision insurance* Life and AD&D Insurance* Financial advisory services* Supplemental Insurance Benefits (Accident, Hospital and Critical Illness)* Health Savings Account* Equity shares* Discretionary PTO plan* Parental leave* 401(k) Flexible working hours Remote-first company Paid company holidays Free digital therapist for you and your familyEligibility: Full-time employees regularly working 25+ hours per week


Note: Applicants must have a legal right to work in the United States, and immigration or work visa sponsorship will not be provided.

SWORD Health, which includes SWORD Health, Inc. and Sword Health Professionals (consisting of Sword Health Care Providers, P.A., SWORD Health Care Providers of NJ, P.C., SWORD Health Care Physical Therapy Providers of CA, P.C.*) complies with applicable Federal and State civil rights laws and does not discriminate on the basis of Age, Ancestry, Color, Citizenship, Gender, Gender expression, Gender identity, Gender information, Marital status, Medical condition, National origin, Physical or mental disability, Pregnancy, Race, Religion, Caste, Sexual orientation, and Veteran status.

Base: $119,000 - $187,000

Variable: $9,450 - $14,850

Net equity value per year: $10,380 - 16,311

These compensation bands are just the starting point. Once someone joins and proves they’re outlier talent, we adjust quickly to ensure their compensation aligns with their impact.

Our job titles may span more than one career level. Actual pay is determined by skills, qualifications, experience, location, market demand, and other factors. Compensation details listed in this posting reflect the base salary and any potential variable, bonus or sales incentives, and the Company’s estimation of the value of private company stock options, if applicable. The pay range is subject to change, future value of company stock options is not guaranteed, and compensation may be modified in the future. In addition to our total compensation, Sword offers a number of benefits as listed below.",https://jobs.lever.co/swordhealth/d3dcf869-b18b-4979-89d6-64d0fd93f1d5/apply?lever-source=LinkedIn
Rhombus,https://www.linkedin.com/company/rhombus-systems/life,"Data Engineer, Palo Alto",https://www.linkedin.com/jobs/view/4258085718,4258085718,"Palo Alto, CA",,,2025-06-26 21:15:05,False,,"Rhombus Power is purposefully transforming defense and global security enterprises with Guardian, our Artificial Intelligence platform for strategic, operational, and tactical decision-making at the speed of relevance.

We provide relevant, actionable, and AI-powered insights at each step in the defense decision-making cycle. Equipped with Guardian's AI-powered tools-- from infrastructure to data to insights -- our clients are able to solve their most complex, interconnected challenges and achieve decision and operational superiority.

Come join our cross-disciplinary and world-class team that is delivering game-changing solutions to transform global security.

Learn more about Rhombus and watch a demonstration of Guardian, our AI Platform here -- https://youtube.com/watch?v=3PxY6su1Q-Q

See the following articles to learn more about what we do:

https://foreignpolicy.com/2023/06/19/ai-artificial-intelligence-national-security-foreign-policy-threats-prediction/

https://federalnewsnetwork.com/air-force/2023/12/new-decision-advantage-tool-will-change-how-air-force-makes-investment-decisions/

https://apnews.com/article/us-intelligence-services-ai-models-9471e8c5703306eb29f6c971b6923187

Location

Palo Alto, CA

Job Description

Rhombus’ Data Engineers are instrumental in designing and implementing data engineering activities on topical, mission-driven projects that inform our national security agenda. They analyze, build, and maintain our organization’s data infrastructure and processing systems.

Additionally, our Data Engineers perform various code development and query needs for integration with new and existing applications, dashboards and machine learning pipelines. Our Data Engineers work cross functionally with other departments to meet business needs. They utilize their skills at all stages of the analytics pipeline to create results that drive business values and customer success.

Responsibilities

Develop code using various programming and scripting languages to automate data ingestion and improve data management processes.Architect data repositories, stand up data platforms and develop data pipelines for ingestion, transformation, and aggregation.Review existing architecture, data strategy, and improve processes for data governance, data quality, and metadata management.Extract and analyze raw data from multiple data sources via APIs, SQL Stored Procedures, or Python scripts.Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms.Collaborate with a multi-disciplinary team of analysts, data scientists, data engineers, developers, and data consumers in a fast-paced, agile environment.You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for clients.Communicate project status and results to various levels of leadership.

Qualifications

At minimum Bachelor’s degree in Data Analytics, Computer Science, Computer Engineering, Information Systems/Sciences, or other relevant area (or equivalent experience) and at least 3 years of professional experience, or a Master’s degree with strong academic project experience and one year of work experience. Experience with 1 or more programming and scripting languages, with a strong focus on Python, Pandas and Numpy. Other languages include: Shell, PERL, Java, C/C++/C#, Scala, etc.Experience with 1 or more of the following relational, noSQL and/or file based storage (e.g. MYSQL, MongoDB, SQL Server, Oracle, Postgres, Hbase, DynamoDB, etc.)Experience building and maintaining ETL data pipelinesExperience with software development life cycle including testing, documenting, delivery and supportWorking knowledge of AWS/cloud technologiesExperience using query optimization as well as data modeling techniques.Familiarity with machine learning frameworks (such as TensorFlow, Scikit-Learn, etc.)

Personal Qualities

Bias for action: Rhombus seeks ‘doers’ who will get things done with accuracy, speed and at scale.Ability to contextualize data as it relates to Rhombus’ vision. We want someone who can understand the “why” and “so what”, and not just the “how”. Our Data Engineers help us transmit the enthusiasm we have around the data work to the individuals doing that work.Must have strong problem-solving skills and demonstrated excellent oral and written communication skills.We’re seeking folks with strong intellectual curiosity and creativity, with an ability and willingness to learn new skills and apply novel solutions to problems.Comfort in a fast-paced start-up environment is crucial, with the ability to consistently revise your approach in response to new information, and to be flexible and adaptive.

Benefits

Full medical, dental, vision coverage for employee and dependents 401k matching program PTO and Holidays Bonus and other incentive programsAccess to mental health programAccess to Flexible Spending Accounts for Health Care, Dependent and Commuter

About Rhombus

Rhombus Power Inc. (Rhombus) is a startup located in the heart of Silicon Valley at Stanford Research Park in Palo Alto.  We use cutting-edge cross-disciplinary approaches to solve pressing Big Data and Sensing problems in security, energy, and healthcare. Our advisory board includes two Nobel Laureates and a Draper Prize winner.

Rhombus compensates, motivates, and develops employees, who are trusted, empowered, and involved. Employees have clear roles and expectations – and their roles are flexible enough to move at the speed of innovation in order to meet and exceed client expectations. We have a unique culture of global purpose, rooted in the innovation and progress of Silicon Valley.

Rhombus knows that diversity is a condition for success. We are committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer.",https://www.adzuna.com/details/5269562043?v=FD06EDCFEEC25AD22D192679A425AB8625D24534&frd=ca3c6deabac424cfaf2bb508da170de9&ccd=e1573393ed716a2ed75730d610bf6714&r=19669799&utm_source=linkedin7&utm_medium=organic&chnlid=1931&title=Data%20Engineer%2C%20Palo%20Alto&a=e
CoEnterprise,https://www.linkedin.com/company/coenterprise/life,"Pre-Sales Engineer – Cloud Data Warehouse, Data Engineering & AI - US Based - Remote",https://www.linkedin.com/jobs/view/4255417459,4255417459,United States,Remote,,2025-06-26 18:51:04,False,,"Company Description

CoEnterprise is an award-winning B2B software and professional services company headquartered in New York City. Founded in 2010, CoEnterprise delivers Data & Analytics solutions and services that transform how companies connect and do business. CoEnterprise approaches each relationship and engagement from the perspective of three core values: collaboration, ownership, and excellence. We value collaboration with both our partners and clients in order to present the best possible outcome for our customers. Our vow to accept ownership ensures that our entire staff takes pride in our work and it is our commitment to excellence that ensures that this work is at the highest standard possible.

Job Description

Job Overview:

We are looking for a highly skilled and motivated Sales Engineer with deep expertise in cloud data warehousing, data engineering, and AI/ML to join our growing sales engineering team. In this client-facing role, you will collaborate with the sales team to present tailored, cutting-edge solutions that leverage platforms like Databricks and Snowflake for cloud data warehousing and AI/ML integration. You will be the bridge between business needs and technical solutions, helping customers navigate complex data challenges and drive impactful outcomes with data engineering and AI technologies.

Key Responsibilities:

Pre-Sales Support & Solution Design:

Collaborate with the sales team to deeply understand customer business requirements and design scalable cloud data architectures using Databricks, Snowflake, and other cloud-based data platforms. Present tailored product demonstrations, Proof of Concepts (POCs), and architectural walkthroughs to showcase the capabilities of cloud data solutions and AI/ML integrations. Develop and deliver high-impact technical presentations and workshops on data engineering best practices, cloud data architecture, and AI-driven analytics. Assist with RFP/RFI responses, crafting technical solutions and aligning with customer objectives to win business. Facilitate data strategy workshops to roadmap client analytics priorities Build scopes of work for data projects 

AI & Data Engineering Expertise:

Work closely with customers to understand their data engineering workflows, including data pipelines, ETL processes, and data integration needs. Advise clients on the optimal design and implementation of data pipelines, automation strategies, and the integration of AI/ML models for real-time or batch processing. Assist clients in identifying key use cases for AI/ML, guiding them on how to integrate machine learning models into their data architecture for predictive analytics, anomaly detection, or automation. 

Customer Engagement & Support:

Act as a trusted advisor to customers, helping them to solve complex data and AI challenges while ensuring they realize the full potential of Databricks, Snowflake, and other cloud technologies. Develop strong, ongoing relationships with customers, providing continuous guidance on optimizing their data infrastructure and AI capabilities. Partner with customer success and support teams to ensure smooth implementation and ongoing customer satisfaction. 

Collaboration & Knowledge Sharing:

Work closely with Product Management, Engineering, and Data Science teams to understand product roadmaps, new feature releases, and customer feedback. Train and mentor internal sales teams on the latest advancements in cloud data warehousing, data engineering, and AI technologies. Stay up-to-date with emerging trends in AI, machine learning, cloud data platforms, and data engineering methodologies, and share that knowledge with the team and customers. 

Technical Enablement & Best Practices:

Build and maintain technical documentation, presentations, and other collateral to support customer engagement. Contribute to the creation of best practices, white papers, and technical case studies around the integration of cloud data warehouses and AI/ML technologies. Lead internal training sessions, webinars, and knowledge-sharing events to educate internal teams and customers on AI-driven data solutions. 

Qualifications

Qualifications:

Technical Expertise:

Strong experience with cloud data platforms such as Databricks, Snowflake, or similar solutions (Google BigQuery, AWS Redshift, etc.). In-depth knowledge of data engineering concepts, including data pipelines, ETL/ELT processes, and big data processing frameworks. Proven experience with AI/ML technologies, including model development, deployment, and integration into data workflows. Familiarity with data integration tools, real-time data processing, and advanced analytics pipelines. Experience with cloud environments (AWS, Azure, Google Cloud) and associated data services for storage, processing, and analytics. 

Sales & Customer Engagement:

Proven ability to engage with senior technical and business stakeholders, demonstrating both technical expertise and business acumen. Experience in sales engineering, pre-sales support, or a similar technical sales role in the data, cloud, or AI/ML domain. Exceptional presentation, communication, and interpersonal skills, with the ability to explain complex technical concepts to a broad audience. Strong customer-focused mindset with the ability to articulate the business value of technical solutions. 

Experience & Education:

Bachelor’s degree in Computer Science, Engineering, Data Science, or a related field (or equivalent practical experience). 5+ years of experience in cloud data architecture, data engineering, AI/ML, or sales engineering. Hands-on experience with cloud data platforms (Databricks, Snowflake, etc.) and AI/ML tools (TensorFlow, PyTorch, etc.) is highly desirable. 

Bonus Skills (Nice to Have):

Certifications in Databricks, Snowflake, AWS, Azure, or other relevant technologies. Familiarity with BI tools (Power BI, Tableau, Looker) and data visualization techniques. Experience working in an Agile development environment and with DevOps practices. Bonus: supply chain experience (if we want to focus/pitch supply chain projects in our analytics practice) 

Additional Information

Come experience our spirited culture and work with a smart, dedicated and high-energy team in a stable and fast-growing company! Here is a small sample of our benefits and perks we offer:

Comprehensive Health Insurance with generous employer contributionMatching 401(k) - $$$$Generous PTO PolicyVirtual Team LunchesWellness ProgramMonthly MinglesBirthday CelebrationsVirtual Events- Happy Hours, Casino Night, Magic Show, Scavenger Hunt of National History Museum, Game Nights and more

At CoEnterprise, we believe diversity drives innovation. We are committed to creating and maintaining a workplace in which all employees have an opportunity to participate and contribute to the success of our business. In recruiting for our team, we welcome the unique contributions that you can bring. We value employees for their differences represented by a variety of dimensions including demographics, behaviors, work style and perspectives.

We are an AA/EOE employer.",https://jobs.smartrecruiters.com/CoEnterpriseLLC/744000067270794-pre-sales-engineer-cloud-data-warehouse-data-engineering-ai-us-based-remote
Imprint,https://www.linkedin.com/company/imprint-payments/life,Data Engineer,https://www.linkedin.com/jobs/view/4256550089,4256550089,Utica-Rome Area,Remote,,2025-06-26 20:22:51,False,,"Who We Are

Imprint is reimagining co-branded credit cards & financial products to be smarter, more rewarding, and truly brand-first. We partner with companies like H-E-B, Turkish Airlines, Brooks Brothers, and Eddie Bauer to launch modern credit programs that deepen loyalty, unlock savings, and drive growth. Our platform combines advanced payments infrastructure, intelligent underwriting, and seamless UX to help brands offer powerful financial products—without becoming a bank.

Co-branded cards account for over $300 billion in U.S. annual spend—but most are still powered by legacy banks. Imprint is the modern alternative: flexible, tech-forward, and built for today’s consumer. Backed by Kleiner Perkins, Thrive Capital, and Khosla Ventures, we’re building a world-class team to redefine how people pay—and how brands grow. If you want to work fast, solve hard problems, and make a real impact, we’d love to meet you.

The Team

The Data Engineering team at Imprint is responsible for building and scaling the data infrastructure that supports product development, analytics, operations, and machine learning across the company. We own the pipelines, platforms, and processes that empower our stakeholders to trust and act on our data.

We’re looking for a Senior Data Engineer to help evolve our modern data stack and deliver reliable, scalable data solutions. Your work will directly power decision-making and innovation across the business—from financial operations to real-time personalization.

What You’ll Do

Design, build, and maintain scalable data pipelines and infrastructure across batch and streaming systems.Own core components of Imprint’s data stack, including Snowflake, dbt Cloud, Change Data Capture frameworks, and reverse ETL integrations.Develop and enforce best practices in data modeling, testing, observability, and governance.Partner with stakeholders across Product, Analytics, Finance, and Engineering to ensure timely and accurate data delivery.Work on external data integrations, such as partner-facing data shares (e.g., S3, SFTP, Snowflake) and financial reporting pipelines (e.g., with Netsuite).Contribute to architectural decisions for how we scale data infrastructure, including schema design, orchestration, and data lineage.Champion clear documentation, reproducibility, and reliability for critical datasets and workflows.Stay informed about modern data tools and trends and help drive their adoption when appropriate.

What We Look For

6+ years of experience in data engineering, analytics engineering, or related roles.Expertise in Snowflake and dbt Cloud, with a strong understanding of dimensional modeling and data warehouse best practices.Experience working with Change Data Capture (e.g., Fivetran, Hevo), ETL/ELT pipelines, and orchestration frameworks (e.g., dbt Cloud, Airflow).Familiarity with reverse ETL tools like Hightouch or Segment, and operational analytics use cases.Strong SQL skills and proficiency in Python or a similar programming language.A track record of technical ownership and shipping production-grade data systems.A detail-oriented mindset and a passion for building clean, maintainable, and observable data systems.Strong communication skills and the ability to collaborate effectively with cross-functional partners.

Nice to Have

Experience in fintech, high-growth startups, or customer-facing data products.Familiarity with event streaming technologies like Kafka or Kinesis.Exposure to data governance, security, or compliance practices.Previous work on ML pipelines or experimentation frameworks.

Perks & Benefits

Competitive compensation and equity packagesLeading configured work computers of your choiceFlexible paid time offFully covered, high-quality healthcare, including fully covered dependent coverageAdditional health coverage includes access to One Medical and the option to enroll in an FSA16 weeks of paid parental leave for the primary caregiver and 8 weeks for all new parentsAn understanding that successful hybrid work requires flexibility and an appreciation for asynchronous workAccess to industry-leading technology across all of our business units, stemming from our philosophy that we should invest in resources for our team that foster innovation, optimization, and productivity

Imprint is committed to a diverse and inclusive workplace. Imprint is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. Imprint welcomes talented individuals from all backgrounds who want to build the future of payments and rewards. If you are passionate about FinTech and eager to grow, let’s move the world forward, together.",https://jobs.ashbyhq.com/imprint/0dad33aa-fcab-40be-8c4f-b31aea8c6ba2?utm_source=aRoGLll8KW
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Sr. Data Engineer,https://www.linkedin.com/jobs/view/4256538870,4256538870,"St Louis, MO",Remote,$50/hr - $60/hr,2025-06-26 20:09:06,False,,"job summary:

Position Purpose:

Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment.

location: Saint Louis, Missouri

job type: Contract

salary: $50 - 60 per hour

work hours: 8am to 5pm

education: Bachelors

responsibilities:

Day to Day:

Demonstrate strong hands-on skills to produce, test and debug code during various phases of the project.Demonstrate strong analytical and problem-solving capabilities to overcome any issues encountered during project planning andimplementation.Validate and verify code deployments, ensure code is generated in compliance with the best practices set forth by the organization and thedevelopment community.Document features being developed in each phase of the project and assist with the service design process.Report project statuses provide visibility to any development delays that could hamper project delivery timelines.Actively collaborate with the development team to develop high quality products and wear multiple hats in the SDLC process.Ensure products being built are using existing cloud infrastructure or be cloud-ready for an easy migration in the near future

Performance Metrics:

Expected to deliver on all accepted Features every quarter.Maintain a high level of support for production applications and make sure we are adhering to the SLAs defined.Lead sprint teams, delegate and monitor development of individual features, guide/mentor junior developers.Demonstrate strong hands-on skills to produce, test and debug code during various phases of the project.

qualifications:

 TOP REQUIREMENTS: 

1

Talend

2

MongoDB, Snowflake

3

AWS, GitLab

skills: Education/Experience:

A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science) and Requires 4 - 6 years of related experience.Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.

Technical Skills:

Years of experience required: 4-6 years minimumStrong ETL development experience using MongoDB, Python.Strong application development experience with Informatica IDMC, Snowflake and AWS S3 bucket.Development experience building DevOps automation pipelines in GitLab and Jenkins.Deployment experience with Amazon ECS and Kubernetes.Experience architecting and developing applications on MongoDB, Snowflake and AWS.Development experience with data intensive AWS Lambda functions, deployment functions to receive and process events from S3 buckets. Exposure to Snowpipe and Snowflake integration patterns.Experience with data extract, transform and load (Talend); Data ProcessingExperience with Other: diagnosing system issues, engaging in data validation, and providing quality assurance testingExperience with Data Manipulation; Data MiningExperience working in a production cloud infrastructureExperience with one or more of the following C# (Programming Language); Java (Programming Language); Programming Concepts; Programming Tools; Python (Programming Language); SQL (Programming Language)Knowledge of Microsoft SQL Servers; SQL (Programming Language)Additional qualities to look for: Teradata/SQL Server, SSIS, React.js, Node.js, Angular.js, jQuery

Soft Skills:

Intermediate - Seeks to acquire knowledge in area of specialtyIntermediate - Ability to identify basic problems and procedural irregularities, collect data, establish facts, and draw valid conclusionsIntermediate - Ability to work independentlyIntermediate - Demonstrated analytical skillsIntermediate - Demonstrated project management skillsIntermediate - Demonstrates a high level of accuracy, even under pressureIntermediate - Demonstrates excellent judgment and decision making skills

Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.

At Randstad Digital, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants. If you require a reasonable accommodation to make your application or interview experience a great one, please contact

Pay offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc. In addition, Randstad Digital offers a comprehensive benefits package, including: medical, prescription, dental, vision, AD&D, and life insurance offerings, short-term disability, and a 401K plan (all benefits are based on eligibility).

This posting is open for thirty (30) days.

It is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability.",https://click.appcast.io/t/eJAdD2b7PDCYcsGemwEjC4J9yzV8EDdPbQdG1FtBajE=
Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,BI Report Developer,https://www.linkedin.com/jobs/view/4256539669,4256539669,United States,Remote,,2025-06-26 20:09:01,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Nextgenpros Inc, is seeking the following. Apply via Dice today!

Role: Report Developer

Location: Remote Need to work on EST (C2H 6 months)

Fulltime

Company Overview:

Our client is an in-home, multidisciplinary medical group providing 24/7 whole-person care. Our clinical team treats physical, behavioral, and social health needs when and where a patient needs help. Everyone on our team from our doctors, nurses, and Care Specialists to our HR, Technology, and Business Services staff are driven by a desire to improve the lives of our patients. We are able to treat a wide range of needs everything from addressing poorly controlled blood sugar to combatting anxiety to accessing medically tailored meals because we know that health requires care for the whole person. It s no wonder 98% of patients report being fully satisfied with Upward Health! Job Title & Role Description: The Report Developer at Upward Health will play a crucial role in transforming complex data into accessible and actionable insights. This individual will be responsible for creating and editing visual content, including maps, charts, and graphs, to present data in ways that are easy to interpret and spot patterns, trends, and correlations.

The ideal candidate will work in a fast-paced environment, developing scalable reporting features and becoming fluent in data warehouse models. They will analyze functional needs, design and develop software solutions, and ensure adherence to security policies and quality assurance best practices. This role requires the ability to meet deadlines, test code, document technical designs, and collaborate effectively with users and business analysts to translate requirements into technical solutions. Additionally, they will support and supervise technology projects in production environments.

Skills Required:

Bachelor's degree from a highly selective university.Industry experience in healthcare.Proficiency in Power BI, Microsoft SQL Server, and Structured Query Language (SQL).Expertise in creating data representations such as charts, graphs, and infographics.Experience developing and optimizing ETL services using Azure Data Factory (a plus).Strong written and verbal communication skills, with the ability to explain technical concepts to non-technical users.Ability to work independently and collaboratively in a virtual environment.Strong curiosity and desire to learn new skills and technologies.Ability to work with large datasets and see the bigger picture.",https://click.appcast.io/t/2ipAsPbWV_m8bDI5LFLKhmu8L0YUM16F0ACLa4QZ0QY=
Jobot,https://www.linkedin.com/company/jobot/life,Lead Data Engineer,https://www.linkedin.com/jobs/view/4254874835,4254874835,"Salt Lake City, UT",Remote,$150K/yr - $190K/yr,2025-06-26 15:19:23,True,51 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Remote - Lead Data Engineer - up to $190K base - join a team building systems to make data driven business decisions

This Jobot Job is hosted by Chuck Wirtz

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $150,000 - $190,000 per year

A Bit About Us

Our client, in the financial services industry, is seeking a Lead Data Engineer to join their team. This is a full-time, direct hire, remote role that can pay $150-190K base salary plus benefits, depending on experience.



Why join us?


This role is ideal for someone who thrives in a dynamic, fast-paced environment, enjoys solving complex data problems, and is passionate about driving innovation in data engineering. If you're looking to make an impact on the financial landscape with cutting-edge data solutions this could be for you!

Job Details

Core Responsibilities

Lead the design and implementation of end-to-end data pipelines, from extraction (API, scraping, pyodbc) to cleansing/transformation (Python, TSQL) and loading into SQL databases or data lakes.Oversee the development of robust data architectures that support efficient querying and analytics, ensuring high-performance and scalable data workflows.Collaborate with data scientists, software developers, business intelligence teams, and stakeholders to develop and deploy data solutions that meet business needs.Ensure smooth coordination between engineering and other teams to translate business requirements into technical solutions.Guide the development of data models and business schemas, ensuring that they are optimized for both relational (3NF) and dimensional (Kimball) architectures.Lead the creation of scalable, reliable data models and optimize them for performance and usability.Develop and maintain the infrastructure for large-scale data solutions, leveraging cloud platforms (e.g., Azure) and containerization technologies (e.g., Docker).Lead the use of modern data platforms such as Snowflake and Fabric, ensuring their effective use in large-scale data solutions.Manage and optimize data pipelines using tools such as Apache Airflow, Prefect, DBT, and SSIS, ensuring that all stages of the pipeline (ETL) are efficient, scalable, and reliable.Ensure robust testing, monitoring, and validation of all data systems and pipelines.Drive continuous improvement in data engineering processes and practices, ensuring they remain cutting-edge, efficient, and aligned with industry best practices.Foster a culture of clean code, best practices, and rigorous testing across the team.Strong experience with data pipeline design and implementation, including data extraction, transformation, and loading (ETL) processes.Proficiency in SQL (Postgres, SQL Server) and experience with modern data warehouse solutions (e.g., Snowflake, Fabric).Expertise in Python for data engineering tasks, including data manipulation (Pandas, NumPy) and workflow management (Dask, PySpark, FastAPI).Solid knowledge of cloud platforms (Azure, AWS) and big data technologies (Hadoop, Spark).Hands-on experience with Docker, Kubernetes, and containerized environments.Strong understanding of dimensional modeling (Kimball), relational database design (3NF), and best practices in data architecture.Experience with API development, including building and managing API integrations.Proficiency with orchestration tools like Prefect or Airflow for workflow management.Strong focus on testing and validation, ensuring that all data systems meet reliability and performance standards.

Experience & Qualifications

 5+ years of experience in data engineering roles, with a proven track record of developing and maintaining data pipelines and architectures. Experience working with large-scale data platforms and cloud environments. Strong background in relational databases, dimensional data modeling, and cloud-native solutions. Familiarity with data engineering tools such as Apache Airflow, Prefect, and cloud storage platforms. Excellent problem-solving skills, with the ability to navigate complex technical challenges.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4254874835
Westernacher Consulting,https://www.linkedin.com/company/westernacherconsulting/life,Microsoft Fabric Data Engineer,https://www.linkedin.com/jobs/view/4256553752,4256553752,"Minnesota, United States",Hybrid,,2025-06-26 21:42:24,False,,"About The Role

We are seeking a skilled Microsoft Fabric Data Engineer to join our data engineering team. In this role, you will be responsible for designing, building, and maintaining scalable data pipelines and analytics solutions using Microsoft Fabric and related Azure technologies. You will play a key role in transforming raw data into trusted, actionable insights to support business decision-making.

Your Responsibility

Design and implement robust data pipelines using Microsoft Fabric and Synapse Data Engineering experiences (formerly Dataflows Gen2, Lakehouse, and Pipelines)Build and manage Lakehouses and integrate with OneLake, supporting data ingestion, transformation, and consumptionCollaborate with data architects, analysts, and business stakeholders to understand data requirements and develop scalable data modelsDevelop, schedule, and monitor data workflows using Data Factory pipelines and Fabric integration featuresOptimize performance and cost of Fabric workloads, including Spark, SQL endpoints, and storageImplement data governance, security, and lineage using Microsoft Purview and Fabric-native featuresEnsure data quality, consistency, and reliability across sources and consumersStay current with Microsoft Fabric updates and best practices

Your Qualifications

Bachelor’s degree in Computer Science, Information Systems, or related fieldOver 5 years of experience in data engineering, with hands-on experience using Azure Data ServicesStrong proficiency with Microsoft Fabric, including Lakehouse, Data Pipelines, Notebooks, and Real-Time AnalyticsSolid experience with Azure Data Factory, Azure Synapse, and Azure Data LakeProficiency in SQL, PySpark, and data modeling (dimensional and relational)Familiarity with Power BI and its integration with Fabric datasetsUnderstanding of data governance, security, and metadata management

Preferred Qualifications

Experience with CI/CD for data pipelines and version control tools (e.g., Git)Knowledge of Delta Lake, KQL, and streaming data frameworksMicrosoft certifications (e.g., Azure Data Engineer Associate, Microsoft Certified: Fabric Analytics Engineer Associate) are a plusStrong communication skills and ability to collaborate in cross-functional teamsFamiliarity with Microsoft Dynamics 365 data structure and integration through DataverseExperience working with Power Platform tools such as Power Automate and Power Apps in data-centric or reporting-driven scenarios

Interested? Then, don't hesitate to apply!

This Is WE

Since 1969, Westernacher Consulting has been committed to innovation. With more than 1300 consultants across Europe, Asia, and the Americas, we deliver global SAP consulting services, helping medium to large-sized enterprises achieve innovation, operational excellence, and sustainable growth. Headquartered in Heidelberg, Germany, we are the global leader in business process and technology innovation.",https://jobs.eu.lever.co/westernacher/d8e4ee2e-7dd7-4d8b-aacb-8f41cabefd87/apply?source=LinkedIn
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256390414,4256390414,"Carson City, NV",Remote,$118K/yr - $122K/yr,2025-06-26 13:54:44,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/56505e02704547f9a0e7b26cd831f942tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Insight Global,https://www.linkedin.com/company/insight-global/life,Guidewire Data Engineer,https://www.linkedin.com/jobs/view/4256543754,4256543754,United States,Remote,$50/hr - $65/hr,2025-06-26 20:19:19,True,84 applicants,"Required Skills & Experience
- 8-10 years of experience within Data Engineering- Experience with CDA implementations- Deep understanding of Guidewire base tables for commercial and personal lines product models (ex: relationships, key structure, grain, table type, etc.)
Nice to Have Skills & Experience
Understanding of Guidewire DataHubKnowledge of GW implementationsExperience in developing ingestion pipelines within Databricks
Job Description
Insight Global's consulting client is seeking a highly skilled Data Engineer to join their team. The ideal candidate will have extensive experience with Guidewire’s core tables and the ability to integrate these into a data lake. This person will be required to implement and manage Guidewire CDA.This person will collaborate with offshore teams to guide the development of ingestion pipelines within Databricks.
This role is fully remote but may require monthly travel to the client in Ohio.
Compensation:$50/hr to $65/hr. Exact compensation may vary based on several factors, including skills, experience, and education. Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.",https://www.linkedin.com/job-apply/4256543754
LeadStack Inc.,https://www.linkedin.com/company/leadstackinc/life,Data Engineer -25-02850,https://www.linkedin.com/jobs/view/4258214112,4258214112,United States,Remote,$70/hr - $80/hr,2025-06-27 00:34:54,True,51 applicants,"Job Description  LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.  
Job title: Data Engineer Level 2 -W2 onlyLocation: Remote, - (EST Time zone) Job Duration: 6+ Months Contract 
 Job DescriptionWe are seeking a highly skilled and motivated Senior Data Engineer with 8+ years of experience to join our team. This role will be pivotal in designing and building our domain data interface. The ideal candidate will be an individual contributor capable of leading design discussions, making informed design decisions, and collaborating with stakeholders to create canonical data models. This role offers the opportunity to play a crucial part in shaping the architecture and success of our data solutions.
Qualifications:Strong expertise in supply chain concepts.Deep understanding of system design principles and data architecture.Proven experience with Databricks, data governance, and data engineering best practices.Excellent coding skills and experience in conducting detailed code reviews.Strong documentation skills to maintain clear and comprehensive project records.Ability to work independently and lead initiatives as an individual contributor.Strong communication skills for effective stakeholder collaboration.
Required Technical Skills:8+ years of experience in data engineering or related fields.Proficiency in Python, PySpark, and SQL.Azure, Databricks, and Azure data factoryExperience developing/designing scalable microservices in Java would be a plus.Experience with data modeling tools and frameworks.Strong understanding of data engineering concepts and hands-on experience with Databricks.Familiarity with system design concepts relevant to data architecture.Knowledge of data governance practices, including the use of Unity Catalog.
Key Responsibilities• Design, build, and maintain scalable data pipelines and ETL processes using Azure Data Factory, Databricks, and PySpark.• Architect and implement cloud-based data solutions on Azure, leveraging services such as Azure Synapse Analytics, Azure Data Lake Storage, and Azure SQL.• Develop and optimize complex SQL queries and stored procedures for data transformation and reporting.• Work with structured and unstructured data to create clean, reusable datasets for analytics and business intelligence.• Monitor, troubleshoot, and improve the performance and reliability of data workflows and pipelines.• Ensure data quality, governance, and security standards are consistently met.• Collaborate with data scientists and analysts to support ML and AI initiatives.• Contribute to best practices, documentation, and knowledge sharing within the data engineering team.


 know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/ Should you have any questions, feel free to call me on (513) 3184502 or send an email on waseem.ahmad@leadstackinc.com",https://www.linkedin.com/job-apply/4258214112
CrowdStrike,https://www.linkedin.com/company/crowdstrike/life,Sr. Engineer - Data Analytics (Remote),https://www.linkedin.com/jobs/view/4254836163,4254836163,"Baltimore, MD",Remote,,2025-06-26 09:00:09,False,,"As a global leader in cybersecurity, CrowdStrike protects the people, processes and technologies that drive modern organizations. Since 2011, our mission hasn’t changed — we’re here to stop breaches, and we’ve redefined modern security with the world’s most advanced AI-native platform. We work on large scale distributed systems, processing almost 3 trillion events per day and this traffic is growing daily. Our customers span all industries, and they count on CrowdStrike to keep their businesses running, their communities safe and their lives moving forward. We’re also a mission-driven company. We cultivate a culture that gives every CrowdStriker both the flexibility and autonomy to own their careers. We’re always looking to add talented CrowdStrikers to the team who have limitless passion, a relentless focus on innovation and a fanatical commitment to our customers, our community and each other. Ready to join a mission that matters? The future of cybersecurity starts with you.

About The Role

CrowdStrike is looking for a Data Analyst or Data Engineer to join our data and analytics team supporting Product and Engineering. In this critical role, you'll partner closely with technical stakeholders to deliver the metrics, models, and infrastructure that drive product strategy, roadmap decisions, and adoption insights.

You’ll work with large-scale product telemetry and system data to help CrowdStrike measure feature usage, guide engineering investments, and understand user behavior across our cybersecurity platform. This role is remote-first, but we strongly prefer candidates located in the U.S. Eastern Time Zone to enable better collaboration across global teams and direct alignment with the hiring manager.

What You’ll Do

Design and implement reliable, scalable data pipelines to ingest and transform product usage and adoption data.Translate business questions from Product and Engineering into clear metrics, models, and visualizations.Collaborate with cross-functional teams to define and track product KPIs tied to adoption, engagement, performance, and customer outcomes.Build and maintain Tableau dashboards and data assets that support decision-making across product lifecycles.Work within Snowflake to manage and optimize data models used by analysts, PMs, and engineers.Write clean, reusable Python and SQL code to automate analytics workflows and ensure data quality

What You’ll Need

Based in the U.S. Eastern Time Zone (EST preferred).Must be eligible for CJIS clearance (typically requires U.S. citizenship or permanent residency).5+ years of experience in a data-centric role (Data Analyst, Data Engineer, or hybrid).Expert in Python for scripting, data transformation, and pipeline development.Strong SQL skills with practical experience in analytics and performance tuning.Hands-on experience with Snowflake, including schema design and transformation logic.Advanced knowledge of Tableau (or similar BI tools) to create intuitive and impactful dashboards.Ability to translate business needs into scalable, trustworthy data solutions

Bonus Points

Experience with dbt, Airflow, or other modern orchestration frameworks.Familiarity with other BI platforms (Looker, Sigma).Experience with Golang and Streaming pipelines. Knowledge of product telemetry systems or event-based data structures.Experience working in cybersecurity, SaaS, or public sector analytics.Exposure to cloud platforms like AWS, GCP, or Azure.

#HTF

Benefits Of Working At CrowdStrike

Remote-friendly and flexible work cultureMarket leader in compensation and equity awardsComprehensive physical and mental wellness programsCompetitive vacation and holidays for rechargePaid parental and adoption leavesProfessional development opportunities for all employees regardless of level or roleEmployee Networks, geographic neighborhood groups, and volunteer opportunities to build connectionsVibrant office culture with world class amenitiesGreat Place to Work Certified™ across the globe

CrowdStrike is proud to be an equal opportunity employer. We are committed to fostering a culture of belonging where everyone is valued for who they are and empowered to succeed. We support veterans and individuals with disabilities through our affirmative action program.

CrowdStrike is committed to providing equal employment opportunity for all employees and applicants for employment. The Company does not discriminate in employment opportunities or practices on the basis of race, color, creed, ethnicity, religion, sex (including pregnancy or pregnancy-related medical conditions), sexual orientation, gender identity, marital or family status, veteran status, age, national origin, ancestry, physical disability (including HIV and AIDS), mental disability, medical condition, genetic information, membership or activity in a local human rights commission, status with regard to public assistance, or any other characteristic protected by law. We base all employment decisions--including recruitment, selection, training, compensation, benefits, discipline, promotions, transfers, lay-offs, return from lay-off, terminations and social/recreational programs--on valid job requirements.

If you need assistance accessing or reviewing the information on this website or need help submitting an application for employment or requesting an accommodation, please contact us at recruiting@crowdstrike.com for further assistance.

Find out more about your rights as an applicant.

CrowdStrike participates in the E-Verify program.

Notice of E-Verify Participation

Right to Work

CrowdStrike, Inc. is committed to equal pay for equal work in its compensation practices. The base salary range for this position in the U.S. is $0 - $0 per year + variable/incentive compensation + equity + benefits. A candidate's salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications, job level, supervisory status, and location.

Expected Close Date of Job Posting is:08-26-2025

",https://crowdstrike.wd5.myworkdayjobs.com/crowdstrikecareers/job/USA---Remote/Sr-Engineer---Data-Analytics--Remote-_R23699?source=LinkedIn_jobs
Analytica,https://www.linkedin.com/company/analytica-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4246666534,4246666534,United States,Remote,$120K/yr - $130K/yr,2025-06-07 17:52:51,False,,"Analytica is seeking a Data Engineer to support a complex data program for the Defense Health Agency. This role will work closely with reporting and analytics developers, data governance and data architects to build robust, high quality data pipelines that enhance productivity and operational efficiency. This position is for US Citizens only and candidate will hold (or be able to hold) a US Secret Clearance. The position is a remote position with occasional onsite meetings in at San Antonio, TX as required.
Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.
Responsibilities: Support the Military Health System (MHS) by supporting data engineering that enhance the Military Health System's (MHS) ability to leverage data as a strategic assetEnsure data interoperability, governance, and quality to support enterprise-wide decision-making and healthcare optimization play a critical role in designing and implementing data architecture frameworks that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset.Design and build robust and scalable data pipelines for managing structured and unstructured data using traditional databases (Oracle, PostgreSQL, etc.) or cloud Databases such as Amazon Redshift or AWS VerticaTranslate business needs into:data architecture solutions development within supported data systems.data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teamsMonitor and troubleshoot data import, analysis, and display errors.
Required Qualifications: Bachelor’s degree in information technology, Computer Science, Engineering or equivalent technical field3+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred)2+ years experience with Python and SQL (Java and Python preferred)Experience working on relational NoSQL and SQL databasesExperience designing and implementing various data pipeline patterns and strategiesStrong knowledge of data security principlesPrior experience with DHA M2 or MDR databases, CERNER EHR a strong plusMust be a US Citizen and must possess or be eligible for a SECRET clearance
About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

Analytica LLC is an Equal Opportunity Employer. We are committed to providing equal employment opportunities to all individuals, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other characteristic protected by applicable federal, state, or local law. As a federal contractor, we comply with the Vietnam Era Veterans' Readjustment Assistance Act (VEVRAA) and take affirmative action to employ and advance in employment qualified protected veterans.We ensure that all employment decisions are based on merit, qualifications, and business needs. We prohibit discrimination and harassment of any kind. Analytica LLC also provides reasonable accommodations to applicants and employees with disabilities, in accordance with applicable lawsWhen receiving email communication from Analytica, please ensure that the email domain is analytica.net to verify its authenticity.",https://analyticallc.applytojob.com/apply/gSgWiMyGlM/Data-Engineer
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256390403,4256390403,"Boise, ID",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:44,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/e358c25f59294b5c93fc21728038065ctjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Principal Engineer Data Engineering - US Remote,https://www.linkedin.com/jobs/view/4256391607,4256391607,"New Orleans, LA",Remote,,2025-06-26 13:54:30,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Anywhere Real Estate.

Anywhere is at the forefront of driving the digital transformation and building best-in-class products that help our agents and brokers sell more homes, make more money, and work more efficiently.

Data & Analytics (DNA) is Anywhere's data arm. We create innovative analytics, data science, and robust data foundation capabilities to generate data-driven insights that serve the heart of Anywhere Advisor and Anywhere Brand business. Together with our business counterparts in the real estate business, we work daily to deliver differentiating insights (AI & BI) for Strategy and AA & AB Operations.

We're seeking a Principal Engineer to join our Data Platform Team. In this critical position, you'll be responsible for designing, implementing, and managing the data infrastructure. You will work closely with data scientists, software engineers, and other stakeholders to ensure the Data Platform's availability, usability, and integrity.

Data Infrastructure Design And Implementation

Evaluate, select, and implement new tools and frameworks to expand our data platform capabilities. Design, build, and maintain robust, scalable, and reliable data pipelines and ETL processes. Develop and maintain data infrastructure and platforms using various technologies (e.g., AWS, Snowflake Cloud Platforms, databases, Kafka streaming platforms). Ensure data quality, consistency, and integrity across the organization. Architect and optimize Data Ingestion and Snowflake ETLs. Production Support and enhancements to the observability of the Data Platform. 

Team Leadership And Mentorship

Lead and mentor data engineers, providing guidance and support to junior engineers. Foster a culture of technical excellence and continuous learning. Collaborate with other teams (e.g., data scientists, software engineers, and product managers) to ensure data solutions meet business needs. 

Data Security And Compliance

Implement and maintain data security measures to protect sensitive data. Ensure compliance with data protection regulations and industry standards. 

Problem Solving And Innovation

Identify and solve complex data-related problems. Stay abreast of industry trends and emerging technologies and identify opportunities to enhance data capabilities. Proactively address performance, scale, complexity, and security considerations. 

Skills And Qualifications

Technical Expertise:

10+ years’ experience with a strong understanding of data engineering principles and technologies. 10+ years’ experience with data pipelines, ETL processes, and data warehousing. 5+ years’ experience building data pipelines using Kafka, Kafka Connect, Airflow, and Snowflake. 5+ years’ experience with Snowflake Data Platform. 5+ years’ experience with AWS Data Services such as DMS, EMR, Glue, Athena, S3, CloudWatch, Lambda, or IAM. 5+ years’ experience with Data Quality, Data Reconciliation 5+ years’ experience managing production data platforms. 5+ years’ experience building observability (Monitoring & Alerting) using tools such as Data Dog and M. Proficiency in programming languages (e.g., Java, Python, SQL). Knowledge of data governance, data modeling, and security best practices. Proficiency in CI/CD, IAC, and Agile Development. 

Leadership And Communication

Strong leadership and mentoring skills. Excellent communication and collaboration skills. Ability to explain complex technical concepts to both technical and non-technical audiences. 

Problem-Solving And Analytical Skills

Ability to identify and solve complex problems. Strong analytical skills to identify data quality issues and performance bottlenecks. 

Anywhere Real Estate Inc. (http://www.anywhere.re/)   (NYSE: HOUS) is moving real estate to what's next. Home to some of the most recognized brands in real estate Better Homes and Gardens® Real Estate (https://www.bhgre.com/) , Century 21® (https://www.century21.com/) , Coldwell Banker® (https://www.coldwellbanker.com/) , Coldwell Banker Commercial® (https://www.cbcworldwide.com/) , Corcoran® (https://www.corcoran.com/) , ERA® (https://www.era.com/) , and Sotheby's International Realty® (https://www.sothebysrealty.com/eng) , we fulfill our purpose to empower everyone's next move through our leading integrated services, which include franchise, brokerage, relocation, and title and settlement businesses, as well as mortgage and title insurance underwriter minority owned joint ventures. Anywhere supports nearly 1 million home sale transactions annually and our portfolio of industry-leading brands turns houses into homes in more than 118 countries and territories across the world.

At Anywhere, we are empowering everyone’s next move – your career included. What differentiates us is our scale, expertise, network, and unique business model that positions us as a trusted advisor throughout every stage of the real estate transaction. We pursue talent – strategic thinkers who are eager to always find a better way, relentlessly focus on talent, obsess about growth, and achieve exceptional results. We value our people-first culture, which thrives on empowerment, innovation, and cross-company collaboration as we keep moving the world forward, together. Read more about our company culture and values in our annual Impact Report (https://anywhere.re/wp-content/uploads/2025/03/2024-Impact-Report.pdf) .

We are proud of our award-winning culture and are consistently recognized as an employer of choice by various organizations including:

Great Place to Work Forbes World's Best Employers Newsweek World's Most Trustworthy Companies Ethisphere World's Most Ethical Companies 

EEO Statement: EOE including disability/veteran

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/6b28f3e6e2cb48969cf72c7be23c16cctjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256393270,4256393270,"Topeka, KS",Remote,$118K/yr - $122K/yr,2025-06-26 13:54:33,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/d34a901ff0f74e3ba0b6dd2417a24332tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256392183,4256392183,"Des Moines, IA",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:55,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/183406f4bad2484e90827377b000d9c0tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Accelon Inc.,https://www.linkedin.com/company/acceloninc/life,Data Engineer ,https://www.linkedin.com/jobs/view/4258096103,4258096103,San Francisco Bay Area,Hybrid,,2025-06-26 21:23:12,False,,"We’re looking for a Data Engineer to help build and maintain data pipelines and systems that power automated data workflows. This role blends data engineering, cloud infrastructure, and software development to support scalable, efficient, and intelligent data processes.
ResponsibilitiesDesign, build, and maintain scalable data pipelines and architecture.Develop and manage ETL processes using Python-based tools and frameworks.Work with cloud platforms (AWS, Azure, or GCP) to deploy and manage infrastructure.Collaborate with engineering and AI/ML teams to integrate automation into workflows.Ensure data quality, performance, and security standards are consistently met.Apply software engineering best practices to testing and validating data processes.Optimize data models and queries for scalability and performance.
RequirementsProficiency in Python and experience with data frameworks (e.g., Pandas, PySpark, Airflow, Prefect, Airbyte).Strong SQL skills and experience with cloud data warehouses (e.g., Snowflake, Clickhouse, Athena).Hands-on experience with at least one major cloud provider (AWS, Azure, or GCP).Strong understanding of software engineering practices in a data context.Self-motivated and comfortable working in fast-moving environments.Eagerness to stay updated with new tools and best practices in data engineering.
Nice to HaveExperience with real-time data streaming architectures.Familiarity with AI/ML workflows and automation.Background in infrastructure-as-code or DevOps for data systems.Contributions to open-source or data engineering communities.",https://zrec.in/x46we?source=CareerSite
Ascendion,https://www.linkedin.com/company/ascendion/life,Data Engineer,https://www.linkedin.com/jobs/view/4256521452,4256521452,"Dallas, TX",On-site,$90K/yr - $100K/yr,2025-06-26 17:55:24,True,over 100 applicants,"About AscendionAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate lifeWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:· Build the coolest tech for world’s leading brands· Solve complex problems – and learn new skills· Experience the power of transforming digital engineering for Fortune 500 clients· Master your craft with leading training programs and hands-on experienceExperience a community of change makers!Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:We are looking for an experienced Data Migration Specialist to lead and support the end-to-end migration of data and workloads from Teradata to Google Cloud Platform (GCP). The ideal candidate will have strong expertise in Teradata, GCP services (such as BigQuery), and large-scale data migration strategies.
Key Responsibilities:Lead the migration of data warehouse workloads from Teradata to GCP, ensuring performance, scalability, and data integrity.Analyze existing Teradata architecture, data models, and SQL scripts to define migration approach and transformation logic.Develop migration plans and execution strategies for large-scale datasets.Create ETL/ELT pipelines using tools like Cloud Dataflow, Apache Beam, or other GCP-native services.Optimize and validate data pipelines, ensuring minimal downtime and accurate data transfer.Collaborate with data engineering, analytics, and infrastructure teams to ensure successful deployment.Troubleshoot issues during the migration and post-migration phases.Document migration processes, mappings, and technical decisions
Minimum Qualifications:5+ years of experience in data engineering or database development roles.Strong experience with Teradata, including SQL optimization and database architecture.Hands-on experience with GCP, especially BigQuery, Cloud Storage, Dataflow, Pub/Sub, and Cloud Composer.Expertise in data migration and transformation projects involving large-scale data sets.Experience with ETL tools and scripting languages (e.g., Python, Shell).Familiarity with CI/CD practices in cloud environments.Strong understanding of data governance, quality, and security best practices.
Location: Dallas ,TX 
Salary Range: $90k to $100k pa 
Want to change the world? Let us know.Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!",https://www.linkedin.com/job-apply/4256521452
Rackspace Technology,https://www.linkedin.com/company/rackspace-technology/life,Senior Tableau BI Engineer,https://www.linkedin.com/jobs/view/4256381858,4256381858,"Egypt, AR",Remote,,2025-06-26 13:12:54,True,over 100 applicants,"As a full spectrum AWS, Azure, and GCP integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies – from migration strategy to operational excellence and immersive transformation.

If you like a challenge, you’ll love it here, because we solve complex business problems every day, building and promoting great technology solutions that impact our customers’ success. The best part is, we’re committed to you and your growth, both professionally and personally.

Overview

We are seeking a highly skilled BI Tableau Engineer to join our data and analytics team. This role is pivotal in transforming raw data into meaningful insights through dynamic dashboards, visualizations, and reports. The ideal candidate will have strong expertise in Tableau and business intelligence tools, along with a solid understanding of data modeling, ETL processes, and database management. This position requires a combination of technical proficiency and business acumen to support data-driven decision-making across the organization.

If you get a thrill working in various industries and love to help solve customers’ problems, we’d love to hear from you. It’s time to rethink the possible. Are you ready?

What you will doing:

Design, develop, and maintain interactive Tableau dashboards and reports to support business intelligence initiativesWork closely with business stakeholders to understand requirements and translate them into effective visual solutionsConnect to various data sources (SQL databases, APIs, cloud data platforms) and prepare datasets for reportingCreate and optimize data models and ETL processes to support scalable and efficient reportingConduct data validation and quality assurance to ensure accuracy and consistency of delivered dashboardsCollaborate with data engineers, analysts, and other IT professionals to ensure data governance and best practices are followedProvide training and support to business users on Tableau functionality and self-service analyticsStay current with Tableau and BI technology advancements and bring innovative solutions to the team


Qualifications & Experience

Bachelor's degree in Computer Science, Information Systems, Data Analytics, or a related field5+ years of experience in BI development, with 3+ years of hands-on Tableau experienceProficiency in SQL for data extraction, transformation, and analysisStrong understanding of data visualization principles and dashboard design best practicesExperience with data warehousing concepts, ETL processes, and data modelingFamiliarity with tools such as Tableau Prep, Power BI, Alteryx, or similarStrong analytical thinking, problem-solving, and communication skillsAbility to work independently and collaboratively in a fast-paced environmentPreferred experience with cloud data platforms (e.g., AWS Redshift, Google BigQuery, Snowflake)Knowledge of scripting languages like Python or R for data manipulation and automationTableau certification (e.g., Tableau Desktop Certified Associate or higher)


",https://www.linkedin.com/job-apply/4256381858
The Nuclear Company,https://www.linkedin.com/company/the-nuclear-company/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4254826441,4254826441,"Seattle, WA",On-site,$179K/yr - $203K/yr,2025-06-26 08:06:41,True,over 100 applicants,"The Nuclear Company is the fastest growing startup in the nuclear and energy space creating a never before seen fleet-scale approach to building nuclear reactors. Through its design-once, build-many approach and coalition building across communities, regulators, and financial stakeholders, The Nuclear Company is committed to delivering safe and reliable electricity at the lowest cost, while catalyzing the nuclear industry toward rapid development in America and globally.

About The Role

The Nuclear Company is a cutting-edge tech startup rapidly transforming the nuclear energy sector through innovation, digitalization, and a commitment to safe, sustainable power. We are building the future of clean energy, and reliable, high-quality data is at the core of everything we do. We are seeking an exceptional and visionary Senior Data Engineer to lead the design, development, and optimization of our foundational data infrastructure.

As a Senior Data Engineer, you will be a foundational member of our engineering team, responsible for architecting, building, and maintaining scalable, robust, and secure data pipelines and data platforms. You will play a critical role in enabling data-driven decision-making across all facets of our business, from engineering and operations to corporate development and regulatory compliance. This role requires deep technical expertise, a strategic mindset, and a commitment to data integrity and security paramount in the nuclear energy industry.

This role reports into the VP, Software Engineering.

Responsibilities

Data Architecture & Strategy:

Lead the design and implementation of our end-to-end data architecture, including data warehousing, data lakes, real-time streaming, and batch processing solutions. Define data governance strategies, standards, and best practices to ensure data quality, security, and compliance with industry regulations. Evaluate and recommend new data technologies, tools, and platforms to support our evolving business needs and growth trajectory. 

Data Pipeline Development & Optimization

Design, build, and optimize highly scalable and reliable ETL/ELT pipelines to ingest, transform, and load diverse datasets from various sources (e.g., sensor data, operational systems, financial platforms, third-party APIs). Implement robust data validation, monitoring, and alerting mechanisms to ensure data pipeline health and data quality. Troubleshoot and resolve complex data-related issues, optimizing data flows for performance and efficiency. 

Data Platform Management

Oversee the management and maintenance of our cloud-based data platforms (e.g., data warehouses, data lakes, streaming services), ensuring optimal performance, security, and cost-effectiveness. Develop and implement data security protocols, access controls, and encryption strategies to protect sensitive nuclear-related data. Ensure data systems support auditability and traceability required for regulatory compliance. 

Technical Leadership & Mentorship

Serve as a subject matter expert in data engineering, providing technical guidance and mentorship to other engineers. Champion data best practices, code quality, and scalable engineering principles across the organization. Lead complex data projects from conceptualization to deployment, ensuring successful execution and adoption. 

Cross-Functional Collaboration

Work closely with data scientists, machine learning engineers, software engineers, business analysts, and leadership to understand data requirements and deliver actionable insights. Collaborate with cybersecurity and regulatory teams to ensure data infrastructure meets stringent nuclear industry standards. 

Experience

Bachelor's degree in Computer Science, Data Engineering, Information Systems, or a related quantitative field. 6+ years of progressive experience in data engineering, with a significant focus on building scalable data pipelines and platforms. Proven experience in a lead or principal data engineering role, involving architectural decision-making. Expertise in cloud data platforms (AWS, Azure, or GCP).Proficiency in big data technologies (e.g., Spark, Kafka, Snowflake, Databricks, Redshift, BigQuery). Strong programming skills in Python, Scala, or Java, with a focus on data manipulation and pipeline development. Expertise in SQL and experience with various relational and NoSQL databases. Experience with data governance, data quality, and metadata management. Understanding of data security best practices. 

Skills

Data Architecture & Design ETL/ELT Development Cloud Data Platforms (AWS/Azure/GCP) SQL & Database Management Big Data Technologies Python/Scala/Java Data Governance & Quality Excellent Problem-Solving & Analytical Skills Strong Communication & Collaboration 

Preferred Qualifications

Master's degree in a relevant field. Experience in the energy sector, critical infrastructure, or another highly regulated industry. Familiarity with IoT data ingestion and processing. Experience with DevOps practices for data pipelines (CI/CD, infrastructure as code). Knowledge of nuclear energy concepts or operational data. 

Benefits

Competitive compensation packages401k with company matchMedical, dental, vision plansGenerous vacation policy, plus holidaysAnnual company retreats

Estimated Starting Salary Range

The estimated starting salary range for this role is $179,000 - $203,000 annually less applicable withholdings and deductions, paid on a semi-monthly basis. The actual salary offered may vary based on relevant factors as determined in the Company’s discretion, which may include experience, qualifications, tenure, skill set, availability of qualified candidates, geographic location, certifications held, and other criteria deemed pertinent to the particular role.

EEO Statement

The Nuclear Company is an equal opportunity employer committed to fostering an environment of inclusion in the workplace. We provide equal employment opportunities to all qualified applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, veteran status, or any other protected characteristic. We prohibit discrimination in all aspects of employment, including hiring, promotion, demotion, transfer, compensation, and termination.",https://www.linkedin.com/job-apply/4254826441
Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256391384,4256391384,"Baton Rouge, LA",Remote,$90K/yr - $110K/yr,2025-06-26 13:54:45,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/0f0c031e70c14f73b8d79cbb72f7deaftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
Lensa,https://www.linkedin.com/company/lensa/life,"Senior Data Engineer (Toronto/ Montreal, Remote)",https://www.linkedin.com/jobs/view/4256390574,4256390574,United States,Remote,,2025-06-26 13:54:33,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Autodesk.

Job Requisition ID #

25WD87271

Position Overview

Autodesk's Entertainment & Media Solutions is hiring a Senior Data Engineer to join our Autodesk Flow team. As a Senior Data Engineer, you'll play a key role in the technical development of engineering and business workflows. Your work will enable our teams to apply data for decision-making and will help drive customer value. In this role, you'll help us transform business decisions and processes with actionable customer insights gained from meaningful research, analysis, experimentation and measurement of the customer experience.

Responsibilities

Improve team productivity through creating future state best practices for business analytics Define, design, and implement AWS/BI technologies and services Support strategic programs and ongoing business requirements through data analytics Create analytical reports and develop BI layer with resulting conclusions 

Minimum Qualifications

3+ years of related professional experience, with at least two years of experience with data modelling, including the design of schemas optimized for data retrieval Proficient in SQL and data modeling for analytics and retrieval 3+ years experience in data visualization tools like Tableau, Looker, AWS Quicksight Programming skills in Python or Java Experience building data pipelines and working with large-scale data systems Hands-on experience with AWS (e.g., Lambda, Step Functions, SQS, IAM) Familiarity with Infrastructure as Code (e.g., Terraform, CloudFormation, CDK) 

Preferred Qualifications

BS or MS in Computer Science or related technical field Experience with data orchestration tools (e.g., Airflow, Temporal) Knowledge of big data technologies like Spark, Hive, or Presto Experience developing GraphQL, RESTful APIs and working with microservices Understanding of secure cloud architecture and CI/CD practices Experience developing data ingestion and processing pipelines, monitoring and self-healing systems Experience with AWS infrastructure as code solutions, such as CDK or Cloud Formation Experience working with Data Lake or Warehouses, ETLs Experience with data modeling, including the design of schemas optimized for data retrieval Programming with SQL, Python, Spark, PySpark, Spark SQL, Java, Jinja, dbt and related languages and technologies Experience with workflow management tools, like Airflow and Temporal Experience designing and implementing GraphQL, RESTful APIs in Python Strong technical skills and interest in learning best of class technologies around data warehousing, data wrangling, data quality, data governance and data ethics 

Learn More

About Autodesk

Welcome to Autodesk! Amazing things are created every day with our software – from the greenest buildings and cleanest cars to the smartest factories and biggest hit movies. We help innovators turn their ideas into reality, transforming not only how things are made, but what can be made.

We take great pride in our culture here at Autodesk – it’s at the core of everything we do. Our culture guides the way we work and treat each other, informs how we connect with customers and partners, and defines how we show up in the world.

When you’re an Autodesker, you can do meaningful work that helps build a better world designed and made for all. Ready to shape the world and your future? Join us!

Salary transparency

Salary is one part of Autodesk’s competitive compensation package. Offers are based on the candidate’s experience and geographic location. In addition to base salaries, our compensation package may include annual cash bonuses, commissions for sales roles, stock grants, and a comprehensive benefits package.

Diversity & Belonging

We take pride in cultivating a culture of belonging where everyone can thrive. Learn more here: https://www.autodesk.com/company/diversity-and-belonging

Are you an existing contractor or consultant with Autodesk?

Please search for open jobs and apply internally (not on this external site).

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/a0ff457c27c8432fb7c17f3f4f2f9782tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual
LotusLynx,https://www.linkedin.com/company/lotuslynx/life,Data Engineer,https://www.linkedin.com/jobs/view/4256532315,4256532315,"Chicago, IL",Hybrid,$115K/yr - $135K/yr,2025-06-26 19:02:13,True,over 100 applicants,"What You’ll Do:Design and implement data architectures using Azure Data Factory, Synapse, Databricks, Data Lake, and Azure SQLBuild and manage ETL/ELT pipelines to ingest and integrate data from diverse sourcesOptimize storage, indexing, and performance for high-volume data processingEnforce data governance and ensure compliance with GDPR, HIPAA, and other standardsImplement RBAC, encryption, and security protocols within Azure environmentsLeverage Azure DevOps for CI/CD and automated deployment of data infrastructureMonitor pipeline health, perform root-cause analysis, and troubleshoot data issuesCollaborate with cross-functional teams and document technical solutionsStay up to date on evolving Azure tools and industry best practicesWhat We’re Looking For:Bachelor’s degree in computer science, information technology, engineering, or related field (Master’s a plus)3–5 years of experience in data engineering or a similar roleStrong problem-solving and communication skillsComfortable working in agile, collaborative environmentsRequired Skills:Azure Data Services: Experience with ADF, Synapse, Databricks, Data Lake, and Azure SQLData Integration: ETL/ELT design, pipeline automation, and transformation logicProgramming: Proficiency in SQL, Python, and/or SparkData Architecture: Scalable data modeling, indexing, and partitioning strategiesSecurity & Compliance: RBAC, encryption, and regulatory standards knowledgeDevOps: Azure DevOps, GitHub Actions, Infrastructure as Code (ARM, Terraform)Data Governance: Familiarity with data quality, lineage, and stewardship best practicesPreferred Skills:Real-Time Data Streaming: Event Hubs, IoT Hub, or KafkaBig Data: Hadoop/Spark ecosystem familiarityAI/ML: Exposure to Azure ML or Azure OpenAIVisualization: Power BI, Tableau, or similar toolsIndustry: Experience in manufacturing or service-based environments",https://www.linkedin.com/job-apply/4256532315
