,company,company_url,job_title,job_url,job_id,location,work_type,salary,posted_at,is_easy_apply,applicant_count,description,apply_url,skills
0,Lensa,https://www.linkedin.com/company/lensa/life,Analytics Engineer - National Remote,https://www.linkedin.com/jobs/view/4256761039,4256761039,United States,Remote,$71.6K/yr - $140.6K/yr,2025-06-27 13:53:55,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for UnitedHealth Group.

Optum is a global organization that delivers care, aided by technology, to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.

You will enjoy the flexibility to telecommute* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities

Own the integrity of operational and performance metrics in given subject areas Independently execute analytics projects: Work autonomously with clear direction to deliver high-quality analytics solutions Knowledge gaining: Learn systems, technology, and processes both through training and independently to solve problems Data modeling and ETL processes: Design, develop, and maintain data models and ETL processes to support analytics and reporting needs Analytics architect: Apply understanding of the mechanics and user flow within a dashboard SQL expertise: Utilize strong SQL skills to query, analyze, and manipulate data efficiently Systems thinking: Foster a bias toward systems thinking, ensuring scalable and maintainable data solutions Business communication: Demonstrate strong business communication skills (both written and verbal) daily, effectively and concisely conveying complex data insights to stakeholders Knowledge sharing: Share learnings from your career and recent projects with teammates, contributing to a culture of continuous improvement and collaboration Collaboration: Work closely with analysts, direct users, and other stakeholders to understand business requirements and deliver data-driven insights 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear directions on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

High School Diploma/GED (or higher) 3+ years of experience in analytics engineering or a similar role 3+ years of experience with data visualization tools (e.g., Tableau, Power BI) 3+ years of experience in SQL and experience with data modeling and ETL processes 

Preferred Qualifications

Bachelor’s or Master’s Degree in Computer Science, Data Science, Statistics or a related field Tableau Desktop certification Strong understanding of systems thinking and its application in data engineering Familiarity with cloud-based data platforms (e.g. Azure, Snowflake) 

Soft Skills

Excellent written and verbal communication skills Proven ability to work independently and manage multiple projects simultaneously All Telecommuters will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

The salary range for this role is $71,600 to $140,600 annually based on full-time employment. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to the volume of applicants.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location, and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups, and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.

UnitedHealth Group is an Equal Employment Opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.

UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.

#RPO #GREEN

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/857212262aae4884ae7cbd225c8244a7tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['SQL', 'Python', 'Container', 'AWS', 'Snowflake', 'Communication', 'Problem-solving', 'Teamwork', 'Analytical', 'Time management']"
1,Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer / Fabric Admin (Remote),https://www.linkedin.com/jobs/view/4256755933,4256755933,"Raleigh, NC",Remote,$140K/yr - $150K/yr,2025-06-27 13:53:45,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currentlyseekinganexperienced and proactivePower BI Developer /Fabric Admin (Remote)to manage and support the enterprise-wide Power BI environment, focusing onPower BI Service administration, Fabric platform oversight, user support across tiers, and end-to-end report lifecycle management. This role requires technical depth in Power BI tools and infrastructure, combined witheffective communicationand stakeholder engagement to work across theCenter of Excellence (COE), business units, and IT support.This position will be fully remote located within the United States.

Responsibilities

Serve as aPower BI Fabric Administrator, overseeing workspace management, deployment pipelines, permissions, and performance monitoring. Develop aPower BI Center of Excellence (COE)to enforce standards, best practices, and governance for enterprise Power BI usage. Conducts PBI Training (Instructor-led Class Development, Delivery). ProvideTier 1 and Tier 2 supportfor Power BI issues—including report access, refresh failures, dataset issues, and workspace configuration. Assist withSelf-Help resources, including FAQs, templates, training materials, and troubleshooting documentation to empower end users. ManagePower BI licensing and desktop software distribution, working with ITassetsand licensing teams to track usage and entitlements. Support integration withexisting data sourcesandfacilitatethedesign and onboarding of new data sources, ensuring data quality, refresh schedules, and secure access. Handle incomingPower BI reporting and workspace requeststhrough a managed queue or mailbox, ensuringtimelyprioritization, response, and resolution. Design, publish, andmaintainPower BI reports and dashboardsaligned with business requirements andoptimizedfor usability and performance. Create andmaintainrobusttechnical documentation, including data dictionaries, architecture diagrams, workspace catalogs, and refresh schedules. Engage in proactive system health checks and performance tuning of Power BI Service, datasets, and gateway configurations. 

Qualifications

Required Skills and Experience

Bachelorswith 12+ years (orcommensurateexperience). Strong experience withPower BI Service Administration, Fabric workspace management, and deployment pipelines. Familiarity withPower BI governance, tenant settings, and organizational policies. Experience supporting and escalating issues acrossTier 1 and Tier 2 support models, including Service Desk and End-User Enablement. Knowledge ofPower BI licensing models(Free, Pro, Premium, PPU) and integration with Microsoft 365 administration tools. 

Preferred Skills And Experience

Solid understanding ofdata connectivity, gateway setup, refresh scheduling, and source control for bothexisting and new data sources.

Clearance Requirements

Ability to obtain and maintain a suitability/public trust clearance

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $140,000.00 - USD $150,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6140/power-bi-developer---fabric-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6140

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/816a6e4287e04d03bbb26ac5480191d6tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Python', 'SQL', 'Python', 'Data Analysis', 'AWS', 'AWS', 'Jenkins', 'Docker', 'Kubernetes', 'Jenkins']"
2,Precision Technologies,https://www.linkedin.com/company/precision-technologies-corp-/life,"Data Engineer (L2S, H4 EAD, EAD GC, GC, & US Citizens)",https://www.linkedin.com/jobs/view/4257006013,4257006013,United States,Remote,$50/hr - $60/hr,2025-06-27 16:30:32,True,over 100 applicants,"We’re hiring Data Engineer (5+ years experience) for our direct clients and implementation partners with active full-time openings! NO-(C2C)
The Hiring Manager is open to L2S, H4 EAD, EAD GC, GC, and US Citizens
📍Client Locations: New York, New Jersey, Texas, Georgia, Pennsylvania, Connecticut, Massachusetts & Chicago, IL 
📢 We welcome applications from candidates looking to restart their careers after a break!
Title: Data Engineer
Experience:- 5+ Years
Key Responsibilities:
Design, develop, and maintain robust, scalable, and efficient ETL/ELT pipelines to ingest, transform, and load data from diverse sources into data warehouses and data lakes.Build and optimize data models (dimensional/star schema) to support business intelligence, analytics, and reporting requirements.Work with large-scale datasets using technologies like Apache Spark, Hadoop, Hive, and Presto.Develop real-time data processing pipelines using Apache Kafka, AWS Kinesis, or Apache Flink to support streaming data applications.Ensure high levels of data integrity, quality, and consistency across multiple data sources and systems.Perform data validation, cleansing, and normalization to ensure data readiness for analytics and machine learning use cases.Optimize performance of databases and queries using SQL, SparkSQL, BigQuery, or Redshift.Collaborate with Data Scientists, Analysts, Product Managers, and Software Engineers to deliver end-to-end data solutions and analytics platforms.Implement data governance, security, and compliance policies in alignment with enterprise standards.Automate manual processes, optimize data delivery, and re-design infrastructure for greater scalability and efficiency.Monitor pipeline performance, perform root-cause analysis, and resolve production issues in a timely manner.
Note:- I apologize in advance if I happen to miss your call or respond late — please rest assured that I will get back to you as soon as possible
Please reach me at +1 732-348-5149 or you can send me your updated resume at javeed@ptcit.com",https://www.linkedin.com/job-apply/4257006013,"['python', 'sql', 'container', 'AWS', 'snowflake', 'Java', 'JavaScript', 'Git', 'Docker', 'Data Structures']"
3,Jobot,https://www.linkedin.com/company/jobot/life,Power BI report developer w/ SAP experience,https://www.linkedin.com/jobs/view/4255822857,4255822857,"Nashville, TN",On-site,$90K/yr - $100K/yr,2025-06-27 15:19:57,True,15 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Hybrid!

This Jobot Job is hosted by Ellie Staver

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $90,000 - $100,000 per year

A Bit About Us

This client of Jobot is a growing manufacturing company that has seen 3 years of consistent growth and poised for continued growth for the foreseeable future. We are seeking a highly skilled Reporting Specialist to design, develop, and maintain reports and dashboards using SAP and Power BI. This role is essential in helping business stakeholders effectively access and utilize data to make informed decisions. The ideal candidate will have strong expertise in both SAP and Power BI platforms and the ability to work closely with various business users to understand and meet their reporting needs through innovative data visualizations. Our client is a growing manufacturing company that implemented SAP a year ago. They pull their SAP data into PowerBI for reporting on all things.



Why join us?


Hybrid remote - in office 3 days in Nashville, 2 days remote

Innovative company that is growing and keeping up to date on tech!

Job Details

Key Responsibilities

Collaborate with various business units to gather reporting requirements and translate them into technical specifications.

Utilize SAP native tools and Power BI to design, develop, test, and maintain reports and dashboards.

Ensure the accuracy and integrity of data in all reports.

Optimize the performance of reports and dashboards for efficient data retrieval and display.

Integrate SQL queries into SAP and Power BI reports to enhance data accuracy and comprehensiveness.

Provide training and support for end-users on SAP and Power BI tools and techniques.

Stay up to date with the latest trends in SAP, Power BI, and SQL reporting and implement best practices for report design and development.

Key Skills & Competencies

Strong data visualization and analytical skills.

Expertise in SAP and Power BI.

Proficiency in SQL querying and database management.

Ability to work autonomously with minimal oversight.

Excellent problem-solving, troubleshooting, and communication skills.

Proven ability to collaborate with business users and IT teams.

Required Qualifications

5+ years of professional experience with data analytics, report development, and presentation in an SAP environment.

5+ years of Power BI experience, with proven experience in creating dashboards and reports.

A Bachelor’s degree in Information Technology, Computer Science, Business Administration, or a related field.

Relevant certifications in SAP, Power BI, and SQL are a plus.

Experience in database administration, automotive manufacturing, or data warehousing is a plus.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255822857,"['python', 'sql', 'container orchestration', 'AWS', 'cloud architecture', 'data analysis', 'machine learning', 'cybersecurity', 'project management', 'Agile methodology']"
4,Jobot,https://www.linkedin.com/company/jobot/life,Data Engineer,https://www.linkedin.com/jobs/view/4255820906,4255820906,"Cooks Crossing, North Carolina, United States",Remote,$80K/yr - $100K/yr,2025-06-27 15:20:01,True,58 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Fully remote Data Engineer opportunity // Data Modeling experience required!

This Jobot Job is hosted by Craig Rosecrans

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $80,000 - $100,000 per year

A Bit About Us

We are currently seeking a seasoned Data Engineer to join our dynamic Tech Services team. The ideal candidate will be a data enthusiast with a solid understanding of the latest data technologies and trends coupled with a strong desire to deliver top-notch solutions to our clients. The successful candidate will be responsible for designing, developing, and maintaining data architectures, databases, and processing systems. With a focus on ETL, Data Modeling, SSIS, Tableau, and PowerBI, this role will be integral in transforming data into readable, goal-driven reports for continued innovation and growth.



Why join us?


 Competitive Base Salary Company paid health plan for employees Flexible Hours Very generous PTO Dental and Vision, FSA, HSA Small team, autonomy Many more great perks!

Job Details

Responsibilities

 Design, develop, automate, and maintain productivity tools using programming, database or scripting languages to improve software modeling and development. Design and implement ETL procedures for intake of data from both internal and outside sources; as well as ensure data is verified and quality is checked. Collaborate with data architects, modelers and IT team members on project goals. Develop and maintain databases by acquiring data from primary and secondary sources, and build scripts that will make our data evaluation process more flexible or scalable across data sets. Employ a variety of languages and tools to marry systems together or try to hunt down opportunities to acquire new data from other systems. Develop and implement data standards, ensuring data quality and consistency. Perform data profiling to identify and understand anomalies. Present information using data visualization techniques through Tableau and PowerBI. Work closely with team members, clients, project managers, and other stakeholders to ensure solutions are delivered timely and accurately.

Qualifications

 Bachelor's degree in Computer Science, Information Systems, or a related field. A minimum of 5+ years of experience in a data engineering role with a proven record of successful data manipulation. Proficiency with ETL tools, SSIS, and experience with SQL/NoSQL databases. Knowledge and experience in data modeling. Experience with business intelligence tools like Tableau, PowerBI or similar. Strong problem-solving skills, attention to detail, and ability to think critically. Excellent written and verbal communication skills, with the ability to present complex data in a clear and concise manner. Ability to work independently and with team members from different backgrounds. Excellent organizational skills and the ability to manage multiple tasks concurrently. A strong desire to learn and develop new skills, staying up to date with the latest data best practices and technologies. Ability to work in a fast-paced, deadline-driven work environment.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255820906,"['developed', 'supporting', 'troubleshooting', 'scalability', 'AWS', 'snowflake', 'python', 'docker', 'gcp']"
5,Jobot,https://www.linkedin.com/company/jobot/life,Machine Learning Engineer – Generative Visuals (Text-to-Image),https://www.linkedin.com/jobs/view/4255827338,4255827338,"Indianapolis, IN",Remote,$150K/yr - $300K/yr,2025-06-27 15:19:42,True,9 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Y Combinator–backed Bio-Tech company is looking for a Senior Data Engineer to join their growing team!

This Jobot Job is hosted by Sydney Weaver

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $150,000 - $300,000 per year

A Bit About Us

With backing from Sequoia Capital and Y Combinator, a growing startup is looking for Senior ML Engineers and Applied AI Software Engineers to play a pivotal role in developing AI models that generate scalable vector graphics. This is a high-impact opportunity at the frontier of generative AI and their platform helps researchers quickly create accurate scientific visuals.



Why join us?


 Excellent Benefits Remote Work (USA or Canada) Growth Opportunity  Pet Benefits Great Culture, PTO, 401K, and much more!

Job Details

Responsibilities

 Design and implement advanced ML architectures for vector graphics generation Own end-to-end development of generative AI systems, from prototyping to scalable deployment Leverage foundational model techniques to power scientific communication tools Collaborate with a world-class team of engineers with deep AI and product experience Deploy multimodal AI pipelines and ensure robust, production-level performance Drive technical decisions and long-term planning for the ML platform Evaluate and incorporate open-source tools where appropriate Participate in hackathons and in-person team events (remote-first culture)

Requirements

 Bachelor’s degree in Computer Science or related field and Master’s or PhD from a top-tier academic institution strongly preferred Text to image gen AI experience Demonstrated experience building and scaling generative models (e.g., foundational models, text-to-image, Adobe Firefly) Strong programming skills in Python and deep learning frameworks like PyTorch or TensorFlow Expertise with text-to-image, foundational model, or generative AI  History of deploying multimodal AI systems in production environments Industry experience in both startup and large tech environments preferred Ability to pragmatically choose between custom solutions and leveraging existing tools Strong ownership mindset and ability to lead model architecture without heavy oversight

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255827338,"['javascript', 'cloud-computing', 'project-management', 'agile', '.NET', 'LaTeX', 'Microsoft Azure', 'Databricks', 'teamwork']"
6,Healthmine,https://www.linkedin.com/company/healthmine/life,Data Engineer II,https://www.linkedin.com/jobs/view/4258530978,4258530978,United States,Remote,$95K/yr - $112K/yr,2025-06-27 21:01:14,True,over 100 applicants,"JOB TITLE: Data Engineer IIDEPARTMENT: Data & AnalyticsREPORTS TO: Engineering Team LeadTYPE: Full Time, ExemptLOCATION: RemoteCOMPENSATION: $95,000 – $112,000
ROLE SUMMARY:
As a Data Engineer II, you are a consistent and highly effective individual contributor and an emerging thought leader as it relates to Healthmine’s data platform. In addition to contributing to the everyday maintenance and improvement of our data warehouse and audience-specific data marts, you identify key opportunities for improvement as they relate to increasing Healthmine’s ability to surface tangible and actionable value from our data at large. You also play a significant role in coordinating with other Engineering Team groups to help prioritize the Business Intelligence Team workstream.
WHAT YOU WILL DO:
Maintain and enhance the data pipelines that keep our data warehouse and audience-specific data marts up to dateMaintain and enhance the logical data models of our data warehouse and audience-specific data marts to support efficient querying strategiesActively monitor the health of the data platform at large and identify strategic opportunities to add/improve automation of the handling of dataSupport other business units as they identify BI-specific needsActively mentor Engineering Team membersBe an excellent team player with the ability to be both independent and highly collaborativeProactively learn new skills, concepts, programming languages, and technology stacks with a strong bias toward action and pragmatismOther duties as assigned
WHAT YOU NEED:
Bachelor’s degree in Computer Science, Mathematics, Management Information Systems, or an equivalent in working experience5+ years defining, implementing, and supporting logical data models of various types (highly normalized, star schemas, Data Vault 2.0, etc.)Mastery in authoring and analyzing complex SQL queriesDemonstrated experience implementing automated ETL workflows (Microsoft SSIS, Informatica, Talend, Airflow, DBT, etc.)Demonstrated experience with business intelligence tools (Tableau, Power BI, etc.)Strong competency in automating data-oriented workflows using open-source software (OSS) solutionsStrong competency working in a highly-automated and -disciplined development environment with tools like GitHub, TeamCity, and modern IDEs (VS Code, IntelliJ, WebStorm)Demonstrated experience working with in a cloud-based environment (Amazon AWS preferred)Significant experience working in an Agile/Scrum/Kanban environment and working with supporting tooling such as Pivotal Tracker, Jira, Rally, or TrelloSignificant experience in maintaining cloud-based documentation and supporting materials using products such as Google Docs/Drive, Microsoft SharePoint/OneDrive, Lucidchart, Slab, or NotionApplicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.
WHAT WE PROVIDE:
Competitive base salaries, full benefits (medical, dental, vision), and company paid STD/LTD benefits.401(k) with match and 100% vested on first day of contribution.100% remote work with semi-flexible schedules.Generous Universal Parental Leave.18 PTO days and 16 Paid Holidays.
ABOUT HEALTHMINE:
Healthmine is a technology-enabled member engagement and rewards company. We build personalized, ongoing engagement strategies for health plans that drive healthy action through incentives and rewards, enhance member experience, and improve outcomes.
Please note any outreach from a Healthmine Team Member will come from an email address ending with @healthmine.com. Any email from @healthminecareers.com email domain is in no way affiliated with Healthmine Services, Inc.
Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.
Healthmine provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Healthmine will provide reasonable accommodations for qualified individuals with disabilities.",https://www.linkedin.com/job-apply/4258530978,"['Python', 'SQL', 'Azure', 'AWS Services', 'Container Orchestration', 'Cloud Formation', 'Database Management', 'Big Data Analytics', 'Project Management', 'Agile Methodologies']"
7,Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256760047,4256760047,"Juneau, AK",Remote,$90K/yr - $110K/yr,2025-06-27 13:54:06,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/d4f5a0e7c2994bca9d5ae88b8750ce19tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Python', 'SQL', 'Data Analysis', 'Cloud Computing', 'AWS', 'Snowflake', 'Containerization', 'Systems Engineering', 'Project Management', 'Agile Development']"
8,Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Data Engineer/ Analyst,https://www.linkedin.com/jobs/view/4256722753,4256722753,"Philadelphia, PA",Remote,$50/hr - $60/hr,2025-06-27 08:08:22,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Aditi Consulting, is seeking the following. Apply via Dice today!

Payrate: $50.00 - $60.00/hr.

Details/Scope of the project:

Analyze current data infrastructure, tools, and integration workflows to assess capabilities and identify gaps. Develop scalable and repeatable processes for integrating structured and unstructured data sources, such as databases, APIs, and third-party platforms. Implement dependency mapping and ensure alignment with protocol specifications. Assist in the development and operationalization of predictive analytics models using Azure Machine Learning or similar services. Build workflows to automate scheduling, monitoring, and error handling. Create dashboards for continuous monitoring of data pipelines and integration processes. Provide training and operational documentation for internal teams.

Must have skills for this role: 

Proven experience in Data Engineering or Data Analysis with extensive knowledge of data infrastructure and analytics. Proficiency in Azure cloud services, data modeling, and machine learning tools. Strong skills in programming languages relevant to data handling and analysis. Excellent documentation and communication skills, with the ability to clearly present technical information. Fluent in Spanish and/or Portuguese.

Preferred skills: 

Excellent documentation and communication skills, with the ability to clearly present technical information. Fluent in Spanish and/or Portuguese.

Pay Transparency: The typical base pay for this role across the U.S. is: $50.00 - $60.00/hr. Final offer amounts, within the base pay set forth above, are determined by factors including your relevant skills, education, and experience and the benefits package you select. Full-time employees are eligible to select from different benefits packages. Packages may include medical, dental, and vision benefits, paid days off based on tenure, up to 40 hours paid sick time, 401(k) plan participation, commuter benefits and life and disability insurance.

For information about our collection, use, and disclosure of applicant's personal information as well as applicants' rights over their personal information, please see our Privacy Policy ().

Aditi Consulting LLC uses AI technology to engage candidates during the sourcing process. AI technology is used to gather data only and does not replace human-based decision making in employment decisions. By applying to this position, you agree to Aditi s use of AI technology, including calls from an AI Voice Recruiter.

#AditiConsulting

# 25-20332",https://click.appcast.io/t/OXy24Bs3Ectk0pg79I4xBBu08VwExeXOwz90jZ-FaIw=,"['javascript', 'HTML', 'CSS', 'React', 'Node.js', 'MongoDB', 'PostgreSQL', 'Java', 'Spring Boot', 'Docker']"
9,Extreme Networks,https://www.linkedin.com/company/extreme-networks/life,"Data Engineer, Lead Analyst, Enterprise Data and Analytics - Remote",https://www.linkedin.com/jobs/view/4258401542,4258401542,"Texas, United States",Remote,,2025-06-27 08:03:37,False,,"Over 50,000 customers globally trust our end-to-end, cloud-driven networking solutions. They rely on our top-rated services and support to accelerate their digital transformation efforts and deliver unprecedented progress. With double-digit growth year over year, no provider is better positioned to deliver scalable outcomes than Extreme.

Inclusion is one of our core values and in our DNA. We are committed to fostering an inclusive workplace that embraces our differences and creates an atmosphere where all our employees thrive because of their differences, not in spite of them.

Become part of Something big with Extreme! As a global networking leader, learn why there’s no better time to join the Extreme team.

The Data Engineer, Lead Analyst, Enterprise Data and Analytics will be a member of the “Expand” (Extreme Process Analytics and Data Governance) team and is responsible for developing the data flows, ETL/ELT pipelines, data marts, and analytics solutions in support of the global, enterprise data & analytics (EDNA) platform.

As part of this team, this position will partner with stakeholders throughout the business to understand data rules, business logic, and reporting and analytics needs to develop a platform of certified data definitions that support both dashboards and ad-hoc, self-service analytics. It includes hands-on development with adherence to architectural and development best practices. This role will support business stakeholders across all functions but primarily will start with Sales, Sales Ops, and Marketing.

This position is remote, reporting to the Senior Manager, Data Engineer of the Analytics pillar.

Specific Duties

 Develop analytical data models that provide actionable insights across a range of business functions. Develop data models, ingestion, transformation, and consumption layers for a scalable data platform in support of growing use cases and user personas. Implement the end-to-end analytics solution using a modern technical stack, e.g. Snowflake, DBT, Fivetran, Informatica, Tableau. Develop using best-practice data modeling techniques, leveraging SQL, ETL/ELT tools, and cloud-driven data architecture.

Qualifications

 Highly self-motivated and able to work independently as well as in a team environment. 3+ years hands-on experience working with BI, ETL/ELT data pipeline and integration patterns, and data modeling best practices, with in-depth knowledge of data governance, data engineering, visualization, and business intelligence functions 2+ years of experience with more than 1 database system, such as Redshift, Azure Synapse, BigQuery, Oracle, SQL Server, MySQL. 1+ years hands-on experience developing on modern data warehousing tools (e.g. Spark, Redshift, Snowflake, Azure Aurora, BigQuery, Incorta), deep expertise with SQL, and high proficiency building in BI solutions (e.g. Tableau, PowerBI, Looker, or other BI reporting layers) Experience implementing a variety of data warehousing concepts and methodologies, including snapshotting, incremental data loads, SCDs, and star schemas. Advanced understanding of SQL development in both stored procedures and leveraging ETL/ELT tools, such as Informatica, DBT, SSIS, Matillion. Can be highly flexible in adapting to the needs of the team and organization. Experience is a plus in managing the ingestion and modeling of the following business application data sources: Salesforce, Oracle Suite (EBS, Fusion, HCM), and Jira. Comfort in working within an agile team, leveraging DevOps concepts and agile-enablement tools including Jira, Confluence, and Github. Working experience with DevOps best practices and version control policies and platforms, e.g. Git Ability to clearly communicate complex technical ideas to both technical and business stakeholders, with demonstrated written and verbal presentation skills to present compelling recommendations.

Extreme Networks, Inc. ( EXTR ) creates effortless networking experiences that enable all of us to advance. We push the boundaries of technology leveraging the powers of machine learning, artificial intelligence, analytics, and automation. Over 50,000 customers globally trust our end-to-end, cloud-driven networking solutions and rely on our top-rated services and support to accelerate their digital transformation efforts and deliver progress like never before. For more information, visit Extreme's website or follow us on Twitter, LinkedIn, and Facebook.

We encourage people from underrepresented groups to apply. Come Advance with us! In keeping with our values, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on “protected categories,” Extreme Networks also strives to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our organization. Whether blatant or hidden, barriers to success have no place at Extreme Networks.",https://jobs.lever.co/extremenetworks/14ee8114-942c-411c-95e0-84c1c32b52c6/apply?source=LinkedIn,"['English', 'problem-solving', 'communication', 'teamwork ', 'VPC ', 'auto patch', 'advanced security ', 'AWS DynamoDB ', 'Docker', 'AWS CloudFormation']"
10,SquarePeg,https://www.linkedin.com/company/squarepeg-ai/life,Data Engineer (Entity Resolution),https://www.linkedin.com/jobs/view/4258441010,4258441010,United States,Remote,,2025-06-27 12:57:53,False,,"About SquarePeg

SquarePeg uses AI to screen and score tens of thousands of job applicants—fast, fairly, and at scale. Our platform ingests messy resume and job data from multiple systems and applies advanced ranking models to help recruiters get to inbox zero with confidence. Clean, deduplicated, and well-resolved data is core to everything we do.

We’re hiring a Data Engineer with deep entity resolution experience to help us improve how we match people to jobs—especially when the inputs are ambiguous, inconsistent, or incomplete.

What you'll do:

Build and scale data pipelines that ingest, clean, and resolve person, company, and job entities across disparate datasets (ATS exports, resumes, job descriptions, data sets, APIs)Own our entity resolution layer: design logic for deduplication, disambiguation, and canonicalization of candidates and companiesImprove our internal identity graphs for people, companies, and job titles by integrating open and proprietary data sourcesImplement and refine blocking strategies, fuzzy matching, and ML-based similarity scoring to improve match precision and recallWork closely with product and Eng to test resolution accuracy and continuously tune performance for production workloadsMonitor data integrity and build systems to surface issues before they affect scoring or UX

What we're looking for: 

4+ years of experience in data engineering or applied data science, ideally working with large-scale B2B or recruiting datasetsHands-on experience with entity resolution, including rule-based and ML-based approaches (e.g., record linkage, string similarity, embeddings, supervised matching models)Proficiency in Python and SQL; experience with Spark, DuckDB, or similar frameworks is a plusStrong understanding of data quality, normalization, and the challenges of real-world messy input dataThoughtful engineering mindset: you write testable, maintainable code and think about edge cases before they bite

Nice to Have

Experience with recruiting/talent data (e.g., resumes, job postings, ATS data)Familiarity with open-source tools like Splink, Dedupe, Scikit-learn, or Faiss for similarity matchingExperience working with skills taxonomies or job-title ontologiesPrior experience in a high-velocity startup environment

Why SquarePeg?

We’re solving one of the most painful and high-volume problems in hiring: figuring out which applicants are actually worth readingYou’ll work on a small, senior team with a bias for shipping, pragmatism, and deep techYour work will directly improve the quality of our applicant scoring, customer trust, and platform performanceCompetitive compensation, early equity, and a remote-first culture that respects your time

",https://www.squarepeghires.com/jobs/87k5kj/data-engineer-entity-resolution?utm_source=linkedin&utm_medium=jobposting&utm_campaign=general,"['Python', 'SQL', 'Container Management', 'AWS', 'Snowflake', 'Excel', 'Data Analysis', 'Project Management', 'Customer Relationship Management (CRM)', 'Communication']"
11,Lensa,https://www.linkedin.com/company/lensa/life,Data Quality Engineer,https://www.linkedin.com/jobs/view/4258453314,4258453314,"Rockville, MD",On-site,$85.5K/yr - $111.2K/yr,2025-06-27 14:06:55,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for The U.S. Pharmacopeial Convention (USP).

Description

Who is USP?

The U.S. Pharmacopeial Convention (USP) is an independent scientific organization that collaborates with the world's top authorities in health and science to develop quality standards for medicines, dietary supplements, and food ingredients. USP's fundamental belief that Equity = Excellence manifests in our

core value of Passion for Quality through our more than 1,300 hard-working professionals across twenty global locations to deliver the mission to strengthen the supply of safe, quality medicines and supplements worldwide.

At USP, we value inclusivity for all. We recognize the importance of building an organizational culture with meaningful opportunities for mentorship and professional growth. From the standards we create, the partnerships we build, and the conversations we foster, we affirm the value of Diversity, Equity,

Inclusion, and Belonging in building a world where everyone can be confident of quality in health and healthcare.

USP is proud to be an equal employment opportunity employer (EEOE) and affirmative action employer. We are committed to creating an inclusive environment in all aspects of our work—an environment where every employee feels fully empowered and valued irrespective of, but not limited to, race, ethnicity, physical and mental abilities, education, religion, gender identity, and expression, life experience, sexual orientation, country of origin, regional differences, work experience, and family status. We are committed to working with and providing reasonable accommodation to individuals with disabilities.

Brief Job Overview

We are seeking a skilled and experienced Data Quality Engineer to join our data management team. This is a hands-on role that requires a strong background in identifying and resolving data quality issues. The ideal candidate will have experience in data analysis, data quality assurance, and the design, implementation, and maintenance data quality systems and processes across a broad set of systems. You will work across departments to ensure the accuracy, consistency, and reliability of our data across various platforms.

How will YOU create impact here at USP?

As part of our mission to advance scientific rigor and public health standards, you will play a vital role in increasing global access to high-quality medicines through public standards and related programs. USP prioritizes scientific integrity, regulatory excellence, and evidence-based decision-making to ensure health systems worldwide can rely on strong, tested, and globally relevant quality standards.

Additionally, USP’s People and Culture division, in partnership with the Equity Office, invests in leadership and workforce development to equip all employees with the skills to create high-performing, inclusive teams. This includes training in equitable management practices and tools to promote engaged,

collaborative, and results-driven work environments.

The Data Quality Engineer Has The Following Responsibilities

 Data Monitoring: Monitor data quality metrics and generate reports to track data quality trends Root Cause Analysis: Investigate and determine the root causes of data quality issues and implement solutions to prevent recurrence. Data Validation: Design and implement effective testing strategies for data systems. Perform regular data validation checks to ensure accuracy and completeness. Data Cleansing: Identify and correct data discrepancies and inconsistencies. Collaboration: Work with data engineers, analysts, data scientists, and other stakeholders to resolve data quality issues. Documentation: Maintain detailed documentation of data quality processes and procedures. Support: Assist in developing and implementing data quality standards and best practices. Data Governance: Participate in data governance initiatives to ensure data quality policies and

standards are adhered to.

 Training: Provide training and support to team members on data quality best practices.

Who is USP Looking For?

The successful candidate will have a demonstrated understanding of our mission, commitment to

excellence through inclusive and equitable behaviors and practices, ability to quickly build credibility

Qualifications

with stakeholders, along with the following competencies and experience:

 Education: Bachelor's degree in Computer Science, Information Systems, Statistics, or a related

field.

 Experience: minimum 3 years of experience in data quality, data engineering, data analysis, or

related roles.

Skills

 SQL Proficiency: Strong knowledge of SQL for querying databases and performing data

analysis.

 Programming Skills: Proficiency in Python Familiarity with Data Quality Tools: data quality tools and frameworks to monitor and improve

data quality such as Amazon DeeQu, python packages (Great Expectations, pytest, unittest), etc.

 Analytical Skills: Strong analytical skills to identify, analyze, and resolve data quality issues. Attention to Detail: A keen eye for detail to ensure data accuracy and consistency. Communication: Excellent communication and teamwork abilities. Root Cause analysis and structured problem solving skills to address complex data quality issues. Data Visualization: Experience with Tableau or Power BI for reporting. Project Management: Ability to manage multiple projects and prioritize tasks effectively,

including managing project dependencies.

Additional Desired Preferences

 Experience with Master Data Hubs, and/or experience with Dell Boomi MDH. Experience with AWS services such as S3, Glue, and Redshift is a plus. Experience with Oracle E-Business Suite (ERP) data model a plus. Experience with ETL processes and common Data Warehouse schemas a plus. Experience with Adobe Analytics digital tracking platform a plus

Supervisory Responsibilities

None, this is an individual contributor role.

Benefits

USP provides the benefits to protect yourself and your family today and tomorrow. From company-paid time off and comprehensive healthcare options to retirement savings, you can have peace of mind that your personal and financial well-being is protected.

Compensation

Base Salary Range: USD $85,500 – $111,250.00 annually.

Target Annual Bonus: % Varies based on level of role.

Individual compensation packages are based on various factors unique to each candidate’s skill set, experience, qualifications, equity, and other job-related reasons.

Note: USP does not accept unsolicited resumes from 3rd party recruitment agencies and is not responsible for fees from recruiters or other agencies except under specific written agreement with USP.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities

This employer is required to notify all applicants of their rights pursuant to federal employment laws.

For further information, please review the Know Your Rights (https://www.eeoc.gov/poster) notice from the Department of Labor.

Job Category Information Technology

Job Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/ff3d497f1bae4049b0c33423ab347a65tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['SEO management', 'Data analysis', 'Data visualization skill', 'Blogging skill', 'Online Marketing skill', 'Strategy Foundation Development skill', 'Online Influencing skill', 'Interactive Content skill', 'Social Media Marketing skill', 'Creative Writing skill']"
12,DS Technologies Inc,https://www.linkedin.com/company/ds-technologies-inc-america/life,Sr. Data Information Engineer,https://www.linkedin.com/jobs/view/4258508971,4258508971,United States,Remote,,2025-06-27 19:41:19,True,48 applicants,"About US: We are a company that provides innovative, transformative IT services and solutions. We are passionate about helping our clients achieve their goals and exceed their expectations. We strive to provide the best possible experience for our clients and employees. We are committed to continuous improvement and innovation, and we are always looking for ways to improve our services and solutions. We believe in working collaboratively with our clients and employees to achieve success.

DS Technologies Inc is looking for Sr. Data Information Engineer role for one of our premier clients.

Job Title:Sr. Data Information Engineer

Location:Washington, DC (Partial Remote; Must Reside Locally)

Industry:Transportation / Rail Systems

Job Category:Data Architecture / Information Engineering

Overview

Amtrak is seeking a seasoned Sr. Data Information Engineer to lead enterprise-level data architecture and modeling initiatives supporting Transportation Operations systems. The role involves cross-functional collaboration, API and data contract design, and ensuring performance, integrity, and security across distributed and cloud-based platforms.

Position

Contract (July 21, 2025 – June 30, 2026)

Hybrid – 2–3 days onsite

Responsibilities

Architect and standardize data, event, and API contracts for transportation systems. Collaborate with engineering, architecture, and business teams to align data solutions with enterprise strategies. Lead performance tuning, indexing, and optimization for relational and NoSQL databases. Conduct architecture reviews and enforce database management standards. Apply industry trends and innovative technologies to advance enterprise data strategies. 

Requirement Qualifications

15+ years in enterprise data architecture and information modeling. Strong knowledge of relational, NoSQL, and streaming data systems. Experience with event-driven architectures and API integrations. Expertise in data modeling (conceptual, logical, physical). Proficient in SQL tuning and performance optimization. Experience in distributed/cloud-based ecosystems (AWS, Azure, GCP). Background in transportation or real-time operational systems is a plus. 

Additional Information

Must be local to the Washington, DC area. Must be US Citizen or Green Card holder. Role requires strong communication, leadership, and cross-team collaboration skills. 

Candidate Details

Ideal roles: Enterprise Data Architect, Data Platform Lead, Information Systems Engineer Preferred industry background: Transportation, Rail, Aviation, Smart Infrastructure Desired soft skills: Enterprise design thinking, mentorship, clear communication 

If you are interested, Kindly share your resume to

Ayan@dstechnologiesinc.com

Thanks

DS Technologies - Team",https://www.linkedin.com/job-apply/4258508971,"['python', 'sql', 'Apache Spark', 'SQL Server', 'NoSQL databases', 'Amazon Web Services (AWS)', 'Snowflake', 'Git', 'Docker']"
13,"Global Soft Systems, Inc.",https://www.linkedin.com/company/global-soft-systems/life,Data Engineer - DBT/Snowflake - Ops Team,https://www.linkedin.com/jobs/view/4258499766,4258499766,United States,Remote,,2025-06-27 18:47:20,True,over 100 applicants,"Long Term

Remote

2X Engineer, Legacy Operations Insurance

Qual Template

Overall Initiative For All Four Openings 

This is a legacy foundation upgrade and a new build.Background: Recently migrated data into snowflake that was run on prem and they need to solidify areas focused on core data elements around providers and member population and how that all integrates other systems.Current flow of the system is sending data into vendors (Milliman Insights) and into their instances of Epic. These two pathways are putting a strain on existing world, in addition to the remaining analytics they are putting on top of this. Thus, showing cracks in the systems.Solution: rebuild, start with a blank slate in some cases, while also keeping existing systems running. To do this, they have been shuffling some people around to make this work, so there will be backfills, and net new openings.Some of these roles will be supporting their legacy operations, and some of the roles will be working on the teams doing the net new builds
﻿

Project Scope (dig Into Details Here)

Will be supporting existing operational work 

﻿

Why is req open?

Backfill opening for legacy operations team

﻿

Must-Haves (Concepts & Tools) 

DBTSnowflake (data warehouse)Data ingestion with Azure Data factoryData analysis, root cause analysisMust be able to trouble shoot, identify issue, do the analysis and propose a solution.See and issue and do validation.Senior level 
What Will Win (formerly Known As “Nice-to-Haves”)

Payer/ population health- insurance experience (united health, cigna, blue cross)Operations background",https://www.linkedin.com/job-apply/4258499766,"['Python', 'SQL', 'Docker', 'AWS', 'Kubernetes', 'Data Analysis', 'Leadership', 'Communication', 'Project Management', 'Agile Methodology']"
14,Lensa,https://www.linkedin.com/company/lensa/life,Principal Engineer Data Engineering - US Remote,https://www.linkedin.com/jobs/view/4256755689,4256755689,"Virginia Beach, VA",Remote,,2025-06-27 13:54:03,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Anywhere Real Estate.

Anywhere is at the forefront of driving the digital transformation and building best-in-class products that help our agents and brokers sell more homes, make more money, and work more efficiently.

Data & Analytics (DNA) is Anywhere's data arm. We create innovative analytics, data science, and robust data foundation capabilities to generate data-driven insights that serve the heart of Anywhere Advisor and Anywhere Brand business. Together with our business counterparts in the real estate business, we work daily to deliver differentiating insights (AI & BI) for Strategy and AA & AB Operations.

We're seeking a Principal Engineer to join our Data Platform Team. In this critical position, you'll be responsible for designing, implementing, and managing the data infrastructure. You will work closely with data scientists, software engineers, and other stakeholders to ensure the Data Platform's availability, usability, and integrity.

Data Infrastructure Design And Implementation

Evaluate, select, and implement new tools and frameworks to expand our data platform capabilities. Design, build, and maintain robust, scalable, and reliable data pipelines and ETL processes. Develop and maintain data infrastructure and platforms using various technologies (e.g., AWS, Snowflake Cloud Platforms, databases, Kafka streaming platforms). Ensure data quality, consistency, and integrity across the organization. Architect and optimize Data Ingestion and Snowflake ETLs. Production Support and enhancements to the observability of the Data Platform. 

Team Leadership And Mentorship

Lead and mentor data engineers, providing guidance and support to junior engineers. Foster a culture of technical excellence and continuous learning. Collaborate with other teams (e.g., data scientists, software engineers, and product managers) to ensure data solutions meet business needs. 

Data Security And Compliance

Implement and maintain data security measures to protect sensitive data. Ensure compliance with data protection regulations and industry standards. 

Problem Solving And Innovation

Identify and solve complex data-related problems. Stay abreast of industry trends and emerging technologies and identify opportunities to enhance data capabilities. Proactively address performance, scale, complexity, and security considerations. 

Skills And Qualifications

Technical Expertise:

10+ years’ experience with a strong understanding of data engineering principles and technologies. 10+ years’ experience with data pipelines, ETL processes, and data warehousing. 5+ years’ experience building data pipelines using Kafka, Kafka Connect, Airflow, and Snowflake. 5+ years’ experience with Snowflake Data Platform. 5+ years’ experience with AWS Data Services such as DMS, EMR, Glue, Athena, S3, CloudWatch, Lambda, or IAM. 5+ years’ experience with Data Quality, Data Reconciliation 5+ years’ experience managing production data platforms. 5+ years’ experience building observability (Monitoring & Alerting) using tools such as Data Dog and M. Proficiency in programming languages (e.g., Java, Python, SQL). Knowledge of data governance, data modeling, and security best practices. Proficiency in CI/CD, IAC, and Agile Development. 

Leadership And Communication

Strong leadership and mentoring skills. Excellent communication and collaboration skills. Ability to explain complex technical concepts to both technical and non-technical audiences. 

Problem-Solving And Analytical Skills

Ability to identify and solve complex problems. Strong analytical skills to identify data quality issues and performance bottlenecks. 

Anywhere Real Estate Inc. (http://www.anywhere.re/)   (NYSE: HOUS) is moving real estate to what's next. Home to some of the most recognized brands in real estate Better Homes and Gardens® Real Estate (https://www.bhgre.com/) , Century 21® (https://www.century21.com/) , Coldwell Banker® (https://www.coldwellbanker.com/) , Coldwell Banker Commercial® (https://www.cbcworldwide.com/) , Corcoran® (https://www.corcoran.com/) , ERA® (https://www.era.com/) , and Sotheby's International Realty® (https://www.sothebysrealty.com/eng) , we fulfill our purpose to empower everyone's next move through our leading integrated services, which include franchise, brokerage, relocation, and title and settlement businesses, as well as mortgage and title insurance underwriter minority owned joint ventures. Anywhere supports nearly 1 million home sale transactions annually and our portfolio of industry-leading brands turns houses into homes in more than 118 countries and territories across the world.

At Anywhere, we are empowering everyone’s next move – your career included. What differentiates us is our scale, expertise, network, and unique business model that positions us as a trusted advisor throughout every stage of the real estate transaction. We pursue talent – strategic thinkers who are eager to always find a better way, relentlessly focus on talent, obsess about growth, and achieve exceptional results. We value our people-first culture, which thrives on empowerment, innovation, and cross-company collaboration as we keep moving the world forward, together. Read more about our company culture and values in our annual Impact Report (https://anywhere.re/wp-content/uploads/2025/03/2024-Impact-Report.pdf) .

We are proud of our award-winning culture and are consistently recognized as an employer of choice by various organizations including:

Great Place to Work Forbes World's Best Employers Newsweek World's Most Trustworthy Companies Ethisphere World's Most Ethical Companies 

EEO Statement: EOE including disability/veteran

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/1279aa1ac8554f948461335887ea674etjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['communication ', 'managerial ', 'problem-solving ', 'analytical thinking ', 'time management ', 'collaboration ', 'customer support ', 'technical writing']"
15,Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Data Engineer / Architect,https://www.linkedin.com/jobs/view/4257033271,4257033271,United States,Remote,,2025-06-27 20:19:56,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Marici Solutions, is seeking the following. Apply via Dice today!

Data Warehouse Engineer / Architect | Location: Remote | 

Mandatory Skills

The candidate's overall IT experience should not exceed 15 years. The duration will be calculated from the date of graduation or post-graduation whichever is earlier and from the start date of their first project. If the total experience exceeds 15 years, the profile will not be considered.Hands-on coding is mandatory. During the first round of interviews, the panel will require the candidate to write code using Spark with Python, so the candidate should be well-prepared.Additionally, the candidate must provide a brief summary of their experience with Snowflake, Databricks, BigQuery, and Data Fabric, including the percentage of hands-on coding involvement in their most recent project.

Job Description:

Ideal candidate should be 80% technical & rest 20% be able to work customer & offshore team members

Cloud DWs: Snowflake / Databricks / BigQuery / Data Fabric / Redshift

Mandatory Skills:

 SQL Python

Spark

Should be good at Performance Tuning

Have Data Modelling experience / knowledge

Experience on at least one cloud platform (AWS / Azure / Google Cloud Platform)

Preferably have experience working on large-scale & long running engagements

Have an engg. bent of mind & should have experience working with offshore teams",https://click.appcast.io/t/uLTmLHZUWh6dWei7_b0uXdkowYEkb487I9cw8bMlGrc=,"['python', 'mysql', 'Jenkins', 'Git', 'SSH', 'AWS CloudFormation', 'Amazon ECS', 'Docker', 'AWS CloudTrail', 'GitOps']"
16,Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Engineer, New Grad & Entry Level",https://www.linkedin.com/jobs/view/4257046836,4257046836,"Illinois, United States",Hybrid,,2025-06-27 22:20:49,False,,"Position OverviewA Data Engineer in Cognizant’s AIA (AI & Analytics) practice brings relevant context to data for business and IT for intelligent systems and critical decisions. They work within the domains of data management, to integrate high-variation and/or high-velocity data to produce answers and enable clients to ask an entirely new class of questions. They will use a variety of tools and techniques to deliver better, faster, and intelligent data to some of the world’s most successful businesses – our clients.
ResponsibilitiesBuild and deliver data pipelines to store data in a way that is accessible, performant, secure, and sustainablePrototype solutions, prepare test scripts, and conduct tests for data replication, extraction, loading, cleansing, and data modeling for data warehousesReview and validate data loaded into data lakes/warehouses for accuracyDevelop proofs of concept and evaluate design options to deliver ingestion, search, metadata cataloging and scheduling of data pipelinesData engineering in line with standard processes and Cognizant’s reference architectureUnderstand and detail technical use case requirements to deliver data movement and transformation solutions
Basic QualificationsBachelor's degree in IT-related field and 0-3 years of IT experienceStrong business interpersonal skills (including written and oral)Programming experience with Python, Python Pyspark or Java development for modern data engineeringStrong background in relational data modelsExposure in Data Pipelines & SQL / NoSQL (e.g. Cassandra, HBase) and Relational Database management systemsExposure to implementing PaaS services on public clouds - Azure, AWS, GCP, Databricks, Snowflake, SingleStore, Oracle, InformaticaStrong problem solving and analytical thinking skills
Distinguishing QualificationsFull Data lifecycle development experienceIndustry experience (financial services, insurance, retail, healthcare, life sciences, communications)Experience in developing and deploying distributed computing applications using Open Source or cloud native services.Shown understanding of data applications including Hadoop components (HDFS, HBase, Hive, Sqoop, Flume, etc.)Exposure to data analytics and data mining tools/applications such as Tableau, QlikSense /QlikView, R or SASExposure to working with SQL, Relational Database Management Systems (e.g., SQL Server, Oracle)Development workflows (e.g., Microsoft VSTS)Experience leading teams",https://jobright.ai/jobs/info/685b32d6ba82630cedcfa039?utm_source=1124&utm_campaign=685b32d6ba82630cedcfa039&tob=true,"['natural language processing', 'systems design', 'security analysis', 'data modeling', 'cloud computing', 'project management', 'team collaboration', 'Agile methodologies', 'front-end development', 'database management']"
17,Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256759174,4256759174,"Baton Rouge, LA",Remote,$90K/yr - $110K/yr,2025-06-27 13:53:57,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/8fa395f20ca7463b935d5c6bbd5b5c7atjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['big data ', 'python ', 'sql ', 'AWS ', 'cloud computing ', 'docker ', 'engineering ', 'continuous integration ', 'user interface design ', 'Agile methodology']"
18,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256759164,4256759164,"Austin, TX",Remote,$118K/yr - $122K/yr,2025-06-27 13:53:58,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/68c023b700004bdca0d3c5dbba3fecf3tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'SQL', 'container technology', 'AWS', 'snowflake', 'communication', 'collaboration', 'problem-solving', 'project management', 'customer service']"
19,Tenth Revolution Group,https://www.linkedin.com/company/tenthrevolution/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4255852073,4255852073,"Addison, TX",On-site,$100K/yr - $130K/yr,2025-06-27 17:58:05,True,over 100 applicants,"Build, Optimize, and Automate Analytics Pipelines in a leading Healthcare organization
A leading organization in the Healthcare space are looking to add a Senior Data Engineer to the Data Analytics division. Data Analytics is a key pillar of the organization and the successful candidate will be joining a team focused on the design and optimization of robust, end-to-end data pipelines focused on clinical analytics. You’ll turn structured and unstructured raw data into clean, analytics-ready datasets, ideal for BI dashboards and machine-learning models. If you are passionate about, and able to demonstrate a strong background of experience, solving data-movement puzzles and automating data processes, this could be the next exciting opportunity for you.
Successful candidates, during the interview, must be able to demonstrate professional experience in the below:Write and tune complex SQL queries, transforming data from raw to analytics ready.Develop robust ETL workflows using SQL Server/SSIS and Python.Containerize and deploy workflows using Docker within Azure-based environments (AWS or GCP experience a plus).Manage and version-control code effectively using Git.Automate end-to-end data processes, supporting analytics and machine-learning model requirements.
Successful applicants will bring, and be able to demonstrate, experience across the following Tech stack:SQL, SQL Server/SSIS, Python, Docker, Git, Azure (AWS or GCP a bonus), structured and unstructured data handling, ETL automation.
This role is focused on hands-on, technical problem-solving, ideal for someone passionate about delivering reliable, scalable analytics data infrastructure.
This position is onsite at the organizations Addison, TX offices.",https://www.linkedin.com/job-apply/4255852073,"['python', 'java', 'automation', 'cloud computing', 'AI/ML', 'Agile methodologies', 'Cybersecurity', 'Project Management', 'Leadership']"
20,Lensa,https://www.linkedin.com/company/lensa/life,Tableau Developer,https://www.linkedin.com/jobs/view/4258450481,4258450481,"Myrtle Point, OR",Remote,$30/hr - $45/hr,2025-06-27 14:07:02,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Actalent.

Job Title: Tableau DeveloperJob Description

This role involves creating cloud dashboards by analyzing data to deliver business insights for cross-functional team leaders. You will derive actionable insights from complex data and present these findings to stakeholders. Additionally, you will create tools, processes, mechanisms, and automation to reduce the burden on our data stack while meeting stakeholder requirements.

Responsibilities

Create cloud dashboards and analyze data to deliver business insights. Derive actionable insights from complex data and present findings to stakeholders. Develop tools, processes, mechanisms, and automation to enhance the data stack. Build and code extensions to enhance dashboard functionality. Interact with the environment using Extensions API. 

Essential Skills

Proficiency in Tableau and dashboard development. Experience with cloud technologies, particularly with 2+ years in cloud experience. 6+ years of experience in dashboard building. 3+ years of knowledge in Marketing/Sales KPIs. Experience with Salesforce, specifically for marketing modules. 1+ year experience in creating cloud extensions. Proficiency in HTML, JavaScript, and APIs. 

Additional Skills & Qualifications

Bachelor's degree in Computer Science (Data Engineering is a plus). Financial KPIs knowledge preferred. Experience with ERP systems and Oracle is beneficial. Must be located in the US. Familiarity with PST time zone is preferred. 

Work Environment

This position is remote, with a preference for candidates in the PST time zone. The role is crucial for maintaining cybersecurity governance in national security and critical infrastructure markets, ensuring certifications and regulatory obligations are met. The role supports balancing workloads across teams and positions the function for long-term stability.

Pay and Benefits

The pay range for this position is $30.00 - $45.00/hr.

Workplace Type

This is a fully remote position.

About Actalent

Actalent is a global leader in engineering and sciences services and talent solutions. We help visionary companies advance their engineering and science initiatives through access to specialized experts who drive scale, innovation and speed to market. With a network of almost 30,000 consultants and more than 4,500 clients across the U.S., Canada, Asia and Europe, Actalent serves many of the Fortune 500.

The company is an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.

If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing due to a disability, please email actalentaccommodation@actalentservices.com (%20actalentaccommodation@actalentservices.com) for other accommodation options.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/5018c818bae7483c96b56b9a938e5c10tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['teamwork ', 'communication ', 'problem-solving ', 'productivity ', 'analytical ', 'innovation ', 'leadership ', 'customer service ', 'adaptability ', 'attention to detail']"
21,Cortwo,https://www.linkedin.com/company/cortwo/life,Senior System Engineer,https://www.linkedin.com/jobs/view/4257050325,4257050325,"Dallas, TX",On-site,,2025-06-27 22:14:57,True,10 applicants,"Location: Dallas, TX (100% On-Site)
About Us
Cortwo is re-architecting the internet by embedding trust into its core. We’re not just upgrading the stack—we’re rebuilding it, with identity and security as first principles. In a world of rising threats and eroding digital trust, we believe a proactive, intelligent foundation isn’t optional—it’s inevitable.
We are seeking a highly skilled Senior Systems Engineer with deep hands-on expertise with Windows and Linux across compute, storage, systems engineering, automation, and security. The ideal candidate will have experience deploying and supporting systems that handle 1 billion+ I/O transactions per day and working within large-scale environments. This role requires strong knowledge of Unix/Linux and Windows systems, data center infrastructure, cloud deployments (AWS preferred), and robust operations support.
Key Responsibilities
Design, deploy, and support enterprise-grade compute and storage solutions, ensuring high availability, scalability, redundancy and performance.Manage and optimize Unix/Linux and Windows server environments, including shell scripting and automation to improve operational efficiency.Lead systems integration projects involving complex architectures and hybrid environments.Implement and maintain security configurations and compliance frameworks in line with industry best practices.Build & support cluster management, virtualization (VMs), and cloud-based deployments with strong focus on HA & DR strategies.Plan and execute large-scale data center migrations and cloud onboarding.Collaborate cross-functionally with network, database, and application teams to deliver integrated solutions, Agile/ DevOps and CI/CD pipelines.Drive continuous improvement initiatives around site reliability, performance engineering, and operational excellence.Participate in 24/7/365 support rotations and incident management as needed.Document system architectures, standard operating procedures (SOPs), and technical presentations for diverse stakeholders.
Qualifications
Bachelor’s degree in Engineering, Computer Science, or related technical field (or equivalent experience).8+ years of hands-on experience in systems engineering roles supporting large enterprise or service provider environments.Proven track record managing compute and storage infrastructure in large scale high demand environmentsStrong proficiency with Unix/Linux (RedHat, CentOS, Ubuntu, etc.) and Windows Server platforms.Expertise in shell scripting and automation tools (e.g., Bash, Python, Ansible, PowerShell).Solid understanding of server and storage hardware architectures (e.g., SAN/NAS, RAID, SSD/HDD technologies).Experience with virtualization technologies (VMware, Hyper-V, KVM) and cluster management.Deep knowledge of cloud infrastructure, preferably AWS (VPC, EC2, EBS, CloudFormation, etc.).Experience with Nutanix HCIFamiliarity with HA & DR design and operational practices.Hands-on experience with data center deployments, migrations, and operations at scale.Experience working with technology products/solutions from vendors such as HPE, Dell, IBM, or equivalent.Knowledge of security principles and compliance standards.Excellent communication, presentation, and documentation skills.Positive, proactive attitude with the ability to work independently and collaboratively in a fast-paced, high-growth environment.
Bonus Points
 Linux Professional Institute Certification (LPIC), RedHat Certified Engineer (RHCE), or equivalent.Microsoft Certified: Azure Administrator / Windows Server.AWS Certified Solutions Architect – Associate or Professional.VMware Certified Professional (VCP).ITIL/ ITSM Foundation or other service management certifications.
Why Dallas? Why In-Person?
Cortwo is building a new kind of intelligence—one that lives in the bloodstream of the internet. This work demands high-context, rapid feedback, and deep cross-functional collaboration. Our team thrives in a high-bandwidth, in-person environment in Dallas, where mission meets momentum every day.
Benefits & Perks
Competitive salary and performance-based bonuses.Comprehensive health, dental, and vision insurance.401(k) plan with company match.Unlimited paid time off.On-site gym.Daily lunch.
We welcome applications from all qualified candidates who are authorized to work in the U.S. However, we are unable to provide visa sponsorship at this time.
Cortwo Corp. is an equal opportunity employer. Cortwo complies with all applicable federal, state, and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state, or local laws. No phone calls or agencies please.",https://www.linkedin.com/job-apply/4257050325,"['sql', 'container', 'AWS', 'Snowflake', 'Python', 'Marketing', 'Scheduling', 'Automation', 'Performance Testing', 'Agile Methodology']"
22,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256757401,4256757401,"Lincoln, NE",Remote,$118K/yr - $122K/yr,2025-06-27 13:54:02,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/f6681919128a4452b4f3ad890571df64tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'data analytics', 'project management', 'communication', 'teamwork', 'agile', 'problem-solving', 'marketing', 'sales', 'leadership']"
23,Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Remote role - Data Engineer-AWS Cloud,https://www.linkedin.com/jobs/view/4257032574,4257032574,United States,Remote,,2025-06-27 20:19:50,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Connexions Data Inc, is seeking the following. Apply via Dice today!

Data Engineer - AWS Cloud services Remote role 6 months

Bachelor s Degree in Computer Science, Information Technology or Computer Engineering or related field or 4 years relevant experience will be considered in lieu of a degree.

5+ years experience in:

Data Engineering, with a focus on AWS Cloud services. Proficiency in Amazon Web Services (AWS)

Data Warehouse (Redshift)

Database (RDS, DynamoDB)

Visualization (Quicksight) Storage (S3)

Other (Glue, Athena, CloudWatch)

Strong experience with SQL and NoSQL database technologies.

Experience with data warehousing concepts and ETL processes.

Experience with data modeling, data warehousing, and big data technologies.

Experience with Encryption, Public Key Infrastructure (PKI), security best practices and compliance requirements.

Excellent problem-solving skills and attention to detail.

Excellent verbal and written communication skills.",https://click.appcast.io/t/csWzhoI90-l2r6QFPfhmqGCzRmH74zPXsIH34NiaE5s=,"['python', 'sql', 'container', 'AWS', 'Snowflake']"
24,Hunter Bond,https://www.linkedin.com/company/hunter-bond/life,"Data Centre Engineer – Elite Quant - Up to $150,000 Starting base + Exceptional benefits/bonus package Fund – New York",https://www.linkedin.com/jobs/view/4255845438,4255845438,"New York, United States",Hybrid,$150K/yr,2025-06-27 18:14:44,True,26 applicants,"Title: Data Center EngineerClient: Quant Fund – Global collaborative firm run by passionate Computer ScientistsSalary: up to $150,000 + bonus + package/perksLocation: New Jersey (Hybrid/Remote)
My client is a dynamic global quant fund that is building cutting-edge systems. In this position you will be responsible for managing the company's cutting-edge Trading Infrastructure, which spans several colocated data centers and thousands of servers. This role focuses on enhancing the efficiency, speed, and reliability of the infrastructure.
The successful candidate will have the following skills/experience;✔️ Managing Infrastructure space, power, electrical systems, security, cabling, facilities – all things Datacentre✔️ Liaising with multiple teams to design and deploy Datacentre’s and associated facilities✔️ Managing build and construction Datacentre projects as per agreement with key stakeholders✔️ Capacity management✔️ Bachelors/Associates Degree

Please apply with your updated CV or email at hrubin@hunterbond.com.",,"['algorithm JSKSKISSISSIPSSIPSH', 'Input:', 'T (""developer"": ""AWS', 'Java', 'SQL', 'Spring."")', 'Output:', 'AWS', 'Java', 'SQL', 'Spring']"
25,Millennium Software and Staffing,https://www.linkedin.com/company/millennium-software-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4248175921,4248175921,"Cincinnati, OH",Remote,,2025-06-12 17:02:30,True,over 100 applicants,"Hi Professional,
We have a W2 job opportunity, and we are currently looking for Data Engineer ,
Job Description: Requirements:• PL/SQL• Python• AWS Redshift",https://www.linkedin.com/job-apply/4248175921,"['JavaScript', 'Python', 'Java', 'SQL', 'AWS', 'Electric Vehicle', 'IoT', 'Data Analysis', 'Project Management', 'Agile Development']"
26,Lensa,https://www.linkedin.com/company/lensa/life,Distinguished Data Engineer - Capital One Software (Remote),https://www.linkedin.com/jobs/view/4256761046,4256761046,"Richmond, VA",Remote,,2025-06-27 13:53:55,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Capital One.

Distinguished Data Engineer - Capital One Software (Remote)

Ever since our first credit card customer in 1994, Capital One has recognized that technology and data can enable even large companies to be innovative and personalized. As one of the first large enterprises to go all-in on the public cloud, Capital One needed to build cloud and data management tools that didn’t exist in the marketplace to enable us to operate at scale in the cloud. And in 2022, we publicly announced Capital One Software and brought our first B2B software solution, Slingshot, to market.

Building on Capital One’s pioneering adoption of modern cloud and data capabilities, Capital One Software is helping accelerate the data management journey at scale for businesses operating in the cloud. If you think of the kind of challenges that companies face – things like data publishing, data consumption, data governance, and infrastructure management – we’ve built tools to address these various needs along the way. Capital One Software will continue to explore where we can bring our solutions to market to help other businesses address these same needs going forward.

We are seeking top tier talent to join our pioneering team and propel us towards our destination. You will be joining a team of innovative product, tech, and design leaders that tirelessly seek to question the status quo. As a Capital One Distinguished Data Engineer, you’ll have the opportunity to be on the forefront of building this business and bring these tools to market.

Responsibilities

Build awareness, increase knowledge and drive adoption of modern technologies, sharing consumer and engineering benefits to gain buy-in Collaborate on Capital One’s toughest issues, to deliver on business needs that directly impact the lives of our customers and associates Strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader Capital One team Promote a culture of engineering excellence, using opportunities to reuse and innersource solutions where possible Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization Operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in an unified manner Lead the way in creating next-generation talent for Tech, mentoring internal talent and actively recruiting external talent to bolster Capital One’s Tech talent 

Basic Qualifications

Bachelor’s Degree At least 7 years of experience in data engineering At least 5 years of experience in data architecture 

Preferred Qualifications

Masters’ Degree 7+ years of experience using Python, Java, Go, and SQL 3+ years of experience with Kafka, Airflow, Spark, AWS Glue/Kinesis 3+ years of experience with Databricks or Snowflake 1+ year of experience deploying machine learning models 

At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).

The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.

Remote (Regardless of Location): $239,900 - $273,800 for Distinguished Data Engineer

Richmond, VA: $239,900 - $273,800 for Distinguished Data Engineer

Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.

This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.

Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website (https://www.capitalonecareers.com/benefits) . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.

This role is expected to accept applications for a minimum of 5 business days.

No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.

For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com

Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.

Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/7e186ba69a37440d85dbfc6a446bc942tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Python', 'SQL', 'Cloud Computing', 'DevOps', 'GitHub', 'Agile Project Management', 'AWS', 'Snowflake', 'Docker/Containerization', 'Linux Administration']"
27,Clear Capital,https://www.linkedin.com/company/clearcapital/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4257010553,4257010553,"Reno, NV",Remote,,2025-06-27 18:10:22,False,,"Clear Capital is building the future of real estate data, and we need your help! We are seeking experienced product builders: with your talent as a Senior Data Engineer, help us reach our goals of knowing more about a property than anybody else and in the process, making home ownership valuations more fair and equitable for millions of people.
Become part of an innovative team supporting and developing the data and machine learning products that will shape Clear Capital’s future. The Senior Data Engineer role at Clear Capital will involve working closely with product teams to build next-generation data products. Working alongside Software Engineers, Data Quality Analysts, ML Engineers, Data Scientists, and ML Ops Engineers who, like you, are dedicated to building exceptional data products.
We are looking for a Senior Data Engineer to assist with the development and implementation of systems leveraging structured and unstructured data to deliver data and data science solutions at scale. As Senior Data Engineer at Clear Capital, you are committed to enabling the best work of others on the team. You help yourself and your team to consistently “level up.” You think ahead to anticipate the needs of others and provide concise information for decision-making.
What You Will Work OnDesign, develop, and maintain Extract Transform Load (ETL) data pipelines.Understand, operate, and improve legacy data pipelines that underpin our existing business operations.Provide direct support to Machine Learning, Business Intelligence, and Property Data Operations teams.
Who We Are Looking ForQualified candidates should have a Bachelor’s degree or relevant experience focused on information technology and 4+ years of overall experience.4+ years developing in PythonUnderstanding of data warehouse, data lakes, and master data management approaches, ETL industry standards, and best practicesExperience creating and maintaining scalable and robust AWS solutionsSolid understanding of data warehousing concepts and hands-on experience with relational databases (e.g., PostgreSQL, MySQL) and columnar databases (e.g., Redshift, Snowflake)Firm grasp of Linux, including BASH scriptingExperience with Airflow, DBTDeep knowledge of core AWS services, especially AWS Glue, AWS Batch, RDS, EMR, EKS, Lambda, and Step FunctionsExperience with containerization and related technologies (e.g., Docker, Kubernetes)
What You Can ExpectCompetitive compensation and immediate contribution!Inclusive benefits package offerings 401k plans and customizable benefits including dental, vision, medical, etc. for you and your dependents.An innovative culture that understands the importance of quality of work over quantity.Company supported and employee-driven ambassador groups that promote diversity, working on a hybrid schedule and philanthropy.Learning and development programs to help advance your career and personal growth.What We ValueWherever it leads, Whatever it takes! We believe in making the impossible possible!Thrive personally, grow professionally―be happy!Innovate, learn, lead- Knowledge and growth is never-ending!We believe in hiring nice people because anything is possible when you have the team's support.Improving the lives around us- A smile could change the entire world.Be the most trusted, respected, and loved real estate valuation company in the world.
About Us
Clear Capital is a national real estate valuation technology company with a simple purpose: build confidence in real estate decisions to strengthen communities and improve lives. Our goal is to provide customers with a complete understanding of every U.S. property through our field valuation services and analytics tools, and improve their workflows with our platform technologies. Our commitment to excellence — wherever it leads, whatever it takes® — is embodied by team members.
Clear Capital is an equal-opportunity employer.
To all recruitment agencies: Clear Capital does not accept agency resumes. Please do not forward resumes to our jobs alias, Clear Capital employees, or any other company location. Clear Capital is not responsible for any fees related to unsolicited",https://jobs.lever.co/clearcapital/07a1241b-100a-49ae-83c3-5727120bb3d9,"['python', 'salesforce', 'resilience', 'report writing', 'teamwork', 'time management', 'accounting practices', 'blueprint design', 'customer service', 'AWS']"
28,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256759268,4256759268,"Juneau, AK",Remote,$118K/yr - $122K/yr,2025-06-27 13:53:48,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/cc33b93fcf07423d946d4ecbeec29cbctjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'SQL', 'Java', 'Docker', 'AWS', 'Cloud Computing', 'Data Engineering', 'Tableau', 'SQL', 'Containerization']"
29,Tire Rack,https://www.linkedin.com/company/tire-rack/life,Data Engineer II,https://www.linkedin.com/jobs/view/4257045875,4257045875,South Bend-Mishawaka Region,Remote,,2025-06-27 22:17:57,True,over 100 applicants,"Tire Rack is embarking on a modernization journey, evolving from a legacy data environment to a scalable, cloud-native data ecosystem. We are a forward-thinking, data-driven organization committed to leveraging modern architecture and engineering practices to deliver innovative solutions across the e-Commerce sector. Join us at a pivotal time as we transform how data powers our decisions, products, and services.
We are seeking a Data Engineer to help drive our digital transformation. In this role, you’ll play a key part in re-architecting legacy systems, building cloud-first data pipelines, and modernizing how we collect, store, and deliver data across the company. This is a hands-on, impact-driven opportunity to shape scalable solutions from the ground up and enable analytics and intelligence in a rapidly evolving environment. You’ll work closely with engineers, analysts, and stakeholders to build the foundation for next-generation data platforms.
Required QualificationsBachelor’s or Master’s degree in Computer Science, Engineering, or a related field or the equivalent through a combination of education and related work experience.3 years minimum related work experience.Strong understanding of data modeling, schema design, ETL/ELT, and data warehousing concepts.Proficient in SQL and scripting/programming languages such as Python, Java, or Scala.Experience with relational and non-relational databases (e.g., PostgreSQL, DB2, NoSQL).Knowledge of cloud-based data platforms such as AWS, GCP, or Azure.Familiarity with CI/CD, Git, containerization (Docker), and orchestration tools (e.g., Airflow, Dagster).Experience working in shared service, hybrid environments.Ability to work creatively and analytically in meeting team needs.Ability to ramp up quickly in an evolving and challenging role.Well-honed troubleshooting skills at various levels of complexity.Must be authorized to work in the U.S. – we are unable to sponsor employment visas at this time
Preferred QualificationsExperience with data orchestration tools.Hands-on experience with CDC frameworks (e.g., Debezium, Fivetran).Experience with modern data engineering tools and technologies (e.g., Spark, Snowflake, Databricks).Familiarity with legacy-to-cloud migration strategies, especially involving mainframe systems (e.g., IBM i, z/OS).Exposure to Lakehouse architectures, including tools like dbt, Delta Lake, and related ecosystem technologies.Understanding of data governance, RBAC, and privacy best practices in large-scale environments.Experience with BI/Visualization platforms such as Tableau, Power BI, or AWS QuickSight.",https://www.linkedin.com/job-apply/4257045875,"['python', 'SQL', 'containerization', 'AWS', 'snowflake', 'cloud services', 'Repositories', 'Communication', 'Problem-solving', 'Teamwork']"
30,Jobot,https://www.linkedin.com/company/jobot/life,Data Engineer,https://www.linkedin.com/jobs/view/4255826114,4255826114,"Hopkinsville, KY",Remote,$80K/yr - $100K/yr,2025-06-27 15:19:48,True,38 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Fully remote Data Engineer opportunity // Data Modeling experience required!

This Jobot Job is hosted by Craig Rosecrans

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $80,000 - $100,000 per year

A Bit About Us

We are currently seeking a seasoned Data Engineer to join our dynamic Tech Services team. The ideal candidate will be a data enthusiast with a solid understanding of the latest data technologies and trends coupled with a strong desire to deliver top-notch solutions to our clients. The successful candidate will be responsible for designing, developing, and maintaining data architectures, databases, and processing systems. With a focus on ETL, Data Modeling, SSIS, Tableau, and PowerBI, this role will be integral in transforming data into readable, goal-driven reports for continued innovation and growth.



Why join us?


 Competitive Base Salary Company paid health plan for employees Flexible Hours Very generous PTO Dental and Vision, FSA, HSA Small team, autonomy Many more great perks!

Job Details

Responsibilities

 Design, develop, automate, and maintain productivity tools using programming, database or scripting languages to improve software modeling and development. Design and implement ETL procedures for intake of data from both internal and outside sources; as well as ensure data is verified and quality is checked. Collaborate with data architects, modelers and IT team members on project goals. Develop and maintain databases by acquiring data from primary and secondary sources, and build scripts that will make our data evaluation process more flexible or scalable across data sets. Employ a variety of languages and tools to marry systems together or try to hunt down opportunities to acquire new data from other systems. Develop and implement data standards, ensuring data quality and consistency. Perform data profiling to identify and understand anomalies. Present information using data visualization techniques through Tableau and PowerBI. Work closely with team members, clients, project managers, and other stakeholders to ensure solutions are delivered timely and accurately.

Qualifications

 Bachelor's degree in Computer Science, Information Systems, or a related field. A minimum of 5+ years of experience in a data engineering role with a proven record of successful data manipulation. Proficiency with ETL tools, SSIS, and experience with SQL/NoSQL databases. Knowledge and experience in data modeling. Experience with business intelligence tools like Tableau, PowerBI or similar. Strong problem-solving skills, attention to detail, and ability to think critically. Excellent written and verbal communication skills, with the ability to present complex data in a clear and concise manner. Ability to work independently and with team members from different backgrounds. Excellent organizational skills and the ability to manage multiple tasks concurrently. A strong desire to learn and develop new skills, staying up to date with the latest data best practices and technologies. Ability to work in a fast-paced, deadline-driven work environment.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255826114,"['python', 'sql', 'container', 'AWS', 'cloud-based database', 'project management', 'team leadership', 'SQL optimization', 'AWS security best practices', 'container orchestration']"
31,Hirenza,https://www.linkedin.com/company/hirenza-pvt-ltd/life,Database Engineer,https://www.linkedin.com/jobs/view/4258550567,4258550567,United States,Remote,,2025-06-27 22:29:52,False,,"About The Company

CrowdStrike, Inc. is a global leader in cybersecurity solutions, dedicated to protecting organizations from cyber threats through innovative technology and expert services. Renowned for its cloud-native platform, CrowdStrike offers advanced endpoint protection, threat intelligence, and proactive security measures that safeguard critical business assets across various industries. The company's commitment to innovation, excellence, and customer-centric solutions has established it as a trusted partner in the cybersecurity landscape. With a focus on leveraging cutting-edge technologies and a talented team, CrowdStrike continues to set industry standards and drive the future of digital security.

About The Role

CrowdStrike is seeking a highly skilled Engineer to join our Data Services team. This role is pivotal in advancing our database systems to support the company's growing infrastructure and operational needs. The ideal candidate will be a hands-on, technically proficient professional with a strong background in managing and automating large-scale data plane services. You will work with critical cloud-based systems such as Cassandra, ElasticSearch, Kafka, and related technologies to ensure the robustness, security, and efficiency of our data infrastructure. Your expertise will directly contribute to maintaining the integrity and availability of petabytes of vital business data, supporting both engineering and customer support teams in resolving complex issues swiftly and effectively.

Qualifications

Experience with configuration management tools such as Chef or Salt, and strong proficiency in Ruby programming.Hands-on experience with large-scale data stores including Cassandra, ElasticSearch, Kafka, Zookeeper, and relational databases like MySQL and PostgreSQL.Proficiency in managing and operating large, business-critical Linux environments.Experience working within cloud platforms, preferably Amazon Web Services (AWS).Proven ability to collaborate effectively with both local and remote teams across different time zones.Strong decision-making skills, especially in high-pressure situations.Excellent verbal and written communication abilities.Demonstrated confidence and independence, balanced with the judgment to seek assistance when necessary.Bachelor's degree in Computer Science, Information Systems, Engineering, or a related field.

Responsibilities

Maintain a comprehensive understanding of the data components, including Cassandra, ElasticSearch/OpenSearch, Kafka, Zookeeper, MySQL, and PostgreSQL, to operate and automate clusters efficiently.Develop and enhance infrastructure services that support the engineering team's pursuit of a full DevOps model, emphasizing automation, scalability, and reliability.Collaborate closely with engineering and customer support teams to troubleshoot and resolve time-sensitive production issues promptly, ensuring minimal downtime and data loss.Ensure the safety, security, and availability of petabytes of critical business data through robust monitoring, maintenance, and security practices.Implement automation strategies to streamline data management processes and improve system performance.Stay updated with emerging technologies and industry best practices to continuously improve data infrastructure and operational procedures.

Benefits

CrowdStrike offers a comprehensive benefits package designed to support the well-being and professional growth of its employees. This includes competitive salary packages, health insurance plans, and wellness programs. Employees have access to flexible work arrangements, opportunities for ongoing training and development, and a vibrant, inclusive work environment that fosters innovation and collaboration. The company also provides various employee assistance programs, retirement savings options, and performance-based incentives to recognize and reward contributions. At CrowdStrike, employees are empowered to grow their careers while maintaining a healthy work-life balance.

Equal Opportunity

CrowdStrike is committed to fostering an inclusive and diverse workplace. We provide equal employment opportunities to all qualified applicants without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, or any other protected characteristic. We believe that diversity enhances our innovation and success, and we strive to create an environment where everyone feels valued, respected, and empowered to contribute their best.",https://www.bestjobtool.com/job-description/2971854370?source=LinkedIn,"['AWS', 'Snowflake', 'Python', 'SQL', 'Azure', 'Docker', 'Kubernetes', 'Linux', 'Git', 'Java']"
32,Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer / Fabric Admin (Remote),https://www.linkedin.com/jobs/view/4256760257,4256760257,"Austin, TX",Remote,$140K/yr - $150K/yr,2025-06-27 13:53:47,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currentlyseekinganexperienced and proactivePower BI Developer /Fabric Admin (Remote)to manage and support the enterprise-wide Power BI environment, focusing onPower BI Service administration, Fabric platform oversight, user support across tiers, and end-to-end report lifecycle management. This role requires technical depth in Power BI tools and infrastructure, combined witheffective communicationand stakeholder engagement to work across theCenter of Excellence (COE), business units, and IT support.This position will be fully remote located within the United States.

Responsibilities

Serve as aPower BI Fabric Administrator, overseeing workspace management, deployment pipelines, permissions, and performance monitoring. Develop aPower BI Center of Excellence (COE)to enforce standards, best practices, and governance for enterprise Power BI usage. Conducts PBI Training (Instructor-led Class Development, Delivery). ProvideTier 1 and Tier 2 supportfor Power BI issues—including report access, refresh failures, dataset issues, and workspace configuration. Assist withSelf-Help resources, including FAQs, templates, training materials, and troubleshooting documentation to empower end users. ManagePower BI licensing and desktop software distribution, working with ITassetsand licensing teams to track usage and entitlements. Support integration withexisting data sourcesandfacilitatethedesign and onboarding of new data sources, ensuring data quality, refresh schedules, and secure access. Handle incomingPower BI reporting and workspace requeststhrough a managed queue or mailbox, ensuringtimelyprioritization, response, and resolution. Design, publish, andmaintainPower BI reports and dashboardsaligned with business requirements andoptimizedfor usability and performance. Create andmaintainrobusttechnical documentation, including data dictionaries, architecture diagrams, workspace catalogs, and refresh schedules. Engage in proactive system health checks and performance tuning of Power BI Service, datasets, and gateway configurations. 

Qualifications

Required Skills and Experience

Bachelorswith 12+ years (orcommensurateexperience). Strong experience withPower BI Service Administration, Fabric workspace management, and deployment pipelines. Familiarity withPower BI governance, tenant settings, and organizational policies. Experience supporting and escalating issues acrossTier 1 and Tier 2 support models, including Service Desk and End-User Enablement. Knowledge ofPower BI licensing models(Free, Pro, Premium, PPU) and integration with Microsoft 365 administration tools. 

Preferred Skills And Experience

Solid understanding ofdata connectivity, gateway setup, refresh scheduling, and source control for bothexisting and new data sources.

Clearance Requirements

Ability to obtain and maintain a suitability/public trust clearance

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $140,000.00 - USD $150,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6140/power-bi-developer---fabric-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6140

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/d3560380e768495dbc012a8252f3ea90tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Communication', 'Leadership', 'Problem-solving', 'Time-management', 'Customer service', 'Adaptability', 'Teamwork', 'Technical skills (specific to industry)', 'Sales skills', 'Project management']"
33,Lensa,https://www.linkedin.com/company/lensa/life,Snowflake Data Engineer (Remote),https://www.linkedin.com/jobs/view/4258452188,4258452188,"Phoenix, AZ",Remote,$60.12/hr - $65.63/hr,2025-06-27 14:07:07,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for NTT DATA North America.

Company Overview

Req ID: 327683

NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.

We are currently seeking a Snowflake Data Engineer (Remote) to join our team in Phoenix, Arizona (US-AZ), United States (US).

Job Description

Role Overview

We are seeking a Snowflake Data Engineer with strong experience working on large EDW programs. This role requires expertise in Snowflake, Informatica, Control-M and github. This role will be responsible for building scalable EDW solutions.

Key Responsibilities

Develop Snowflake data pipelines for migration of data from one EDW to another EDW Design and implement datawarehouse models required to support the project requirements. Execute data migration scripts and Informatica ETL jobs to migrate data for various enviroments. Schedule jobs using Control-M Support end to end testing activities. Maintain solution versioning using Github Deploy solution in production using approved release proceedures. 

Qualifications & Skills

5+ years of Snowflake developer experience specifically working on large EDW programs 3+ years of Snowflake query tuning, ETL, external file handling, CTM skills. 2+ years of experience building and executing Informatica ETL jobs. Knowledge of Control-M and Github Must be US Citizen due to data restrictions 

Preferred Skills

Understanding of Agile methodologies. Knowledge of Salesforce and SAP data Problem-Solving: Strong analytical and troubleshooting skills. 

About NTT DATA

NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com.

Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this role is $60.12- $65.63/hour. Actual compensation will depend on several factors, including the candidate’s relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.

This position is eligible for company benefits including participation in medical, dental, and vision insurance, flexible spending or health savings account, and AD&D insurance, employee assistance, participation in a 401k program, and additional voluntary or legally-required benefits.

NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us. This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here. If you’d like more information on your EEO rights under the law, please click here. For Pay Transparency information, please click here.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/19ff94209fa748deb8752a75c97ac0ectjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['system', 'UI design', 'communication', 'budgeting', 'programming knowledge', 'virtual teamwork', 'Amazon Web Services (AWS)', 'data analysis', 'Salesforce', 'AWS Certified Solutions Architect']"
34,Travel + Leisure Co.,https://www.linkedin.com/company/travelleisureco/life,Data Engineer ,https://www.linkedin.com/jobs/view/4257036173,4257036173,"Orlando, FL",Hybrid,,2025-06-27 20:11:46,True,over 100 applicants,"The Data Engineer will partner with a wide range of business teams to implement analytical and data solutions that drive business value and customer satisfaction. He or She will be responsible for collecting, storing, processing, analyzing, modeling large sets of data and building applications and solutions using data. The primary focus will be on building, maintaining, implementing, monitoring, supporting and integrating analytical and data solutions with the architecture used across the company.
How You'll Shine:Maintain and monitor our analytics data warehouses and data platform.Design, Implement, test, deploy, and maintain stable, secure, and scalable data engineering solutions and pipelines in support of data and analytics projects, including integrating new sources of data into our central data warehouse, and moving data out to applications and affiliates.Responsible for hands-on development, deployment, maintenance and support of variety of Cloud and on-premise Solutions, web service infrastructure and supporting technologies.Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks.Works closely with project managers, business analysts, data scientists and other groups in the organization to understand and translate functional requirements and processes into technical specifications.Collaborate with key stakeholders to make sure our data infrastructure meets our business needs in a scalable way.Keep a critical eye on our technical strategy, identify gaps, and come up with creative solutions.
What You'll Bring:Bachelor’s degree in computer and information science requiredMaster’s degree preferred.Snowflake and Python certification preferred but not requiredExcellent listening, interpersonal, communication (written & verbal) and problem-solving skills.Ability to collect and compile relevant dataExtremely organized with great attention to detailExcellent ability to analyze information and think systematicallyStrong business analysis skillsA strong team player with some ability to work independentlyGood understanding of the company’s business processes and the industry at largeGood working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing data pipelines and data sets leveraging various scripting languages or ETL tools.Ability to perform root cause analysis on internal and external data processes to answer specific business questions and identify opportunities for improvement.Good analytic skills related to working with unstructured datasets.Ability to build and use APIs to push and pull data from various data systems and platforms.Build processes supporting data extraction, transformation, and loading of data into data structures.A successful history of manipulating, processing and extracting value from large, disconnected datasets.Ability to build data models and manage data warehouses3 years of related data engineering/IT experience1+ years of proven experience working with Apache Spark framework, Hadoop, Java/Scala, Python and AWS architecture.1+ years of proven experience in Microsoft .Net technologies such as C#, VB.Net and experience in designing, developing and deploying Windows & Web applications2+ years of experience in data modeling/database development using PL/SQL and SQL Server 2016 or later and Snowflake1+ years of proven experience building data pipelines and ETL flows in Cloud and on-premise environments using Snowpipe, Informatica, Airflow, Kafka etc.",https://www.linkedin.com/job-apply/4257036173,"['python', 'SQL', 'container orchestration', 'AWS', 'Docker', 'AWS cloud services', 'Snowflake Data Platform', 'DevOps', 'SQL proficiency', 'Big Data analysis']"
35,THOUGHT BYTE,https://www.linkedin.com/company/thought-byte/life,"AWS Data Engineer (Python, Dataiku)",https://www.linkedin.com/jobs/view/4255838804,4255838804,"San Francisco, CA",Hybrid,,2025-06-27 17:31:19,True,over 100 applicants,"Job Title: AWS Data Engineer (Python, Dataiku)Location: SFO, CA (Hybrid)Duration: Long Term Contract
Job Brief:As an AWS Data Engineer, your role will be to design, develop, and maintain scalable data pipelines on AWS.You will work closely with technical analysts, client stakeholders, data scientists, and other team members to ensure data quality and integrity while optimizing data storage solutions for performance and cost-efficiency.This role requires leveraging AWS native technologies and Databricks for data transformations and scalable data processing.
Responsibilities:Lead and support the delivery of data platform modernization projects.Design and develop robust and scalable data pipelines leveraging AWS native services.Optimize ETL processes, ensuring efficient data transformation.Migrate workflows from on-premise to AWS cloud, ensuring data quality and consistency.Design automations and integrations to resolve data inconsistencies and quality issues.Perform system testing and validation to ensure successful integration and functionality.Implement security and compliance controls in the cloud environment.Ensure data quality pre- and post-migration through validation checks and addressing issues regarding completeness, consistency, and accuracy of data sets.Collaborate with data architects and lead developers to identify and document manual data movement workflows and design automation strategies.
Skills and Requirements:7+ years of experience with a core data engineering skillset leveraging AWS native technologies (AWS Glue, Python, Snowflake, S3, Redshift).Experience in the design and development of robust and scalable data pipelines leveraging AWS native services.Proficiency in leveraging Snowflake for data transformations, optimization of ETL pipelines, and scalable data processing.Experience with streaming and batch data pipeline/engineering architectures.Familiarity with DataOps concepts and tooling for source control and setting up CI/CD pipelines on AWS.Hands-on experience with Databricks and a willingness to grow capabilities.Experience with data engineering and storage solutions (AWS Glue, EMR, Lambda, Redshift, S3).Strong problem-solving and analytical skills.Knowledge of Dataiku is needed.Graduate/Post-Graduate degree in Computer Science or a related field.",https://www.linkedin.com/job-apply/4255838804,"['C#', 'Java', 'JavaScript', 'React', 'Node.js', 'Python', 'SQL', 'AWS', 'Snowflake', 'Docker']"
36,RandomTrees,https://www.linkedin.com/company/randomtrees/life,GCP Data engineer,https://www.linkedin.com/jobs/view/4255867314,4255867314,"Philadelphia, PA",Hybrid,,2025-06-27 21:57:26,True,over 100 applicants,Experience in Google dataflow and apache Beam .Proficient in Java programming,https://www.linkedin.com/job-apply/4255867314,"['Python', 'SQL', 'Container Management', 'AWS', 'Big Data Analytics', 'Data Visualization', 'Communication', 'Teamwork', 'Problem Solving', 'Strategic Thinking']"
37,Jobot Consulting,https://www.linkedin.com/company/jobot-consulting/life,AI & Data Engineer - Python,https://www.linkedin.com/jobs/view/4255820984,4255820984,"Cincinnati, OH",Remote,$75/hr - $100/hr,2025-06-27 15:20:01,True,over 100 applicants,"Want to learn more about this role and Jobot Consulting? Click our Jobot Consulting logo and follow our LinkedIn page!

Job details

About Us

We leverage the latest in AI and data to provide highly specific and accurate results to our clients across the country.

Job Details

Are you a good fit?

 Work with cutting-edge AI and data technologies to deliver precise results. Collaborate in a serverless-first, containerized environment. Utilize Python, SQL, Spark, AWS, Amazon Athena, CDK, Terraform, and Docker.Proficiency in Python and SQLExperience with AWS and DockerFamiliarity with infrastructure-as-code tools like Terraform

Want to learn more about this role and Jobot Consulting?

Click our Jobot Consulting logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255820984,"['Python', 'SQL', 'Containerization', 'AWS', 'Snowflake', 'Cloud Strategy', 'Big Data', 'Reporting', 'Communication', 'Teamwork']"
38,Vetoquinol USA,https://www.linkedin.com/company/tomlyn-vetoquinol-usa/life,BI Data Engineer,https://www.linkedin.com/jobs/view/4255863487,4255863487,"Fort Worth, TX",Hybrid,$95K/yr - $105K/yr,2025-06-27 20:50:57,True,over 100 applicants,"DescriptionThe Business Intelligence Data Engineer administers key business processes and programs that involve significant use of data manipulation and visualization tools. This key position supports Commerical Operations teams by building business intelligence, including ETL and visualizations, providing training, answering end-user questions, and providing in-depth and ad-hoc analysis. This role is also responsible for the administration of special business programs, including reconciliation of sales data to rebate and promotional programs. A successful candidate will demonstrate exceptional organizational skills, be detail-oriented, thrive in a collaborative, team environment, and be passionate about customer service for internal stakeholders and external customers.
Essential FunctionsData AnalysisInterpret data, analyze results using statistical techniques and provide ongoing reports.Develop and implement data collection systems and other strategies that optimize statistical efficiency and data quality, ETL/ELT pipelines using SQL and Python in a Microsoft SQL environment.Administer SQL Server environments, security, and performance tuning.Acquire data from primary or secondary data sources and maintain databases/data systems.Identify, analyze, and interpret trends or patterns in complex data sets.Build and maintain end user reporting via PowerBI and other tools through staging, model optimization, and documentation.Filter and “clean” data, and review computer reports, printouts, and performance indicators to locate and correct code problems.Research and integrate tools or APIs that enable new automation capabilities.Work closely with management to prioritize business and information needs.Locate and define new process improvement opportunities.Communicate results and provide suggestions to end-users.
Systems ManagementServe as the primary point of contact for business stakeholders around prioritization, coordination, and escalation of projects and enhancements for the CRM system.Own functional tasks and deliverables including process definitions and re-engineering, high and detailed-level requirements gathering, fit/gap analysis, functional design, testing, training, reporting, support, and continuous improvement of business applications within the CRM.Negotiate business requirements across multiple groups and manage user expectations.Translate technical information into a common business language to business stakeholders.
Data Governance & Collaboration Ensure adherence to group policies related to operational procedures, IT security, and data governance.Select and implement tools and platforms that conform to established database, visualization, and security standards.Support the group’s data harmonization initiatives by aligning on definitions, implementations, and calculations.Ensure consistency in visualization methods and data presentation to support unified analytics across teams.Collaborate closely with group functions, including Data Management & Methodology (DMM) and Digital Systems Infrastructure (DSI), to align efforts with global data strategies.
General/Administrative Supports the company vision and mission and demonstrates the corporate core values in all professional activities.Complies with all safety requirements, work rules, and regulations.Maintains departmental housekeeping standards.All other duties as requested by management.This is a hybrid role with 2-days onsite required per week.
QualificationsFormal Education and CertificationBachelor’s degree in computer science, Information Systems, Data Engineering, or a related technical field.5-7 years of experience as a data engineer or similar may be substituted for the educational requirement.
Knowledge and ExperienceMinimum of 5-7 years as a Business/Data Analyst, Data Engineer, or equivalent education.Advanced T-SQL coding skills.3+ years of professional experience designing and maintaining ETL/ELT pipelines using SQL and Python.Proven expertise administering Microsoft SQL Server environments, including performance tuning and security management.Proficiency with PowerBI, or similar tools (e.g, Tableau), with a strong understanding of architecture, including staging, model optimization, and tabular modeling.Proficiency in building resilient pipelines with built-in logging, error handling, and monitoring.Familiarity with integrating external APIs and automation tools to enhance data workflows.Basic business analytics, including target setting, budget comparison, and data processing.Experience with automation such as Microsoft Fabric, PowerAutomate, or other tools highly desirable.
Vetoquinol USA is an equal opportunity employer. We are committed to providing a workplace that is free from discrimination of any kind and that promotes diversity, inclusion, and fairness. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status, and will not be discriminated against on the basis of disability. Join us and be a part of a great place to work!",https://www.linkedin.com/job-apply/4255863487,"['python', 'Java', 'React', 'Node.js', 'SQL', 'Docker', 'AWS', 'Snowflake', 'AWS CodeCommit', 'AWS SDK']"
39,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256760168,4256760168,"Richmond, VA",Remote,$118K/yr - $122K/yr,2025-06-27 13:53:54,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/23a6ad7ab1274e809326d443eef99c34tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'sql', 'container orchestration', 'AWS', 'cloud architecture', 'snowflake SQL', 'data visualization', 'Linux Ubuntu', 'Jenkins', 'communication']"
40,Analytica,https://www.linkedin.com/company/analytica-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4240380993,4240380993,"Falls Church, VA",Remote,,2025-05-04 22:41:03,True,over 100 applicants,"Analytica is seeking a Data Engineer to support a complex data program for the Defense Health Agency. This role will work closely with reporting and analytics developers, data governance and data architects to build robust, high quality data pipelines that enhance productivity and operational efficiency. This position is for US Citizens only and candidate will hold (or be able to hold) a US Secret Clearance. The position is a remote position with occasional onsite meetings in at San Antonio, TX as required.

Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.   

Responsibilities: Support the Military Health System (MHS) by supporting data engineering that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset Ensure data interoperability, governance, and quality to support enterprise-wide decision-making and healthcare optimization play a critical role in designing and implementing data architecture frameworks that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset.Design and build robust and scalable data pipelines for managing structured and unstructured data using traditional databases (Oracle, PostgreSQL, etc.) or cloud Databases such as Amazon Redshift or AWS Vertica Translate business needs into: data architecture solutions development within supported data systems. data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems. Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams Monitor and troubleshoot data import, analysis, and display errors.
Required Qualifications: Bachelor’s degree in information technology, Computer Science, Engineering or equivalent technical field  3+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred) 2+ years experience with Python and SQL (Java and Python preferred) Experience working on relational NoSQL and SQL databases Experience designing and implementing various data pipeline patterns and strategies Strong knowledge of data security principles Prior experience with DHA M2 or MDR databases, CERNER EHR a strong plusMust be a US Citizen and must possess or be eligible for a SECRET clearance 
About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S.  Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider. 
 ",https://www.linkedin.com/job-apply/4240380993,"['python', 'sql', 'container orchestration', 'AWS', 'cloud architecture', 'snowflake', 'programming languages', 'data analysis', 'machine learning', 'project management']"
41,Lensa,https://www.linkedin.com/company/lensa/life,HR Data Engineer,https://www.linkedin.com/jobs/view/4258454085,4258454085,"Annapolis, MD",On-site,$94.8K/yr - $151.4K/yr,2025-06-27 14:07:04,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for General Motors.

Job Description

Work Arrangement:

This role is based remotely, but if you live within a 50-mile radius of Atlanta (GA), Austin (TX), Detroit (MI), Warren (MI), or Mountain View (CA) you are expected to report to that location three times a week, at minimum.

The Role

The People Analytics CoE is seeking an HR Data Engineer to support data engineering, data automation, and data analytics solutions development, enhancements and management. In this role, you’ll be responsible for enhancing our existing HR data foundation (Databricks) by designing, building, and maintaining scalable data pipelines and integrations that enable accurate, secure, and timely access to HR data across the organization. You will build data models and optimize our databases to improve decision making, and evaluate the integrity, quality, and reliability of data outputs and outcomes. You’ll work closely with the broader PA team, IT and HR COEs to empower data-driven decisions around workforce strategy, talent acquisition, performance management, and employee engagement. This role reports to a People Analytics Solutions Lead and requires strong HR data engineering technical expertise working with large HR datasets and demonstrated knowledge of different areas of HR (data, analytics, and processes)

What's in it for you? You will have a chance to influence our talent strategy, design the PA data infrastructure stack, develop insights that matter, and be part of a great team that puts innovation and curiosity at the center of everything we do.

Responsibilities

Design, develop, and maintain ETL/ELT processes for HR data from multiple systems including Workday to empower data-driven decision-making Drive implementation of robust HR data models and pipelines optimized for reporting and analytics, ensuring data quality, reliability, and security for on-prem and Azure cloud solutions. Develop pipelines and testing automation to ensure HR data quality and integrity across multiple data sources Collaborate with People Analytics and HR business partners to understand data requirements and deliver reliable solutions. Collaborate with technical teams to build the best-in-class data environment and technology stack for People Analytics teams. Ensure data integrity, quality, consistency, security, and compliance (e.g., GDPR, CCPA, HIPAA where applicable). Design and implement secure processes for handling sensitive information in our data tech stack while maintainingappropriate access controls and confidentiality Automate manual HR reporting and improve data accessibility through scalable data pipelines across the entire HR employee lifecycle Troubleshoot and resolve data-related issues quickly and efficiently. Contribute to HR tech stack evaluations and migrations, especially around data capabilities and API integrations. Incorporate external data sources into internal datasets for comprehensive analysis Manage and optimize platform architecture including Databricks environment configuration and performance optimization Stay up to date with emerging trends and advancements in data engineering – both technically and in the HR and People Analytics/sciences domain 

Additional Job Description

Requirements :

5+ years of experience in HR Data Engineer role leading HR data engineering transformation and implementing data pipelines and data solutions in the People Analytics/HR domain Very good understanding of HR data and HR employee lifecycle processes (talent acquisition, talent development, workforce planning, engagement, employee listening, external benchmarking etc.) Very good understanding of HCM data architecture , models and data pipelines and experience designing and implementing data integrations and ETLs with Workday (RaaS, APIs) Experience designing and automating data and analytics solutions that can provide insights and recommendations at scale Proficiency in SQL, R/Python and ETL tools Deep expertise in modern data platforms (particularly Databricks ) and end-to-end data architecture (DLT Streaming Pipelines, Workflows, Notebooks, DeltaLake, Unity Catalog) Experience with different authentication (Basic Auth, Oauth, etc.) and encryption methods and tools (GPG, Voltage, etc.) Very strong data analytics skills and ability to leverage multiple internal and external data sources to enable data-driven insights and inform strategic talent decisions Knowledge of compliance and regulatory requirements associated with data management Experience working in environments requiring strict confidentiality and handling of sensitive data Great communication skills and ability to explain complex technical concepts to non-technical stakeholders. Degree with quantitative focus (e.g., Mathematics, Statistics) and/or degree in Human Resources is a plus 

Relocation:

This job is not eligible for relocation benefits.

Compensation

The salary range for this role is $94,800 - $151,400. The actual base salary a successful candidate will be offered within this range will vary based on factors relevant to the position. Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance. Benefits: GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more. 

GM DOES NOT PROVIDE IMMIGRATION-RELATED SPONSORSHIP FOR THIS ROLE. DO NOT APPLY FOR THIS ROLE IF YOU WILL NEED GM IMMIGRATION SPONSORSHIP (e.g., H-1B, TN, STEM OPT, etc.) NOW OR IN THE FUTURE.

About GM

Our vision is a world with Zero Crashes, Zero Emissions and Zero Congestion and we embrace the responsibility to lead the change that will make our world better, safer and more equitable for all.

Why Join Us

We believe we all must make a choice every day – individually and collectively – to drive meaningful change through our words, our deeds and our culture. Every day, we want every employee to feel they belong to one General Motors team.

Benefits Overview

From day one, we're looking out for your well-being–at work and at home–so you can focus on realizing your ambitions. Learn how GM supports a rewarding career that rewards you personally by visiting Total Rewards Resources (https://search-careers.gm.com/en/working-at-gm/total-rewards) .

Non-Discrimination and Equal Employment Opportunities (U.S.)

General Motors is committed to being a workplace that is not only free of unlawful discrimination, but one that genuinely fosters inclusion and belonging. We strongly believe that providing an inclusive workplace creates an environment in which our employees can thrive and develop better products for our customers.

All employment decisions are made on a non-discriminatory basis without regard to sex, race, color, national origin, citizenship status, religion, age, disability, pregnancy or maternity status, sexual orientation, gender identity, status as a veteran or protected veteran, or any other similarly protected status in accordance with federal, state and local laws.

We encourage interested candidates to review the key responsibilities and qualifications for each role and apply for any positions that match their skills and capabilities. Applicants in the recruitment process may be required, where applicable, to successfully complete a role-related assessment(s) and/or a pre-employment screening prior to beginning employment. To learn more, visit How we Hire (https://search-careers.gm.com/en/how-we-hire) .

Accommodations

General Motors offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email (Careers.Accommodations@GM.com) us or call us at 800-865-7580. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.

About

We are leading the change to make our world better, safer and more equitable for all through our actions and how we behave. Learn more about:

Our Company (https://search-careers.gm.com/en/working-at-gm/)

Our Culture

How we hire (https://search-careers.gm.com/en/how-we-hire/)

Our diverse team of employees bring their collective passion for engineering, technology and design to deliver on our vision of a world with Zero Crashes, Zero Emissions and Zero Congestion. We are looking for adventure-seekers and imaginative thought leaders to help us transform mobility.

Explore our global location s

The policy of General Motors is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity/expression or veteran status. Additionally, General Motors is committed to being an Equal Employment Opportunity Employer and offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email us at Careers.Accommodations@GM.com .In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.

If you have questions about this posting, please contact support@lensa.com

",https://lensa.com/cgw/48102178aae443018f7d6f0deb58eee0tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['cloud', 'teamwork', 'communication', 'problem-solving', 'reporting']"
42,Zip Co,https://www.linkedin.com/company/zip-co-limited/life,Senior Data & ML Engineer I,https://www.linkedin.com/jobs/view/4257004889,4257004889,"New York, NY",Remote,$150K/yr - $165K/yr,2025-06-27 16:53:28,False,,"Senior Data Engineer with strong experience in Azure, Snowflake, Databricks, dbt, and streaming technologiesVersatile and adaptable, able to design both batch and real-time pipelines while collaborating cross-functionally in a ""blameless"" engineering cultureRemote-first opportunity for US-based employees with the option to work in-person out of our Manhattan office
Start your adventure with Zip
Join Zip’s Engineering function and put your name to solving fascinating challenges at scale in an agile, test-driven development environment. If you value thoughtful data modeling, pipeline performance, and delivering trusted data fast—you’ll be a great fit with the squads responsible for building our modern, Azure-native data platform that fuels analytics, AI, and product innovation.
We’re seeking a well-rounded Senior Data Engineer who is comfortable working across data ingestion, transformation, and delivery. You’ll work across both batch and streaming paradigms and have a strong hand in shaping our data models, scaling our pipelines, and improving observability and performance across the stack. We’d love it if you also brought experience with infrastructure or DevOps—but that’s a nice-to-have, not a must.
As we double down on scaling our data platform, you’ll help us build for performance, reliability, and long-term maintainability. If you're passionate about clean design and modern data architecture, we'd love to hear from you.
Interesting problems you’ll get to solve
Architect and scale streaming pipelines with tools like Kafka/Event Hubs and Databricks Structured Streaming.Design and optimize batch processing workflows using dbt, Snowflake, and Azure Data Lake.Build robust ELT and CDC pipelines using Airbyte, and model them cleanly for downstream use.Implement observability and testing frameworks for data quality, lineage, and freshness.Implement feature pipelines in Spark for machine learning models, and microservices to host them for production use.Develop self-service patterns and tooling for analytics and ML teams to move faster.Help maintain and evolve our Delta Lake environment and push performance boundaries in Databricks.Collaborate with analytics, engineering, and product teams to ensure data is trusted and accessible.
What you’ll bring to the team
5+ years of experience in Data Engineering, Machine Learning Engineering or similar.Proven experience working with both batch and streaming data pipelines (e.g., dbt, Spark, Snowflake for batch; Kafka/Event Hubs, Delta Lake for streaming)Strong SQL and Python skills, and comfort working with large-scale datasets.Solid understanding of data modeling and architecture paradigms: Kimball/dimensional modeling, Data Vault, Medallion.Hands-on experience with Snowflake, Azure Blob Storage, Databricks, and dbtExperience working in Azure-native environments, ideally with exposure to tools like Event Hubs, ADLS, Azure DevOps, or Synapse.Exposure to MLOps and machine learning workflows: supporting ML teams, building feature pipelines, managing model inputs/outputs, monitoring model performance, or deploying models via Databricks ML, MLflow, or Azure ML.Experience writing and working in a microservices architecture and writing asynchronous python code.Understanding of ML-specific challenges such as feature drift, data versioning, or batch scoring at scale.Familiarity with NoSQL databases such as Azure CosmosDB.Infrastructure-as-code or DevOps tooling experience (Terraform, CI/CD, monitoring) - nice-to-have. Knowledge of Atlan or other data management tools.Knowledge of Tableau, Power BI or other Analytics tools.Strong communication skills, with an ability to work collaboratively with cross-functional teams, both technical and on-technical.Demonstrated use of AI in the performance of your role.
What you’ll get in return
Zip is a place where you’ll get out what you put in. The newness of our sector means we need to move at pace and embrace change, and our promise to you when you join the team is that you’ll feel empowered and trusted to make big things happen quickly.
We want you to feel welcome and as though you have the support to be yourself, and care for yourself at work. Because it’s important to us that you make the most of the opportunities you’ll get to grow your skills and your career, and be surrounded by smart, friendly people and leaders that have your back.
We think these are just some of the best things about being a Zipster. We will also offer you:
Flexible working cultureIncentive programs20 days PTO every yearGenerous paid parental leaveLeading family support policies100% employer covered insuranceBeautiful Union Square office with a casual dress codeLearning and wellness subscription stipendCompany-sponsored 401k match
Zip is committed to a straightforward and transparent pay structure. The actual base salary will be determined by various individualized factors, including job-related knowledge, skills, experience, location, internal equity, as well as other objective business considerations.
The annual base Pay Range for this position is $150,000-$165,000. This range reflects our US national compensation (USN). Additional premium percentages may apply based on our tiered premium strategy. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits.
If hired, employees will be in an 'at-will position' and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.
Be a part of a team that reflects the diversity of our customers
We pride ourselves on being a workplace that provides equal opportunities to people of all ages, cultural backgrounds, sexual orientations, gender identities, abilities, veteran status, and everything else that makes you unique.
Equally, we’re committed to ensuring our recruitment processes are accessible and inclusive. Please let us know If there are any adjustments that need to be made to ensure you have a fair and equitable experience.
And finally…get to know us
Zip Co Limited (ASX: ZIP) is a digital financial services company, offering innovative, people-centred products that bring customers and merchants together.
Operating in two core markets - Australia and New Zealand (ANZ) and the US, Zip offers point-of-sale credit and digital payment services, connecting millions of customers with its global network of tens of thousands of merchants.
We’re proud to be a values-led business and our values - Customer First, Own it, Stronger Together and Change the Game - guide us in everything we do.
I acknowledge by clicking ""Submit Application"", that the information provided is true and correct. I also understand that any willful dishonesty may render for refusal of this application or immediate termination of employment. By providing your information, you acknowledge that you have read our Zip Applicant and Candidate Privacy Notice and authorize Zip to process your data subject to those terms
Before you apply, give Zip a try -> rebrand.ly/check-zip-out

Zip participates in the federal government’s E-Verify program",https://grnh.se/1b87d5e26us?gh_src=8399a8616us&source=LinkedIn,"['Java', 'Spring', 'Spring Boot', 'Docker', 'Kubernetes', 'AWS', 'SQL', 'Git', 'Communication', 'Problem-Solving']"
43,Infinite Computer Solutions,https://www.linkedin.com/company/infinite-computer-solutions/life,K2VIEW Data Masking Engineer,https://www.linkedin.com/jobs/view/4257037435,4257037435,United States,Remote,,2025-06-27 20:28:44,True,23 applicants,"K2VIEW Data Masking Engineer – Job description Summary (K2VIEW, EDI and Javs scripting are MUST have skills) The Data Masking Engineer should be familiar with data masking, GitHub, pipelines and branching strategies. This position would involve code review and testing the masking process, creating pipelines and branches for code check-in and automation. The work includes:Creating and maintaining GitHub Pipelines for checking in an automated code deploymentMasking Database and files (csv, txt, 837, 834) – using K2viewCode ReviewTestingHelping gather specs for the upcoming pods",https://www.linkedin.com/job-apply/4257037435,"['python', 'java', 'visualization', 'analytics', 'data_engineering', 'SQL', 'machine_learning', 'data_management', 'earned_value_analysis', 'AWS', 'snowflake']"
44,Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Data Engineer / BI Specialist (Azure and ETL),https://www.linkedin.com/jobs/view/4257035090,4257035090,"Warren, MI",On-site,,2025-06-27 20:19:56,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Strivector, is seeking the following. Apply via Dice today!

Strivector Corp is a National Recruiting and Staffing agency established in 2012 and headquartered in Austin, Texas. We are a preferred partner for several Fortune 500 companies nationwide. Strivector has been consistently rated a rare 4.6/5 on Google, Indeed and Glassdoor by our candidates, customers, employees and contractors.

Elevate your professional journey with us. Strivector s premier client (a well-known market leader in their space) is hiring for the following full-time direct hire position.

Position Overview

As a Data Engineer / BI Specialist (Azure & ETL) your roles and responsibilities will include:

 Create scalable data integration and transformation (ETL/ELT) pipelines using modern tools and cloud platforms. Develop and manage secure API connections across various internal and third-party systems. Support AI/ML and automation tools through a robust data infrastructure. Monitor data quality, enforce compliance standards, and uphold governance best practices. Partner with BI teams to deliver dashboards, models, and visualizations using Power BI and other tools. Evaluate and introduce new tools and technologies to enhance data capabilities.

Qualifications

Most importantly, you need to be a passionate  Data Engineer / BI Specialist (Azure & ETL) who enjoys his work and is considered to be one of the best within your organization. The ideal Data Engineer / BI Specialist (Azure & ETL) would be someone with deep experience in many (if not all) of the following:

 Proven experience with SQL Server, T-SQL, and Azure SQL Hands-on skills with Azure Data Factory or other ETL platforms Knowledge of API management tools (Azure API Management, MuleSoft, etc.) Experience using Power BI, Power Apps, and Power Automate Strong understanding of data modeling, predictive analytics, and security standards Experience working in Agile teams and managing multiple projects Bachelor s degree in a related field 3 5 years of experience in data engineering, system integration, or automation

We understand that even if you are a seasoned  Data Engineer / BI Specialist (Azure & ETL) you may not have all the skills listed here.

Additional Information

 Compensation:  Based on Experience. One of the best in the industry

 Minimum Education:  Bachelor s degree

 Minimum Experience:  [3-5] y ears of equivalent experience

 Type of position:  Full-time Permanent position with benefits

 Location: Warren, MI

 Remote / Hybrid :  Hybrid with [3] days remote

 Relocation accepte d:  No",https://click.appcast.io/t/dhx3Iui9XR8TX618SwpXVYS9qKSdbmEwVcih3LoD9uk=,"['python', 'SQL', 'containerization', 'AWS', 'snowflake', 'project management', 'communication', 'analytical skills', 'problem-solving', 'customer service']"
45,TekNavigators Staffing,https://www.linkedin.com/company/teknavigators/life,SQL Developer,https://www.linkedin.com/jobs/view/4258546890,4258546890,"Dallas, TX",On-site,,2025-06-27 22:24:26,True,13 applicants,"Role: - SQL Programmer AnalystLocation- Tyler, TX (Day 1 Onsite)Full Time Job: Permanent Role Relocation amount will be paid Summary-The SQL Programmer Analyst will be responsible for the efficient and effective management of the organization’s Merchandise and Finance applications, along with their databases and integrations. This position will focus on the development of SQL Stored procedures, database maintenance, and optimization efforts to ensure data integrity, confidentiality, and availability. You will play a key role in improving database integrations and ensuring these systems meet the growing needs of the business. This role reports to the IT Manager and collaborates with cross-functional teams to optimize application architecture, integration processes, and overall database health.
DUTIES AND RESPONSIBILITIES •Support the “Culture” and drive the Mission, Vision, and Values •Collaborate with D365 and middleware integration teams to identify integration issues, performance bottlenecks, and opportunities for improvement.•Migrate legacy SQL integration procedures from Merch, Finance, and ERP to new ETL technologies. Reverse engineer legacy code and develop a comprehensive understanding of transaction flows between business applications.•Manage application databases, optimize queries, ensure data integrity, and ensure the scalability of databases. Maintain database security and compliance with industry standards.•Develop and optimize a SQL database knowledge base to automate regular tasks and allow for delegation to other IT team members.•Acquire a strong understanding of key merchandise and finance applications and their interactions with backend databases and integration points.•Evaluate current database automation practices and identify improvement opportunities. Be comfortable reverse-engineering SQL and other database objects.•Contribute to the design and optimization of application architecture from a database and integration perspective, ensuring scalability, reliability, and performance.•Develop incident response plans, perform risk assessments, and implement strategies to mitigate risks that could affect system availability or business operations.•Ensure adherence to security protocols and manage up-to-date patching for compliance.•Collaborate with application and integration owners to maintain security.•Build a secure, scalable database and integration infrastructure to support the company’s growth.•Perform additional duties as assigned. QUALIFICATIONS AND REQUIREMENTS•Bachelor's degree in computer science, Information Technology, or a related field experience•5+ years of experience in SQL development, database administration, query optimization, application architecture, and integration management.•Proficiency in MS SQL, SSIS, SSMS, PL/SQL, Oracle DB, Snowflake.•Proven track record of managing complex application database and integration environments.•Strong knowledge of database best practices, including performance improvements, health monitoring, and security protocols.•Excellent leadership, communication, and interpersonal skills, with the ability to collaborate effectively with cross-functional teams and senior management.•Self-motivated with the ability to work independently and as part of a team.•Excellent time management and project prioritization skills, able to manage multiple projects and meet deadlines.•Based in Tyler, TX at our home office; Tyler area residence.
PREFERRED SKILLS•MS SQL Development and Architecture including deep knowledge on complex functions, stored procedures, SSMS, SSIS, Linked Server and performance tuning•Experience in reverse engineering legacy SQL code and integrations•Merchandising systems and application knowledge, financial application knowledge (D365), Service based integration experience and knowledge (Logic App, Boomi, Azure Service Bus, Snowflake)•Experience with cloud platforms (eg, AWS, Azure), virtualization technologies•Relevant Microsoft SQL Server certifications•Experience working in a retail environment a plus",https://www.linkedin.com/job-apply/4258546890,"['Python', 'SQL', 'Cloud Computing', 'AWS', 'Data Analysis', 'AWS Certified Solutions Architect', 'Snowflake', 'Container Orchestration', 'ETL Processing', 'DevOps']"
46,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, Analytics",https://www.linkedin.com/jobs/view/4258451512,4258451512,"Austin, TX",On-site,$220.7K/yr - $235.4K/yr,2025-06-27 14:06:44,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer, Analytics Responsibilities:

Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains. Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. Define and manage SLA for all data sets in allocated areas of ownership. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts. Influence product and cross-functional teams to identify data opportunities to drive impact. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors. Demonstrate good judgment in selecting methods and techniques for obtaining solutions. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires a Master's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and three years of work experience in the job offered or in a computer-related occupation. Requires three years of experience in the following: Features, design, and use-case scenarios across a big data ecosystemCustom ETL design, implementation, and maintenanceObject-oriented programming languagesSchema design and dimensional data modelingWriting SQL statementsAnalyzing data to identify deliverables, gaps, and inconsistenciesManaging and communicating data warehouse plans to internal clientsMapReduce or MPP system andPython.
Public Compensation

$220,720/year to $235,400/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/1582886f549243a59c4324fbc2df2dbatjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['python', 'sql', 'containerization', 'AWS', 'AWS-native', 'Apache Kafka', 'MongoDB', 'Docker', 'Git', 'JIRA']"
47,Jobot,https://www.linkedin.com/company/jobot/life,Data Engineer,https://www.linkedin.com/jobs/view/4255825157,4255825157,"Raleigh, NC",Remote,$80K/yr - $100K/yr,2025-06-27 15:19:50,True,32 applicants,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Fully remote Data Engineer opportunity // Data Modeling experience required!

This Jobot Job is hosted by Craig Rosecrans

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $80,000 - $100,000 per year

A Bit About Us

We are currently seeking a seasoned Data Engineer to join our dynamic Tech Services team. The ideal candidate will be a data enthusiast with a solid understanding of the latest data technologies and trends coupled with a strong desire to deliver top-notch solutions to our clients. The successful candidate will be responsible for designing, developing, and maintaining data architectures, databases, and processing systems. With a focus on ETL, Data Modeling, SSIS, Tableau, and PowerBI, this role will be integral in transforming data into readable, goal-driven reports for continued innovation and growth.



Why join us?


 Competitive Base Salary Company paid health plan for employees Flexible Hours Very generous PTO Dental and Vision, FSA, HSA Small team, autonomy Many more great perks!

Job Details

Responsibilities

 Design, develop, automate, and maintain productivity tools using programming, database or scripting languages to improve software modeling and development. Design and implement ETL procedures for intake of data from both internal and outside sources; as well as ensure data is verified and quality is checked. Collaborate with data architects, modelers and IT team members on project goals. Develop and maintain databases by acquiring data from primary and secondary sources, and build scripts that will make our data evaluation process more flexible or scalable across data sets. Employ a variety of languages and tools to marry systems together or try to hunt down opportunities to acquire new data from other systems. Develop and implement data standards, ensuring data quality and consistency. Perform data profiling to identify and understand anomalies. Present information using data visualization techniques through Tableau and PowerBI. Work closely with team members, clients, project managers, and other stakeholders to ensure solutions are delivered timely and accurately.

Qualifications

 Bachelor's degree in Computer Science, Information Systems, or a related field. A minimum of 5+ years of experience in a data engineering role with a proven record of successful data manipulation. Proficiency with ETL tools, SSIS, and experience with SQL/NoSQL databases. Knowledge and experience in data modeling. Experience with business intelligence tools like Tableau, PowerBI or similar. Strong problem-solving skills, attention to detail, and ability to think critically. Excellent written and verbal communication skills, with the ability to present complex data in a clear and concise manner. Ability to work independently and with team members from different backgrounds. Excellent organizational skills and the ability to manage multiple tasks concurrently. A strong desire to learn and develop new skills, staying up to date with the latest data best practices and technologies. Ability to work in a fast-paced, deadline-driven work environment.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Jobot is an Equal Opportunity Employer. We provide an inclusive work environment that celebrates diversity and all qualified candidates receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Sometimes Jobot is required to perform background checks with your authorization. Jobot will consider qualified candidates with criminal histories in a manner consistent with any applicable federal, state, or local law regarding criminal backgrounds, including but not limited to the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!

",https://www.linkedin.com/job-apply/4255825157,"['Leadership', 'Communication', 'Project Management', 'AI/ML', 'Cybersecurity', 'Big Data', 'Cloud Computing', 'Data Analysis', 'Python', 'SQL']"
48,AccruePartners,https://www.linkedin.com/company/accruepartners/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4257027178,4257027178,"Charlotte, NC",Hybrid,,2025-06-27 19:43:18,False,,"The Team You Will Be Joining

A confidential, fast-growing financial services company that is redefining the digital collections space. Privately held and led by a mission-driven leadership team, the company partners with financial services clients to drive effective recovery outcomes through omnichannel, AI-powered engagement. The organization is building out a new Charlotte hub and scaling its operations team to support continued expansion across clients and product offerings. 

What They Offer You

A hybrid role based in Charlotte, NC with meaningful visibility across leadership, strategy, and product teams Fast-paced, collaborative culture that values ownership, speed, and innovation Ability to directly impact outcomes for nationally recognized financial services clients Competitive compensation, benefits, and the chance to leave a lasting mark on a scaling business 

Location

Hybrid flexibility in Charlotte NC 

Why This Role Is Important

Design, build, and maintain scalable ETL/data pipelines Model and transform data within Snowflake, ensuring performance and security for analytics and reporting. Work alongside analytics and marketing teams to ingest and prepare data for AI/ML-based collections optimization. Write and optimize complex SQL queries, stored procedures, views, and performance-tuned table structures. Implement data quality, validation, and monitoring frameworks across pipelines. Automate operational workflows (e.g., incremental/delta loads, anomaly detection) Troubleshoot and resolve production data issues to ensure high availability and pipeline reliability. Document data flows, schemas, and system architectures. Stay current on industry best practices in AWS, data pipeline engineering, Snowflake, and ETL methodologies. 

The Background That Fits

5+ years professional experience in data engineering or related field. Hands-on experience in building ETL/data pipelines Strong design & query skills in SQL for relational, analytical, and performance-optimized databases. Experience with Snowflake (or equivalent cloud data warehouse). Familiarity with data modeling (star/snowflake schemas, fact/dimension tables). Experience with AWS (strongly preferred) Solid understanding of data warehousing principles and best practices. Excellent problem-solving, organization, and communication skills. Experience working in fast-paced/adaptive startup environments.",https://accruepartners.com/job-details/senior-data-engineer-in-information-technology-jobs-1236770,"['Java', 'SQL', 'Python', 'AWS', 'Git', 'Docker', 'Linux', 'Agile', 'Project Management', 'Communication']"
49,Lensa,https://www.linkedin.com/company/lensa/life,DATA ENGINEER/DBA,https://www.linkedin.com/jobs/view/4258452493,4258452493,"Seattle, WA",On-site,"$9,800/month - $10.4K/month",2025-06-27 14:06:38,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for University of Washington.

Req #: 244916

Department: OFFICE OF RESEARCH INFORMATION SERVICES

Appointing Department Web Address: https://www.washington.edu/research/oris/

Job Location Detail: Available for telework/hybrid/remote

Posting Date: 04/04/2025

Closing Info: Open Until Filled

Salary: $9,800 - $10,400 per month

Shift: First Shift

Notes: As a UW employee, you will enjoy generous benefits and work/life programs. For a complete description of our benefits for this position, please visit our website, click here. (https://hr.uw.edu/benefits/wp-content/uploads/sites/3/2018/02/benefits-professional-staff-librarians-academic-contract-covered-exempt-20250130-a11y.pdf)

As a UW employee, you have a unique opportunity to change lives on our campuses, in our state and around the world. UW employees offer their boundless energy, creative problem-solving skills, and dedication to build stronger minds and a healthier world.

UW faculty and staff also enjoy outstanding benefits, professional growth opportunities and unique resources in an environment noted for diversity, intellectual excitement, artistic pursuits, and natural beauty.

The Office of Research Information Services has an outstanding opportunity for a Data Engineer/DBA (Database Administrator) to join their team.

The University of Washington is one of the world’s preeminent public universities in impact and funding. Since 1972, the UW has continued to receive more externally sponsored research funding than any other U.S. public university and is one of the few universities in the U.S. with total research funding nearing two billion. That funding drives crucial, ground-breaking research that transforms lives around the globe. In the Office of Research at UW, we take pride in creating an outstanding climate of support for UW researchers that helps them compete and succeed. However, the complexity of research administration and regulation grows each day. Our mission in ORIS (Office of Research Information Services) is to reduce that administrative burden by streamlining and automating information services so researchers can spend more time researching.

The Data Engineer/DBA (Database Administrator) will be responsible for ensuring the stability, performance, and security of the databases and data that support ORIS systems. ORIS’s data infrastructure supports systems and services used by award-winning faculty and staff to perform administrative activities including submission of funding applications for consideration, routing them electronically for approval, managing detailed multi-year grant budgets, integrating with other UW systems, and providing reporting capabilities to academic and administrative users. This will reduce the administrative burden on researchers across the University by improving the amount of time researchers can spend on researching; improving University compliance; and ensuring that the University can continue to be at the forefront of academic research.

The Data Engineer/DBA position is charged with ensuring that database resources meet security, performance, reliability, and availability requirements. The Data Engineer/DBA will partner with development teams across multiple products and will need to work with different database systems and hosting platforms. The DBA will provide consultation and assistance in database and data design and will be responsible for provisioning, optimization, operations, and business continuity. The Data Engineer/DBA will work to ensure secure and dependable database performance and operations through implementation, test, integration test, and migration and production deployment and cutover phases. Operations consist of supporting a range of data software components, infrastructure, integrations, and services, some of which target 24/7/365 availability.

Successful candidates must have a solid background in design and implementation of databases and other data infrastructure that support online applications, reporting and analytics, and integrations with other systems.

The Data Engineer/DBA Supports Databases Across Platforms And Products, Both In The Cloud And On-premises. The Data Engineer/DBA Must Provide Support Through All Phases Of Development As Well As Production Support. A Successful Person In This Position Must

Collaborate and consult with multiple teams of diverse staff including junior and senior engineering staff as well as technical leads, product owners, and project managers across multiple products. Align data and database configuration, maintenance, and operations with overall ORIS IT operational goals and business objectives as outlined by ORIS program portfolio that supports UW’s $1.89 billion research enterprise. Lead, plan, and advise on the strategic and tactical direction of all facets of data and database administration across multiple products, from meeting minimum security requirements to ensuring stability, performance, and availability requirements are met.This position advances the Office of Research’s mission to support the integration of electronic research information systems both as a data engineer, ensuring the optimized and performance applications with the data layer and, as an administrator, ensuring the security and reliability of ORIS’s core database operations. This will reduce the administrative burden on researchers across the University by improving the amount of time researchers can spend on researching; improving University compliance; and ensuring that the University can continue to be at the forefront of academic research. DUTIES AND RESPONSIBILITIES Operations and Administration – 60%: Design, deploy, configure, maintain, and deprecate database environments, databases, data schema, and data elements to support novel applications development and support. Develop and assist with SQL query design for applications and reporting to meet organizational needs and ORIS commitments. Recommend, design, implement, and maintain automation to reduce repetitive software development and maintenance tasks and increase application performance. Maintain configuration and management of databases across Non-Prod and Prod environments to ensure optimal uptime and performance for ORIS products and services. Engage and participate in ORIS’ culture of review both as reviewer and reviewee to ensure shared knowledge of development practices and high quality code. Implement and maintain regular database backup and retention plans to ensure that database archiving and disposition requirements are met. Determine, implement, and maintain performance monitoring and tuning strategies to ensure that databases operate within security, reliability, and availability requirements. Configure logging and monitoring to ensure reliable and accurate reporting on database and application health and performance. Participate in software release (possibly including off hours release and update times). Plan and carry out any needed data migrations and system upgrades and updates. Monitor ETLs and other regular or scheduled database processes and take appropriate steps when failures occur. Research and respond to alerts and notifications of real or potential issues to proactively communicate and address problems. Research data and database issues as they pertain to production support and non-production support requests, including application SQL queries. Ensure business continuity and disaster recovery for ORIS databases. Troubleshoot and resolve application and data infrastructure incidents.Technical Planning, Consultation, and Collaboration – 25%: Partner with development teams to ensure data and database components and mechanisms meet all functional and non-functional requirements, including data modeling, data schema design, database design, query development and optimization, and similar tasks. Participate in UW technology integration initiatives that involve ORIS supported systems. Participate in organizational risk assessment activities, security planning, and vendor analysis to ensure solutions meet data and data infrastructure standards. Build strong relationships, working with diverse technical and business partner groups, both internal and external to the University of Washington. Effectively communicate and collaborate technical information with a wide range of people with varying levels of technical expertise.Administrative Duties – 10%: Attend recurring organization meetings and one-on-ones with a supervisor. Engage in unplanned communications. Participate in recurring HR processes.Professional and Organizational Development – 5%: Maintain an awareness of industry, institutional, and organizational trends, best practices, and standard solutions. Socialize key concepts, industry best practices, and organizational principles, strategies, and conventions related to data and database administration to both technical and non-technical staff via mentoring, meetings, hands-on learning, and documentation. Become familiar with research administration in an R1 university setting in order to streamline its supporting business processes. MINIMUM REQUIREMENTS Bachelor’s degree in Engineering, Computer Science, Math/Statistics, Business Administration, or equivalent experience. 6 Years of experience in data engineering and database administration to include at least the following minimum professional experience Equivalent education/experience will substitute for all minimum qualifications except when there are legal requirements, such as a license/certification/registration. ADDITIONAL REQUIREMENTS 6 years hands-on experience as an individual contributor on database implementation, maintenance, and operation in accordance with industry standards, data policies, and security best practices using SQL Data and data infrastructure implementation and optimization for application development and maintenance. Microsoft SQL Server. Indexing strategies. DB performance and monitoring tools (such as SolarWinds, Datadog, RDS Performance Insights, and/or Redgate). Query profiling and optimization tools. ETL tools (such as SQL Server Integration Services (SSIS), AWS Glue, Informatica, or Alteryx). Modeling tools (such as erwin or DbSchema). Secure data management practices. 2 years’ experience configuring and managing databases in the AWS cloud environment including: RDS, EC2. Demonstrated ability to communicate clearly and effectively in both oral and written mediums with individuals and groups in order to socialize information and knowledge with a diverse group of colleagues. Demonstrated ease in technical and non-technical review as both reviewer and reviewee in order to facilitate collaborative group activities such as change control and pull request review. DESIRED QUALIFICATIONS Experience with AWS Services such as Glue, DMS, Quicksight, Redshift, data pipeline. Performing security audits and reviews of databases and data systems. REST API development and management platforms, such as MuleSoft or Boomi (Mulesoft preferred). Open-source database technologies: PostgreSQL, MySQL, MongoDB, Oracle, etc. Information Security standards and frameworks. Working in a Scrum-Agile environment. Workday suite of applications/modules. Workday data model, security, integrations, and data load capabilities. CONDITIONS OF EMPLOYMENT This position: Contributes in a collaborative teamwork environment. Collaborates across diverse backgrounds, personalities, and disciplines in a shared space working environment through scheduled and ad hoc meetings focused on problem solving. May be required to work in a hybrid environment, using remote and on-site workplaces. While normally has a Monday-Friday day shift, will occasionally have responsibilities or emergent situations where work outside of standard business hours is required to ensure software deployments go smoothly, to respond to incidents, and/or to meet critical deadlines. May be required to be on call. Is an essential position and is required to report to work when UW suspends operations when needed. Office of Research Executive Statement on Diversity, Equity, and Inclusion: The Office of Research shares President Cauce’s commitment to combat inequities and racism. The values of diversity, equity and inclusion are integral to the success of our research enterprise and are embedded in the culture of who we are as an institution and employer. We will honor different and unique identities and nurture an accessible, welcoming and respectful environment for all staff, students and faculty in the Office of Research and for all members of the UW research community. We will regularly review our services and systems and adapt them to reflect the evolving environments and work styles of our employees and those we serve. We are ALL responsible for confronting bias and inequities, both individual and institutional, that persist here and throughout our society. Our call is to remove barriers and promote access, opportunity, and justice for all. We all must commit to this ongoing work. Recognizing and valuing diversity will make the Office of Research and the University stronger, and enrich the stellar research carried out at the UW. Application Process: The application process may include completion of a variety of online assessments to obtain additional information that will be used in the evaluation process. These assessments may include Work Authorization, Cover Letter and/or others. Any assessments that you need to complete will appear on your screen as soon as you select “Apply to this position”. Once you begin an assessment, it must be completed at that time; if you do not complete the assessment, you will be prompted to do so the next time you access your “My Jobs” page. If you select to take it later, it will appear on your ""My Jobs"" page to take when you are access ready. Please note that your application will not be reviewed, and you will not be considered for this position until all required assessments have been completed. 

University of Washington is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to, among other things, race, religion, color, national origin, sexual orientation, gender identity, sex, age, protected veteran or disabled status, or genetic information.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/06028f69c1ea4eb2ab3037928cd7934atjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['Python', 'SQL', 'Container Orchestration', 'AWS', 'Serverless Computing', 'Snowflake', 'Data Analysis', 'Cloud Architecture', 'DevOps', 'Business Intelligence']"
50,Tekgence Inc,https://www.linkedin.com/company/tekgence-inc/life,Lead Data Engineer,https://www.linkedin.com/jobs/view/4257006838,4257006838,"San Francisco, CA",Hybrid,,2025-06-27 17:51:50,True,over 100 applicants,"Location: SFO, CA – HybridRole: AWS Data Engineer (Python, Dataiku) 
Job Brief
As an AWS Data Engineer, your role will be to design, develop, and maintain scalable data pipelines on AWS. You will work closely with technical analysts, client stakeholders, data scientists, and other team members to ensure data quality and integrity while optimizing data storage solutions for performance and cost-efficiency. This role requires leveraging AWS native technologies and Databricks for data transformations and scalable data processing.
ResponsibilitiesLead and support the delivery of data platform modernization projects.Design and develop robust and scalable data pipelines leveraging AWS native services.Optimize ETL processes, ensuring efficient data transformation.Migrate workflows from on-premise to AWS cloud, ensuring data quality and consistency.Design automations and integrations to resolve data inconsistencies and quality issues Perform system testing and validation to ensure successful integration and functionality.Implement security and compliance controls in the cloud environment.Ensure data quality pre- and post-migration through validation checks and addressing issues regarding completeness, consistency, and accuracy of data sets.Collaborate with data architects and lead developers to identify and document manual data movement workflows and design automation strategies.
Skills and Requirements7+ years of experience with a core data engineering skillset leveraging AWS native technologies (AWS Glue, Python, Snowflake, S3, Redshift).Experience in the design and development of robust and scalable data pipelines leveraging AWS native services.Proficiency in leveraging Snowflake for data transformations, optimization of ETL pipelines, and scalable data processing.Experience with streaming and batch data pipeline/engineering architectures.Familiarity with DataOps concepts and tooling for source control and setting up CI/CD pipelines on AWS.Hands-on experience with Databricks and a willingness to grow capabilities.Experience with data engineering and storage solutions (AWS Glue, EMR, Lambda, Redshift, S3).Strong problem-solving and analytical skills.Knowledge of Dataiku is neededGraduate/Post-Graduate degree in Computer Science or a related field

Nitesh Jaiswal | Tekgence Inc Linkedin:- linkedin.com/in/nitesh-jaiswal-a378b5222 Direct: 469-421-5604 , Ext- 218 • nitesh.j@tekgence.com  6655 Deseo Dr, Suite 104,Irving, TX , 75039 • www.tekgence.com",https://www.linkedin.com/job-apply/4257006838,"['Python', 'SQL', 'Containerization', 'AWS', 'Salesforce', 'Agile methodologies', 'Project management', 'Problem-solving', 'Communication skills', 'Data analysis']"
51,Lensa,https://www.linkedin.com/company/lensa/life,Associate Data Engineer - National Remote,https://www.linkedin.com/jobs/view/4258449660,4258449660,"Eden Prairie, MN",Remote,$59.5K/yr - $116.6K/yr,2025-06-27 14:06:46,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for UnitedHealth Group.

Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.

At Consumer Engineering EIMP team, we are committed to developing cutting-edge applications and products that utilize our proprietary matching algorithm to optimize member experiences. We manage vast amounts of data with significant business impact, creating solutions that improve the effectiveness and efficiency of the healthcare system. Our team is at the forefront of adopting next-generation technologies, constantly exploring new possibilities to drive meaningful change.

We are seeking an Associate Data Engineer to design, build, and maintain scalable data pipelines and storage solutions. The ideal candidate will have experience with SQL, ETL tools, big data technologies, and a growing interest or background in AI/ML systems and prompt engineering.

In this role, you will collaborate closely with data scientists and machine learning engineers to support model training, deployment, and monitoring. You’ll also contribute to the development of intelligent data-driven applications by crafting and optimizing prompts for large language models (LLMs) and other generative AI tools.

We value individuals who are willing to step out of their comfort zones, try new things, and learn from failure. If you are ready to embrace challenges and grow with us in the evolving world of data and AI, we encourage you to apply.

You will enjoy the flexibility to telecommute* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities

Develop, fine-tune, and deploy AI/ML models to solve complex business problems, ensuring scalability and performance in production environments Collaborate with cross-functional teams to integrate AI/ML solutions into existing systems, while continuously monitoring and refining model performance Assist in the development and maintenance of data pipelines for the Enterprise Individual Mastering Platform (EiMP), which consolidates individual data from multiple sources, including eligibility, pharmacy and patient data Support real-time data ingestion using tools like Kafka, as well as batch processing using Spark Perform data analysis using AWS tools such as Athena to support various business needs Generate reports and dashboards to provide insights and support decision-making processes Participate in the monitoring and maintenance of data quality and integrity across various data sources Assist in resolving data-related issues and discrepancies to ensure accurate and reliable data for the organization Contribute to the creation and maintenance of documentation for data pipelines, processes, and procedures Ensure compliance with data governance and security policies Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to understand data requirements and deliver solutions Provide support for ad hoc data requests and troubleshooting under the supervision of senior team members 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

High School Diploma/GED (or higher) 2+ years of expert level SQL development experience 2+ years of experience communicating with senior level business leaders and stakeholders 2+ years of experience with relational databases, database structures and design, systems design, data management, data warehouse and data profiling 2+ years of experience performing significant data analysis and data pipeline development 1+ years of experience with AI/ML workflows, model lifecycle or MLOps practices 1+ years of experience in assessing and interpreting customer needs and requirements and experience in a customer facing role leading/managing projects, including requirements and documentation 1+ years of experience with agile / scrum methodologies 1+ years of experience in Unix scripting 1+ years of experience in cloud platform (like AWS services, GCP etc.) 1+ years of experience developing reports in Tableau or equivalent reporting interface 

Preferred Qualifications

Experience of the different domains within health care Experience working with member demographic Data experience in implementation of ETL applications, Data warehousing/ data modelling principles, architecture and its implementation in large environments Experience in prompt engineering and working with LLMs Experience in working on sensitive, complex and urgent regulatory requests where data may be shared with internal and external clients Expert level of understanding complex problems and troubleshooting knowledge Solid soft skills (e.g. communication, interpersonal, collaborative, resilient) All Telecommuters will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

The salary range for this role is $59,500 to $116,600 annually based on full-time employment. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location, and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups, and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.

UnitedHealth Group is an Equal Employment Opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.

#RPO #GREEN

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/6d23cf5dbc8947348152d87e322ec92dtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['python', 'sql', 'containers', 'AWS', 'snowflake', 'agile methodologies', 'data analysis', 'UI/UX design', 'problem-solving', 'communication skills']"
52,Inherent Technologies,https://www.linkedin.com/company/inherent-technologies-pvt-ltd/life,Data Engineer Banking or FIS,https://www.linkedin.com/jobs/view/4258544284,4258544284,"Plano, TX",On-site,,2025-06-27 21:21:39,False,,"Mandatory Skills : big data +aws or azure + java + python + banking/capital markets + excellent 

job responsibilities

supports review of controls to ensure sufficient protection of enterprise dataadvises and makes custom configuration changes in one to two tools to generate a product at the business or customer requestupdates logical or physical data models based on new use casesinnovate new ways of managing, transforming and validating dataapply quality assurance best practices to all work productsfrequently uses sql and understands nosql databases and their niche in the marketplaceadds to team culture of diversity, equity, inclusion, and respect

Required Qualifications, Capabilities, And Skills

formal training or certification on data engineering disciplines and 3+ years applied experienceexperience across the data lifecycleadvanced at sql (e.g., joins and aggregations)working understanding of nosql database platforms and writing efficient sql'sknowledge of application, data and infrastructure architecture disciplinessolid experience in big data technologies; (spark, impala, hive, redshift, kafka, etc.)skilled in java and python development - must significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysisexperience customizing changes in a tool to generate productstrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy

Preferred Qualifications, Capabilities, And Skills

experience with data analytics/data management processes on awsfinancial services and commercial and investment banking experiencefamiliarity with dynamodb and/or cassandra",https://www.adzuna.com/details/5271450391?v=9C8802EFD8BB8EDC4B8D4D0E7424AE49A0367092&frd=46925b1be435b0614ec70c58dffcbb34&r=19676998&ccd=89da4ab549e1f62e781ee0828374beab&utm_source=linkedin7&utm_medium=organic&chnlid=1931&title=Data%20Engineer%20Banking%20or%20FIS&a=e,"['Microsoft Azure', 'Docker', 'Kubernetes', 'Linux administration', 'AWS', 'AWS CLI', 'Ansible', 'Docker', 'AWS S3', 'Snowflake']"
53,Foodsmart,https://www.linkedin.com/company/hellofoodsmart/life,Data Engineer,https://www.linkedin.com/jobs/view/4226046067,4226046067,United States,Remote,$130K/yr - $155K/yr,2025-06-25 14:45:23,False,,"About Us

Foodsmart is the leading telenutrition and foodcare solution, backed by a robust network of Registered Dietitians. Our platform is designed to foster healthier food choices, drive lasting behavior change, and deliver long-term health outcomes. Through our highly personalized, digital platform, we guide our 2.2 million members—including those in employer-sponsored health plans, regional and national Medicaid managed care organizations, Medicare Advantage plans, and commercial insurers—on a tailored journey to eating well while saving time and money.

Foodsmart seamlessly integrates dietary assessments and nutrition counseling with online food ordering and cost-effective meal planning for the entire family, optimizing ingredients both at home and on the go. We partner with national and regional retailers across the U.S., many of whom accept SNAP/EBT, making healthier food more accessible. Additionally, we assist members with SNAP enrollment and management, providing tangible access to nutritious food.In 2024, Foodsmart secured a $200 million investment from TPG’s Rise Fund, which supports entrepreneurs dedicated to achieving the United Nations’ Sustainable Development Goals. This investment will help us expand our reach, particularly to low-income workers who are disproportionately affected by diet-related diseases.

At Foodsmart, Our Mission Is To Make Nutritious Food Accessible And Affordable For Everyone, Regardless Of Economic Status. We Are Committed To a Set Of Core Values That Shape Our Culture And Work Environment

Measured: We make data-driven, truth-seeking decisions.

Impactful: We are fueled by achieving our mission and vision.

Collaborative: We help each other be better and create a positive environment.

Hungry: We maintain a healthy growth mindset, seeking to overcome challenges with courage.

Joyful: We take joy in each other, our work, and the privilege of doing this work.

Whether you're a dietitian, a commercial leader, or a technologist, working at Foodsmart means being part of a team that is passionate, supportive, and driven by a shared purpose. Join us in transforming the way people access and enjoy healthy food.

About The Role

The Data Engineer is a critical role responsible for constructing and optimizing our data pipeline architecture, collaborating closely with data scientists and analysts to facilitate data-related functionalities. The Data Engineer will be pivotal in designing, building, and maintaining highly scalable data pipelines, optimizing data delivery, and automating data processes. They will work closely with cross-functional teams to ensure efficient data flow and contribute to the success of our data-driven initiatives.

You Will

Own the optimization of data delivery for various cross-functional teams.Design, construct, install, test, and maintain highly scalable data pipelines.Collaborate closely with data architects, data scientists, and analysts to fulfill data requirements.Develop automated data processes for cleaning, validation, correction, and data mining.Identify, implement, and enhance internal process improvements, automating manual processes, and enhancing scalability.

You Are

Proactive and act as a driving force for efficient data delivery and infrastructure.Focused on quality and approach every data-related project with enthusiasm.Diligent in ensuring secure and compliant handling of data in accordance with relevant regulations.Collaborative and adept at addressing data-related technical issues and supporting stakeholders' data infrastructure needs.An expert in data warehouse architecture, data modeling, and automated data pipelines.

You Have

A minimum of 2 years of experience in a Data Engineering role.Hands-on experience with data warehouse solutions such as Snowflake or RedshiftExperience with cloud platforms such as AWS, GCP, or AzureAdvanced SQL knowledge and proficiency in working with relational databases.Familiarity with data pipeline and workflow management tools like Apache Airflow or Luigi.Strong analytical skills and the ability to thrive in a fast-paced environment.Familiarity with healthcare data standards like FHIR and HL7 is advantageous but not mandatory.Bachelor’s degree in Computer Science, Engineering, Mathematics, or related field; Master’s degree is a plus.

$130,000 - $154,999 a year

Role: Senior Data Engineer

Location: Remote

Base Salary Range: $130,000/yr to $155,000/yr + equity + benefits

Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries at our headquarters in San Francisco, California. Individual pay is determined by work location, job-related skills, experience, and relevant education or training.

About Our Benefits And Perks

Remote-First Company

Unlimited PTO

Healthcare Coverage (Medical, Dental, Vision)

401k, bonus, & stock options

Gym reimbursement

Foodsmart is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other protected class.",https://jobs.lever.co/foodsmart/3864c246-5eda-45e9-92e9-f6d804fa96b0/apply?source=LinkedIn,"['SQL', 'Python', 'AWS', 'Snowflake', 'Containers', 'Domain Expertise', 'Data Analysis', 'Data Visualization', 'Cloud Management', 'Cybersecurity']"
54,Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Analytics Engineer, New Grad & Entry Level",https://www.linkedin.com/jobs/view/4257051068,4257051068,United States,Remote,,2025-06-27 21:43:22,False,,"Who we areAdelaide is the leader in one of the fastest-growing areas of digital advertising: attention metrics. Since 2020, we’ve been a trusted measurement partner for 40% of Fortune 50 companies. They rely on our metric, AU, to maximize the effectiveness of media spend. AU is “the attention economy's most widely recognized metric,” according to Adweek, and we swept the measurement category in the 2024 Adexchanger awards.
Position OverviewThis position reports to the Senior Data Analytics Engineer; it will behave cross functionally across Data teams with an emphasis on supporting the Analytics team and their workflows.In this role, you will be joining a team of data scientists and engineers. You'll help build and maintain the semantic layer of our data pipeline, ensuring clean, consistent, and reusable data models. Day-to-day activities range from developing and testing data models, editing LookML, managing a data catalog, automating data analysis workflows, and working closely with stakeholders to understand and support their data needs.
What you'll learnAn important part of our culture is continuing education and the sharing of ideas. We offer:A large network of investors and advisors for you to access that will help your team succeedMentorship from executives with decades of experience in adtech and mediaRegular internal knowledge-sharing sessionsEducation budget to accelerate your team’s development
Core responsibilities:Assist in building, testing, and maintaining transformational data models (dbt, Redshift)Help create and manage a clean, reliable reporting layer in Looker using LookMLWork cross-functionally with emphasized support to the Analytics TeamHelp identify and automate manual data analysis processes (Python, dbt)Contribute to maintaining a well-organized data catalog with accurate, accessible metadata for both technical and non-technical audiencesEnsure metric consistency across dashboards and data toolsCollaborate with analysts and PMs to document key metrics and definitionsMonitor adoption and identify opportunities for improved data usability
What you'll bring:SQL Proficiency – Strong ability to write, optimize, and debug SQL queries. You also understand data modeling and warehouse best practices, and you’re committed to writing clean, readable, and well-documented code.BI & Visualization Tools – Hands-on experience with BI/dashboarding platforms (e.g., Looker, Tableau).Data Modeling Tools – Familiarity with semantic modeling tools (e.g., LookML) and dbt; understanding of ETL concepts. Experience with orchestration tools (e.g., Airflow) is a plus.Data Quality & Testing – Strong attention to detail in building data validation, profiling routines, and root cause analysis workflows.Programming Skills – Experience with Python for data transformation, scripting, and automation; experience with associated libraries (e.g., pandas, openpyxl) is a plus.Communication & Collaboration – Excellent interpersonal skills with experience working cross-functionally; ability to translate technical concepts for non-technical audiences and deliver training/support.Educational Background & Experience – Bachelor’s degree in a quantitative, technical, or analytical field (e.g., Computer Science, Math, Physics, Engineering) or a rigorous coding bootcamp with a portfolio demonstrating the above skills.",https://jobright.ai/jobs/info/6859f854d3b885d69bbfe235?utm_source=1124&utm_campaign=6859f854d3b885d69bbfe235&tob=true,"['Cloud computing', 'agile methodologies', 'SQL', 'Python', 'AWS', 'containerization', 'Docker', 'database management', 'competency assessment', 'leadership']"
55,AMC Health,https://www.linkedin.com/company/amc-health/life,"AI, ML and Data Engineer",https://www.linkedin.com/jobs/view/4258469871,4258469871,United States,Remote,$125K/yr - $135K/yr,2025-06-27 15:35:50,True,over 100 applicants,"AI, ML and Data Engineer 
About AMC Health: AMC Health is a leading virtual care and digital health company, founded on the mission to help people lead healthier, longer lives from the comfort of their homes. We achieve this by providing proactive insights to spot complications earlier and delivering the clinical support needed for patients to thrive. With over 20 years of experience, AMC Health is trusted by health plans, providers, and government entities to deliver advanced remote patient monitoring solutions that improve quality scores, close care gaps, and reduce readmissions. We leverage real-time data, advanced machine learning algorithms, and dedicated clinical teams to provide unparalleled precision in care management and coordination.
Job Summary: As a Remote AI, ML, and Data Engineer at AMC Health, you will play a critical role in advancing our mission by building and optimizing the data infrastructure and machine learning capabilities that power our innovative virtual care platform. You will be involved in the entire lifecycle of one of the most unique healthcare data repositories in the country, from secure ingestion and transformation to the deployment and monitoring of AI/ML models that generate actionable insights for improved patient outcomes. This is an exceptional opportunity to apply your technical expertise to real-world healthcare challenges, making a tangible difference in people's lives from a remote setting.
Job Responsibilities
AI and ML Engineering (AWS SageMaker & Amazon Bedrock)Build, train, and deploy machine learning models using SageMaker.Automate model workflows with SageMaker Pipelines.Monitor deployed models for performance and manage data drift using SageMaker Model Monitor.Develop and integrate large language models (LLMs) through Amazon Bedrock, ensuring effective implementation for various business use cases.Leverage agentic AI frameworks to build intelligent, autonomous AI agents capable of decision-making tasks.Data EngineeringDesign, implement, and optimize scalable ETL pipelines.Design and maintain data models, schemas, and database structures for analytical and reporting use cases in SQL.Optimize data storage solutions for performance and scalability.Machine Learning Operations (MLOps) and AI EngineeringImplement end-to-end ML and AI workflows using AWS services.Develop reproducible training pipelines and robust CI/CD workflows for ML models using SageMaker and infrastructure as code (IaC) through AWS CDK.Ensure scalable model deployment, monitoring, and continuous improvement within production environments.API IntegrationDesign and integrate RESTful APIs or event-driven architectures for serving data, ML models, and AI-driven applications.
Job Requirements
Bachelor’s or Master’s degree in Computer Science, Data Engineering, or a related field.Proficiency in Python programming, including experience with libraries for data manipulation (e.g., Pandas, NumPy) and machine learning (e.g., Scikit-learn, TensorFlow, PyTorch).Strong knowledge of database systems, data modeling techniques, and SQL proficiency.AWS certifications, particularly in machine learning or cloud architecture, are highly desirable, including experience with SageMaker features (endpoints, AutoML, hyperparameter tuning, batch transform) and familiarity with Amazon Bedrock.Experience or strong interest in agentic AI frameworks, AI engineering best practices, and LLM deployment.Proven ability to implement and manage CI/CD processes and infrastructure as code (IaC) using AWS CDK for scalable, maintainable systems.Strong communication skills to collaborate effectively with data scientists, analysts, and business stakeholders.Proven ability to address complex business challenges using scalable, reliable, and maintainable data, ML, and AI solutions.Demonstrates a very positive attitude toward learning and rapidly adopting new technologies in a highly dynamic technological environment.
Disclaimer:The above statements are intended to indicate the general nature and level of work performed by employees within this position. They are not designed to contain or be interpreted as an exhaustive list of all duties, responsibilities, skills, and qualifications required of employees assigned to this job.
AMC Health is an equal opportunity and affirmative action employer and ensures that all qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, national origin, disability or veteran status.
Job Type: Full-timeSalary Range: $125,000 - $135,000Benefits:401(k)Dental insuranceHealth insuranceLife insurancePaid time offVision insuranceWork Location: Remote

",https://www.linkedin.com/job-apply/4258469871,"['Java', 'Java Spring', 'Java EE', 'Spring Boot', 'Spring Framework', 'Spring Security', 'Hibernate API', 'Spring Data', 'Docker', 'AWS EC2', 'CI/CD', 'docker', 'container', 'AWS S3', 'AWS RDS', 'CI tool like Jenkins', 'Kubernetes (optional)', 'postgresql', 'C#', 'AWS Cognito', 'Spark', 'AWS Lambda', 'AWS SNS', 'AWS SES', 'AWS OpsWorks', 'AWS VPN', 'Docker API', 'Spark API', 'AWS SDK', 'SQL', 'CloudWatch', 'IAM', 'EC2', 'locks', 'EC2 auto scaling', ""NOTE: There are duplicate skills such as 'Java'"", ""'Docker"", ""' and 'AWS AMI' removed"", 'to obtain clear top 10 skills.']"
56,Pride Health,https://www.linkedin.com/company/pride-health/life,Database Engineer,https://www.linkedin.com/jobs/view/4255835360,4255835360,"Greensboro, NC",Remote,$40/hr - $45/hr,2025-06-27 17:02:38,True,over 100 applicants,"Job Title: Database EngineerShift: Monday – Friday, 5x8 (starting as early as 6 AM EST)Location: Remote (Must reside on the U.S. East Coast)Contract Duration: 56 weeks
Job Summary:The Database Engineer (Intermediate) will design, analyze, and modify Electronic Health Record (EHR) software applications. Working under moderate supervision, this role involves encoding, testing, debugging, and installing software modules to ensure seamless system operation. The candidate will play a vital role in maintaining secure, scalable, and efficient database solutions tailored to organizational needs.Key Responsibilities:Analyze and implement database designs to ensure user-friendly and secure systems.Monitor database performance, scalability, and security to address performance issues and optimize usage.Evaluate existing database designs, ensuring they meet organizational needs and improve system efficiency.Develop and maintain data architectures for databases and data warehouses.Assist in troubleshooting complex data processing issues during migrations and conversions.Contribute to strategy development for data acquisition, archive recovery, and database implementation.Requirements:Residency: Must reside on the U.S. East Coast (availability to start at 6 AM EST).Education: Bachelor’s Degree in a related field.Experience: 4+ years of relevant experience, including:Proficiency in Power BI and SQL.At least 1 year of experience with Health Catalyst platform.Certifications: Platform-centric certification required.Additional Experience: Hands-on experience with EHR applications is preferred.BenefitsPride Global offers eligible employees comprehensive healthcare coverage (medical, dental, and vision plans), supplemental coverage (accident insurance, critical illness insurance, and hospital indemnity), 401(k)-retirement savings, life & disability insurance, an employee assistance program, legal support, auto, home insurance, pet insurance, and employee discounts with preferred vendors.As a certified minority-owned business, Pride Global and its affiliates - including Russell Tobin, Pride Health, and Pride Now - are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.
Equal Opportunity EmployerAs a certified minority-owned business, Pride Global and its affiliates - including Russell Tobin, Pride Health, and Pride Now - are committed to creating a diverse environment and are proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.",https://www.linkedin.com/job-apply/4255835360,"['Python', 'Python Flask', 'JavaScript', 'MongoDB', 'AWS', 'Docker', 'full-stack development', 'Agile methodologies', 'communication', 'teamwork']"
57,APR Consulting,https://www.linkedin.com/company/aprconsulting/life,Database Engineer,https://www.linkedin.com/jobs/view/4257014007,4257014007,"Greensboro, NC",Remote,$48/hr,2025-06-27 18:22:09,True,over 100 applicants,"APR Consulting, Inc. has been engaged to identify a Database Engineer

 

Location: Greensboro, NC 27401

Position: Database Engineer

Pay Rate: $48/hr

Duration: 8 weeks

Expected Shift: 40 hrs/week

Shift: 5X8 Monday - Friday, may require to be on call - starting time as early as 6AM EST

Location: Remote - MUST reside on United States East Coast ; facility to provide laptop, docking station and two monitors

 

 

JOB SUMMARY

ALL CANDIDATES MUST COMPLETE T PRESCREEN QUESTIONS TO BE CONSIDERED. 

  

**NOT looking for candidates with database administrator experience**

 

Job Summary:

The Database Engineer Intermediate designs, analyzes, and modifies Electronic Health Record (EHR) software applications for the organization. Working under moderate supervision, the Database Engineer Intermediate encodes, tests, debugs, and installs the software modules to ensure the system is running smoothly and efficiently for users.

 

Requirements:

- Must reside on United States East Coast - candidates may be subjected to working as early as 6AM EST

-Bachelor's Degree in related field

- 4+ years of relevant experience

- Platform-centric Certification

- Power BI and SQL experience required

- 1+ Years of Health Catalyst platform experience

 

 

Responsibilities :

Analyzes the implementation and design of organization databases to ensure databases are both user-friendly andsecure.

Monitors database performance, scalability and security to identify and resolve performance issues and ensure optimaldatabase performance.

Prepares processes to evaluate existing database designs to determine if final solutions meet organizational needs andimprove the overall system.

Participates in the creation of data architectures for databases and data warehouses to ensure a well-structured andefficient environment.

Reviews data migrations/conversions and seeks advice from senior management regarding troubleshooting dataprocessing issues that may be complex or difficult.

Researches the strategy development for data acquisition, archive recovery, and database implementation to ensuresuccessful implementation is aligned with organizational needs.

 

 

 

Our client is the one of the largest Healthcare Staffing Provider in the United States, to be assigned at one of their affiliated hospitals/healthcare facilities.

 

 

This particular client is requiring that all new hires show proof of vaccination. However, accommodations may be made for those with disabilities or religious reasons who cannot obtain a vaccine.

 

Since 1980 APR Consulting, Inc. has provided professional recruiting and contingent workforce solutions to a diverse mix of clients, industries, and skill sets nationwide.

 

We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.

 

Don't miss out on this amazing opportunity! If you feel your experience is a match for this position please apply today and join our team. We look forward to working with you!



#HC1",https://www.linkedin.com/job-apply/4257014007,"['backend', 'AI/ML', 'cloud computing', 'data visualization', 'JavaScript', 'Kubernetes', 'Git', 'SQL', 'AWS']"
58,Lensa,https://www.linkedin.com/company/lensa/life,Data Platform Engineer - Remote,https://www.linkedin.com/jobs/view/4256758077,4256758077,United States,Remote,,2025-06-27 13:54:07,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Akamai.

Does building the next generation of AI/ML platforms excite you?

Do Big Data challenges and open-source innovation speak your language?

Join our Aegis team

Aegis is an end-to-end Big Data AI/ML platform built on top of Akamai's Public Cloud (Linode). It bridges the gap between research and production, enabling teams to innovate faster and deliver ML products more efficiently. We leverage open-source tools and AI accelerators to avoid reinventing the wheel – integrating across platforms where it matters most.

Make a difference in your own way

You’ll join our growing Engineering group, working hands-on to shape a powerful, flexible ML platform that supports real-world, large-scale data and AI workflows. You’ll be a key contributor to an environment that spans Kubernetes, Spark, MLflow, JupyterHub, and PyTorch – and that pushes the boundaries of performance and collaboration.

As a Data Platform Engineer, you will be responsible for:

Designing and implementing scalable data and ML pipelines using Apache Spark or other distributed processing frameworks Building platform components in Python or Scala to connect research and production environments Integrating with orchestration tools such as Argo Workflows (or equivalents) Supporting a hybrid-cloud platform, including Azure, Akamai's public cloud (Linode), and more Collaborating with researchers and MLOps engineers to support the ML lifecycle from exploration to production Ensuring basic observability across components, including monitoring and alerting tools such as Prometheus or Grafana Working with open table formats such as Delta Lake, Iceberg, or similar a plus 

Do What You Love

To be successful in this role you will:

Have 4+ years of experience in backend or data platform development Be proficient in Python or Scala (Java) Have significant hands-on experience with Docker and Kubernetes, or with equivalent containerization and cluster management technologies Have strong understanding of Big Data principles, with hands-on experience in distributed data processing using Apache Spark (on Databricks, or similar platforms) or equivalent frameworks such as Ray or Dask Have hands-on experience working with data scientists throughout the ML lifecycle - from experimentation to deployment - using tools such as MLflow, Jupyter Notebooks, and PyTorch or TensorFlow, and integrating these into scalable production pipelines Have experience with at least one cloud provider (Azure, AWS, or GCP) Have experience with monitoring and observability tools such as Prometheus, Grafana, OpenTelemetry, or ELK/Opensearch Have experience with Argo Workflows or similar orchestration tools Be proactive, motivated, curious, and thrive in a collaborative and high-ownership environment 

Build your career at Akamai

Our ability to shape digital life today relies on developing exceptional people like you. The kind that can turn impossible into possible. We’re doing everything we can to make Akamai a great place to work. A place where you can learn, grow and have a meaningful impact.

With our company moving so fast, it’s important that you’re able to build new skills, explore new roles, and try out different opportunities. There are so many different ways to build your career at Akamai, and we want to support you as much as possible. We have all kinds of development opportunities available, from programs such as GROW and Mentoring, to internal events like the APEX Expo and tools such as Linkedin Learning, all to help you expand your knowledge and experience here.

Learn more

Not sure if this job is the right match for you or want to learn more about the job before you apply? Schedule a 15-minute exploratory call with the Recruiter and they would be happy to share more details.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/2a703d96490d4f99b549937945061893tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['SQL', 'Container Management', 'AWS', 'Snowflake', 'Docker', 'Kubernetes', 'Java', 'Android Development', 'Python Development', 'Cybersecurity']"
59,Jobright.ai,https://www.linkedin.com/company/jobright-ai/life,"Data Engineer, New Grad & Entry Level",https://www.linkedin.com/jobs/view/4257043977,4257043977,"Irving, TX",On-site,,2025-06-27 21:35:49,False,,"Why GMF Technology?
GM Financial is set to change the auto finance industry and is leading the path of embarking on tech modernization – we have a startup mindset, and preserve our small company culture, in a public company environment with financial stability and intense growth over a decade-plus history. We are *data* junkies and trust in *data* and insights to advance our business objectives. 
We take our goal of zero emission, zero collision, zero congestion, and zero friction very seriously. We believe as an auto finance market leader we are in the driver's seat to lead us in the GM EV mission to change the world. We are building global platforms, in LATAM, Europe, China, U.S. and Canada – and we are looking to grow our high-performing team. 
GMF is comprised of over 10,000 team members globally. Join our fintech culture within a Blue-Chip company where we are changing the way we use technology to support our customers, dealers and business.
As a Data Engineer I, you will apply software and data engineering practices to design, develop, and maintain scalable DataOps and MLOps solutions. You will support the full lifecycle of analytical models, focusing on automation, deployment, monitoring, and compliance. This role requires collaboration with cross-functional teams to deliver impactful data and analytics solutions.",https://jobright.ai/jobs/info/685d76a9fe79d35d9c32bfe9?utm_source=1124&utm_campaign=685d76a9fe79d35d9c32bfe9&tob=true,"['API documentation', 'cloud computing', 'AI and ML', 'data analysis', 'project management', 'problem-solving', 'communication', 'collaboration', 'SQL', 'AWS']"
60,Lensa,https://www.linkedin.com/company/lensa/life,Big Data Systems Engineer (Remote),https://www.linkedin.com/jobs/view/4256759102,4256759102,"Los Angeles, CA",Remote,$150K/yr,2025-06-27 13:54:03,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for KBR.

Title

Big Data Systems Engineer (Remote)

Belong, Connect, Grow, with KBR!

KBR’s National Security Solutions (NSS) team provides high-end engineering and advanced technology solutions to our customers in the intelligence and national security communities. In this position, your work will have a profound impact on the country’s most critical role – protecting our national security.

KBR is seeking a Big Data Systems Engineer to join our team. The successful candidate will be part of the KBR team supporting the Test Resource Management Center’s (TRMC) Big Data (BD) and Knowledge Management (KM) Team deploying BD and KM systems for DoD testing Ranges and various acquisition programs

Big Data Systems Engineer – Job Summary

This is being hired nationwide as it is a remote work capable position.

The candidate can either work in one of KBR’s facilities or work from home, assuming the candidate has a stable internet connection.

The Big Data Systems Engineer will work on the deployment and integration of a highly visible data analytic project called Cloud Hybrid Edge-to-Enterprise Evaluation Test & Analysis Suite (CHEETAS) at multiple DoD ranges and labs. As a Big Data Systems Engineer, you will be a critical part of our technical team responsible for deploying CHEETAS within customer environments. You will be the frontline interface that customers will have when first experiencing CHEETAS within their DoD Range and lab environments. This position will require you to work closely with system administrators and software developers to communicate, document and ultimately resolve deployment issues as they arise. You will deploy CHEETAS within disparate DoD testing Ranges and acquisition programs environments (on different non-standard hardware stacks and integrated into different existing ecosystems) sometimes located within DoD vaults with no outside internet connectivity. Come join the KBR BDKM team and be a part of the award-winning team responsible for revolutionizing how data analysis is performed across the entire Department of Defense! 

Roles And Responsibilities

Work on the deployment and integration of a highly visible data analytic project called Cloud Hybrid Edge-to-Enterprise Evaluation Test & Analysis Suite (CHEETAS) at multiple DoD ranges and labs Deploy CHEETAS within customer environments Work closely with system administrators and software developers to communicate, document and ultimately resolve deployment issues as they arise Deploy CHEETAS within disparate DoD testing Ranges and acquisition programs environments (on different non-standard hardware stacks and integrated into different existing ecosystems) sometimes located within DoD vaults with no outside internet connectivity 

Basic Qualifications

Must have an active U.S. government TS/SCI security clearance to be considered for this position This position requires a bachelor's degree in a STEM Computer Science, Data Science, Statistics or related, technical field, and 10 years of DoD experience. Entry level Integration Engineers will NOT be considered due to the breadth of knowledge necessary to be successful in the position. Previous experience must include integration with and configuration of: Hadoop, SQL Server Big Data Cluster, Kubernetes, CentOS, Ubuntu, RedHat, Windows Server, VMWare, etc.) Previous experience must include five (5) years of hands-on experience in big data environments. 

Knowledge / Skills / Abilities

Must be adept at deploying and configuring Big Data and Knowledge Management tools in an enterprise environment. Must have extensive technical expertise in the configuration and troubleshooting of big data ecosystems. Must have excellent written and verbal communication skills and be comfortable assisting customers with installation and configuration of their big data infrastructure. Must have strong troubleshooting skills and the ability to become a CHEETAS deployment subject matter expert. Must be comfortable working with a wide range of stakeholders and functional teams at various levels of experience. Excellent interpersonal skills, oral and written communication skills, and strong personal motivation are necessary to succeed within this position. Experience with installation, configuration, integration with and usage of the following tools and technologies: Helms Charts, YAML, Kubernetes, Kubectl, Kubernetes IDE, NFS, SMB, S3, SQL Server, Windows Server, Windows 10/11, Linux (CentOS, Ubuntu, RedHat), Hadoop. Must be prepared to learn new business processes or CHEETAS application nuances every Agile sprint release (roughly every 6 weeks) prior to deploying to customer sites. Ability to problem solve, debug, and troubleshoot while under pressure and time constraints is required. Ability to communicate effectively about technical topics to both experts and non-experts at both the management and technical level is required. Ability to work independently and provide appropriate recommendations for optimal design, analysis, and development. Excellent verbal communications skills are required, as the Integration Engineer will be in frequent contact with the project technical lead, be taking direction from various government leads, and will frequently be interacting with end users to gather requirements and implement solutions while away from other team members. Excellent testing, debugging and problem-solving skills are required to be successful in this position. Experience designing, building, integrating with and maintaining both new and existing big data systems and solutions. Ability to speak and present findings in front of large groups. Ability to document and repeat procedures. This position is anticipated to require travel of 25% with surges possible up to 50% to support end users located at various DoD Ranges and Labs across the United States. 

Preferred Qualifications

10+ years of experience working in government / defense labs and within their computing restrictions. Experience working in government/defense labs and their computing restrictions. Experience working with major DoD acquisition programs, such as Joint Strike Fighter (JSF). Knowledge of the Test and Training Enabling Architecture (TENA), the Joint Mission Environment Testing Capability (JMETC) and distributed testing and training. Experience with working in distributed team environment is preferred. Ability to teach and mentor engineers with a variety of skill levels and backgrounds is a plus. Knowledge of DoD cybersecurity policies. 

KBR Benefits

KBR offers a selection of competitive lifestyle benefits which could include 401K plan with company match, medical, dental, vision, life insurance, AD&D, flexible spending account, disability, paid time off, or flexible work schedule. We support career advancement through professional training and development.

Basic Compensation

$150,000-200,00

Belong, Connect and Grow at KBRAt KBR, we are passionate about our people and our Zero Harm culture. These inform all that we do and are at the heart of our commitment to, and ongoing journey toward being a People First company. That commitment is central to our team of team’s philosophy and fosters an environment where everyone can Belong, Connect and Grow. We Deliver – Together.

KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/02bf1baeb81f4b62aa0c0e7aa8da6a38tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=None&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['sql', 'container-based', 'AWS', 'Amazon Web Services', 'Amazon RDS', 'cloud', 'Azure', 'Microsoft Azure', 'Kubernetes', 'Linux', 'TypeScript', 'Python', 'Note: The skills are inferred from keywords typically associated with operating virtual or containerized environments across cloud platforms such as AWS', 'Azure', 'and Google Cloud. Actual job titles might require different specific technologies mentioned in job descriptions.']"
61,Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256760241,4256760241,"Topeka, KS",Remote,$90K/yr - $110K/yr,2025-06-27 13:53:48,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/50634e1a357f4c3299bf6cdc4850c189tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'sql', 'cloud architecture', 'AWS', 'big data', 'container management', 'machine learning', 'Agile methodology', 'DevOps', 'Python', 'SQL', 'Cloud architecture', 'AWS', 'Big data', 'Container management', 'Machine learning', 'Agile methodology', 'DevOps']"
62,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256760183,4256760183,"Indianapolis, IN",Remote,$118K/yr - $122K/yr,2025-06-27 13:53:52,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/82bcc3314dfa428d98c5a16064e1edb2tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['marketing Automation ', 'Lease Management ', 'POS Management ', 'Event Management ', 'Content Marketing ', 'Real Estate Leasing ', 'Selling ', 'Mortgage Process Automation ', 'Loans ', 'CRM Maria', '- Marketing Automation', '- Lease Management', '- POS Management', '- Event Management', '- Content Marketing', '- Real Estate Leasing', '- Selling', '- Mortgage Process Automation', '- Loans', '- CRM']"
63,Aha!,https://www.linkedin.com/company/aha-labs-inc-/life,Sr. Platform Engineer,https://www.linkedin.com/jobs/view/4258296984,4258296984,"Miami, FL",Remote,,2025-06-27 07:54:28,False,,"Aha! is the world's #1 product development software. We help more than 1 million product builders go from discovery to delivery and bring their strategy to life. Our suite of tools includes Aha! Roadmaps, Aha! Discovery, Aha! Ideas, Aha! Whiteboards, Aha! Knowledge, Aha! Teamwork, and Aha! Develop. Product teams rely on our expertise, guided templates, and training programs via Aha! Academy to be their best. We are proud to be a very different type of high-growth SaaS company. The business is self-funded, profitable, and 100% remote. We are recognized as one of the best fully remote companies to work for, champion the Bootstrap Movement, and have given over $1M to people in need through Aha! Cares. Learn more at www.aha.io.

Our team

Aha! engineering is a mid-sized, fully remote team that is highly productive. We are centered around North American time zones so we can collaborate during the workday.

We move quickly: We ship code multiple times a day. We believe in getting new features in front of customers and iteratively improving as we learn what works and what does not.We collaborate: We each bring unique experiences and skills to the table. Working together to share that knowledge benefits the entire team and helps us produce the best results for our customers.We value product over process: We want the team to have the time and focus to solve complex challenges. We aim to minimize the overhead introduced by heavyweight processes and excessive meetings.We enjoy: We like what we do. And we want you to love your job too. Learn more about The Responsive Method, our company values, and the generous benefits we offer.

Our technology

Our web application is a single-instance, multi-tenant Ruby on Rails monolith supported by Postgres (database), Redis (background jobs), memcached (Rails caching), and Kafka. We also run a Node.js webserver to support collaborative editing and real-time updates. Our application is hosted on Amazon Web Services and architected with ECS for reproducibility and scalability.

We embrace new technologies that help us deliver a lovable product, but we also remain cognizant of the maintenance overhead that a new library or platform brings. We solve the problems in front of us rather than prematurely optimizing to address issues that may never materialize. That said, some of the largest companies in the world store a large amount of their precious strategic data in Aha!, so we do plenty of optimizing too.

We do most of our planning and collaboration in Aha! Roadmaps and built Aha! Develop so that software engineers and their teams could take advantage of those same rich features. We use Slack and Zoom for video calls. (Email? Rarely.)

Your experience

Skills

We believe that being a kind person who elevates the rest of the team is just as valuable as writing great code. You have strong problem-solving skills and experience working on important functionality for a cloud-based product. You are humble, eager to learn, and always willing to help others learn as well. You want to work with people who enjoy picking up a problem and solving it, regardless of the technologies and techniques involved. You have worked at meaningful scale before and want to do so again. You also have the below experience and skills:

Four+ years of experience working in Ruby on Rails as well as Rails codebaseManaging AWS infrastructure using terraformUsing containerization and build systemsProducing fast, efficient, and reliable servicesWriting high-performance systems based on Kafka

Your work at Aha!

As Sr. Platform Engineer, You Will Support The Rest Of The Engineering Team By Developing Abstractions, Frameworks, And Tools Which Help Other Aha! Engineers Build The Product. This Includes

Building developer tooling such as CI pipelines, local and staging environments, and helpful CLI tools. You will also assist the SRE team in building out monitoring and observability pipelines.Building infrastructure services to be used by Aha! product developers. This will usually include writing a reference architecture in our main Rails app.Working with the SRE team to write and maintain well-written production runbook and operations documentationConsulting with application developers on performance optimization or scaling challenges

If the Sr. Platform Engineer role sounds appealing, we would love to hear from you. (A real human reviews every application.)

Grow with us

Everyone deserves to reach their fullest potential. We know that when we do work that matters with people we care about in a high-growth environment, we feel engaged and alive. It is why we joined Aha! and how we achieve our very best.

We offer all the benefits you would expect and more, including profit sharing. The specific benefits listed below are reflective of what we offer U.S.-based hires. We also do our best to extend identical benefits to international teammates.

The base salary range for this role in the U.S. is between $110,000 and $190,000Cash-based compensation also includes profit sharing, and we contribute a percentage of your total pay each month toward your retirementMedical, dental, and vision plans (for many teammates, we cover 100% of the premiums)Up to 200 hours of paid time off a year to spend however you want30 to 90 days of paid parental leave and five to 10 days of paid care and bereavement leaveUp to $1,000 annually for third-party education, along with paid time off to immerse yourself in learningVolunteer opportunities throughout the year

Base salary and total compensation are dependent upon many factors, including skills, experience, and relevant past roles.",https://www.aha.io/company/careers/current-openings/sr-platform-engineer-remote,"['Javascript', 'HTML5', 'CSS3', 'React', 'Node.js', 'MongoDB', 'Azure Blob Storage', 'Azure DevOps', 'Hashicorp Vault', 'PostgreSQL']"
64,Piper Companies,https://www.linkedin.com/company/pipercompanies/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4257031427,4257031427,United States,Remote,$160K/yr - $165K/yr,2025-06-27 20:21:49,False,,"Piper Companies is seeking a Senior Data Engineer, to join a leading shipping and logistics organization. The Senior Data Engineer will be responsible for building robust & scalable code to transform and deliver data for reporting & analytics.

Responsibilities of the Senior Data Engineer include:

Utilize advanced knowledge of Azure Databricks to design, develop, and deploy solutions for reporting and analytics.Construct real-time streaming pipelines using tools like Streamsets and Qlik.Manage data movement and orchestration through Azure Data Factory.Partner with internal customers and stakeholders to gather requirements, build robust models, and deploy solutions. Create and maintain data management frameworks to properly tag and manage curated data assets.

Qualifications for the Senior Data Engineer include: 

10+ years of experience in data strategies, data analysis, database architecture and management, development, or administration.Strong skills in Azure Data Factory, Databricks, SQL, Python (PySpark), and/or Scala.Proven experience in building ETL processes and data warehouse transformation processes.Hands-on experience designing and delivering solutions using the Azure Data Analytics platform, including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, and Azure Stream Analytics.Experience with Apache Kafka and event-based data processing.Bachelor's degree in Computer Science

Compensation for the Senior Data Engineer include:

Salary: $160,000 - $165,000 Comprehensive Benefits: Medical, Dental, Vision, 401K, PTO, Sick Leave as required by law, and Holidays

This job opens for applications on May 14, 2025. Applications for this job will be accepted for at least 30 days from the posting date.

Keywords: Senior Data Engineer, Azure Databricks, Data Analytics, Real-Time Streaming, Azure Data Factory, ETL Processes, Data Warehousing, Python (PySpark), Scala, SQL, Apache Kafka, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics, Database Architecture, Data Management, Big Data, Cloud Services, Data Transformation, Data Pipeline.

",https://careers.pipercompanies.com/details/144565/senior_data_engineer,"['Agile ', 'Communication ', 'Requirements Engineering ', 'Testing ', 'Automation ', 'AWS ', 'SQL ', 'Linux', 'Scrum Master ', 'Data Analysis']"
65,UNCOMN,https://www.linkedin.com/company/uncomn/life,Data Engineer 2,https://www.linkedin.com/jobs/view/4258511822,4258511822,"O'Fallon, IL",On-site,$80K/yr - $95K/yr,2025-06-27 19:55:12,False,,"At UNCOMN, we don’t just fix issues; we deliver secure solutions that power essential operations in both the public and private sectors. Our team tackles tough challenges—whether it’s improving organizational systems, streamlining logistics, or making sense of complex data—all with one goal: helping our clients succeed so they can continue serving others.
We’ve built a culture rooted in our Core Values, where innovation thrives, work-life balance is respected, and career growth is always encouraged. We're also proud to be recognized as a ‘Top Workplace,’ and that’s not just us talking; it’s our employees. Now, we’re looking for another Uncommon Genius to join our dynamic team. If you love solving puzzles, building new things, fixing what’s broken, or pushing the boundaries of creativity, we encourage you to explore the details of this position below!
UNCOMN is seeking a Data Engineer 2 (Associate) to:Design, develop, and maintain data integration pipelines using tools such as Databricks and/or Foundry, within the Advana and/or Palantir data environment, to acquire, ingest, and transform data from various sources.Ensure data quality, accuracy, and consistency by implementing data cleansing and validation processes.Create and maintain data models and schemas that support efficient data analysis and reporting.Develop and optimize ETL processes to efficiently extract, transform, and load data into data warehouses and analytics platforms.Automate data processing workflows for scalability and reliability.Monitor system performance, identify bottlenecks, and implement optimizations to ensure data pipelines and systems run smoothly.Work on continuous improvement of data engineering processes and best practices.Implement data security measures to protect sensitive information and ensure compliance with relevant data protection regulations.Assist in auditing and documenting data lineage and access controls.Collaborate closely with cross-functional teams, including data scientists, analysts, and business stakeholders, to understand their data needs, provide timely support, understand business requirements, and translate them into effective data structures.Epitomize UNCOMN Core Values.
Requirements3+ years of related professional experience.Must have an active, fully adjudicated clearance, granted by the US Government.Prior experience with scripting languages like Python and SQL for data analysis tasks—including data cleaning, manipulation, joining, and visualization—as well as exposure to Business Intelligence tools such as Tableau, Power BI, and Qlik Sense for reporting and dashboard creation.
PreferredActive (top) secret clearance granted by the US Government.Bachelor of Science in STEM-related (Science, Technology, Engineering, Math, etc.) major.Accredited Data Engineering-related certification(s), Certified Data Management Professional (CDMP) certification preferred.Experience utilizing geospatial tools, Python, PySpark, Databricks, Qlik, and platforms like Advana and Palantir to perform data analysis tasks—including time series analysis, data exploration, statistical analysis (e.g., hypothesis testing, regression), and predictive modeling.
Why UNCOMN?Instant Flexible PTO: Enjoy flexible paid time off starting your very first day with us!Generous Holidays: Benefit from 7 paid holidays and up to 3 floating holidays annually.Immediate Health Coverage: Get access to comprehensive health benefits from day one.401K Safe Harbor Match: Secure your future with our top-tier 401K matching program.Performance Bonus: Stand out and you could earn an annual performance bonus.Growth Opportunities: Advance your career with our training and education assistance programs.Free Employee Assistance Program (EAP): Access complimentary support services for you and your family.Note: Benefits apply to full-time employees only.
Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",https://uncomn.com/careers/?gnk=job&gni=8a7887a197ae195e0197b2232560428c&gns=LinkedIn+Premium,"['SQL', 'Python', 'Data Analysis', 'AWS', 'Cloud Management', 'AWS Certified Solutions Architect', 'Snowflake Certification', 'Big Data Management', 'AWS Certified SysOps Administrator']"
66,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer,https://www.linkedin.com/jobs/view/4258454332,4258454332,"Seattle, WA",On-site,$67.30/hr - $77.30/hr,2025-06-27 14:06:42,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Accenture.

Accenture Flex offers you the flexibility of local fixed-duration project-based work powered by Accenture, a leading global professional services company. Accenture is consistently recognized on FORTUNE's 100 Best Companies to Work For and Diversity Inc's Top 50 Companies For Diversity lists.

As an Accenture Flex employee, you will apply your skills and experience to help drive business transformation for leading organizations and communities. In addition to delivering innovative solutions for Accenture's clients, you will work with a highly skilled, diverse network of people across Accenture businesses who are using the latest emerging technologies to address today's biggest business challenges.

You will receive competitive rewards and access to benefits programs and world-class learning resources. Accenture Flex employees work in their local metro area onsite at the project, significantly reducing and/or eliminating the demands to travel.

Key Responsibilities

Utilize strong SQL & Python skills to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features Build reporting dashboards and visualizations to design, create and track campaign/program KPIs Perform analyses on large data sets to understand drivers of operational efficiency Manage end-to-end process of analytic tooling feature development, including request intake, requirements evaluation, cross-functional team alignment, feature execution, QA testing, and stakeholder communication Consult with business analysts, technical data SMEs, and cross-functional partners to understand their reporting and data needs, serving as the point of contact for requests, inquiries, and actions Interface with other data engineering, product, and data science teams to implement client needs and initiatives Bay Area preferred

Basic Qualifications

Minimum 5 years of Data Engineering experience High School Diploma or GED 

Preferred Qualifications

Experience with Python and SQL programming languages Bachelor's or Associate's 

Compensation at Accenture varies depending on a wide array of factors, which may include but are not limited to the specific office location, role, skill set, and level of experience. As required by local law, Accenture provides a reasonable range of compensation for roles that may be hired in California, Colorado, District of Columbia, Illinois, Maryland, Minnesota, New York/New Jersey or Washington as set forth below.

We accept applications on an on-going basis and there is no fixed deadline to apply.

Information on benefits is here. (https://www.accenture.com/us-en/careers/local/flexcareers#block-section-total-rewards)

Role Location Hourly Salary Range

California $67.30 to $77.30

Colorado $67.30 to $77.30

District of Columbia $67.30 to $77.30

Illinois $67.30 to $77.30

Minnesota $67.30 to $77.30

Maryland $67.30 to $77.30

New York/New Jersey $67.30 to $77.30

Washington $67.30 to $77.30

What We Believe

We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.

Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here (https://www.accenture.com/us-en/about/inclusion-diversity/us-workforce)

Equal Employment Opportunity Statement

Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Accenture is committed to providing veteran employment opportunities to our service men and women.

For details, view a copy of the Accenture Equal Employment Opportunity and Affirmative Action Policy Statement (https://www.accenture.com/content/dam/accenture/final/accenture-com/document/Annual-Policy-Statement-Regarding-EEO-2023-Applicant.pdf#zoom=50) .

Requesting An Accommodation

Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.

If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email (https://www.accenture.com/us-en/about/contact-us) or speak with your recruiter.

Other Employment Statements

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/5318db140bcc4a8a9385e237f2a00a5ftjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['python', 'SQL', 'containerization', 'AWS', 'Snowflake', 'Leadership', 'Communication', 'Problem-solving', 'Analytical skills', 'Project Management']"
67,Pinnacle Technology Partners,https://www.linkedin.com/company/pinnacletechnologypartners/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4258468386,4258468386,United States,Remote,,2025-06-27 15:02:21,True,over 100 applicants,"Work Model & ExpectationsHybrid role: remote with on-site work at the client’s New Jersey office 3–4 consecutive days per month
Location: Hybrid (Remote with monthly on-site in New Jersey)
About the CompanyWe are a rapidly growing technology consulting firm specializing in advanced data engineering, cloud architecture, and analytics-driven solutions. Our mission is to empower global enterprises to transform their data into actionable insights through innovative platforms and scalable infrastructure. We value continuous learning, teamwork, and excellence in engineering across all our engagements.Job SummaryWe are looking for an experienced Senior Data Engineer to join our team. In this role, you will design, build, and manage robust and scalable data architectures using Snowflake, Python, SQL, and orchestration tools. You will work closely with cross-functional teams to enable high-performance data solutions that support analytics, reporting, and enterprise-wide business intelligence initiatives.Key ResponsibilitiesDesign and implement enterprise-scale data solutions using SnowflakeBuild and maintain robust ETL/ELT pipelines for structured and unstructured dataDevelop Python and SQL-based data transformations for analytics and reportingOptimize Snowflake-based data warehouses to support BI initiativesUse Spark, Scala, and Python to implement scalable ingestion and processing pipelinesAutomate data workflows with orchestration tools such as Airflow or AutomicManage metadata, data lineage, and governance to ensure data quality and complianceApply advanced SQL analytical functions for deep data insightsDevelop automation scripts using shell scripting and JavaScriptImplement CI/CD practices and performance engineering strategiesCollaborate using tools like Git, Confluence, and JiraTroubleshoot data issues and develop preventive measures to ensure reliabilityWork cross-functionally to align data strategy with business needsRequired Qualifications & Experience8+ years of experience in data engineering and enterprise data architecture3+ years of hands-on experience with Snowflake3+ years of professional Python development experienceStrong command of SQL and Python for data transformation and processingExperience with Spark and Scala in production environmentsPractical knowledge of orchestration tools like Airflow or AutomicFamiliarity with metadata management and data lineage principlesProven experience in agile and fast-paced delivery environmentsExcellent problem-solving, communication, and teamwork skills",https://www.linkedin.com/job-apply/4258468386,"['to ', 'cloud ', 'aws ', 'aws sdk ', 'sql ', 'container ', 'azure ', 'containers ', 'organization ', 'service-oriented']"
68,ValueMomentum,https://www.linkedin.com/company/valuemomentum/life,Data Modeler,https://www.linkedin.com/jobs/view/4257048613,4257048613,"Princeton, NJ",On-site,,2025-06-27 22:08:38,True,25 applicants,"Job Title: Data Modeler with Insurity Insurance Primary skills: Insurity Insurance, Data Analyst, Data Modelling, SQL, Data Warehouse.Secondary skills: Agile knowledge, Insurance Domain, AWS Certification.Experience: 12+ Years of Experience
About the jobWe are seeking an experienced Data Modelling Data Analyst with 6-8 years of experience to design, develop, and maintain data models and warehouse solutions with hands-on experience working with Insurity’s insurance platforms. This role is pivotal in designing, implementing, and maintaining data models that support our insurance systems, enabling advanced analytics, reporting, and seamless integration across platforms. The role involves working closely with business analysts, database architects, and reporting teams to ensure high-quality data models that support business intelligence and reporting needs. You will be responsible for data profiling, validation, and implementing physical data models according to enterprise standards. 

Responsibilities:Design data models using ER diagrams and tools like Erwin.Translate business requirements into technical designs for data models.Translate business requirements into scalable and efficient data structures within Insurity solutions.Collaborate with business analysts, data engineers, and developers to implement data solutions that align with enterprise architecture.Map and integrate data between Insurity systems and other enterprise applications (CRM, billing, policy admin, etc.).Collaborate with architecture and business teams to formulate data requirements.Perform data profiling, validation, and analysis.Ensure data models follow enterprise standards and best practices.Generate DDLs and collaborate with DBAs for physical design alignment.Support the reporting team with data needs, including hierarchical and aggregated data.
Requirements - Must Have:6-8 years of experience in Data Modelling, Data Analysis, and Database Design.Experience with ER diagrams and tools like Erwin for data modelling.3+ years of data Modeling experience, preferably in the insurance domain.Hands-on experience with Insurity insurance platforms (e.g., Policy Decisions, Claims Decisions, Sure AI).Proficiency in SQL and data warehouse concepts.Ability to collaborate with cross-functional teams and translate business needs into technical solutions.Strong analytical skills with attention to detail and accuracy.Experience in data profiling, validation, and analysis.Understanding of Agile methodologies.",https://www.linkedin.com/job-apply/4257048613,"['python', 'SQL', 'Docker', 'AWS', 'Snowflake', 'project management', 'critical thinking', 'problem-solving', 'communication', 'collaboration']"
69,Claritev,https://www.linkedin.com/company/claritev/life,Data Engineer (remote),https://www.linkedin.com/jobs/view/4245580105,4245580105,"Naperville, IL",Remote,$80K/yr - $100K/yr,2025-06-27 13:50:22,False,,"At Claritev, we pride ourselves on being a dynamic team of innovative professionals. Our purpose is simple - we strive to bend the cost curve in healthcare for all. Our dedication to service excellence extends to all our stakeholders - internal and external - driving us to consistently exceed expectations. We are intentionally bold, we foster innovation, we nurture accountability, we champion diversity, and empower each other to illuminate our collective potential.

Be part of our amazing transformational journey as we optimize the opportunity towards becoming a leading technology, data, and innovation voice in healthcare. Onward and Upward!!!

Job Roles And Responsibilities

 Understand business processes and how they are modeled in various systems Work with business users, technology teams, and executives to understand their data needs to create innovative solutions to fulfil them Implement data structures, workflows, and integrations between enterprise platforms to ensure the accurate and timely execution of business processes. Maintain scalable data pipelines to support continuing increases in data volume and complexity. Adhere to established best practices on data integration/engineering, as well as the future of our data infrastructure Managing and improving the performance of our database, queries, tools, and solutions Creating and maintaining data warehouse, databases, tables, SQL queries, and ingestion pipelines to power report, dashboards, predictive models, and downstream analysis Writing complex and efficient queries to transform raw data sources into easily accessible models for our teams and reporting platforms Prepare data for predictive and prescriptive modeling Identify and analyze data patterns Identify ways to improve data reliability, efficiency and quality Work with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure Collaborate, coordinate, and communicate across disciplines and departments. Ensure compliance with HIPAA regulations and requirements. Demonstrate Company's Core Competencies and values held within. Please note due to the exposure of PHI sensitive data -- this role is considered to be a High Risk and priveleged Role. The position responsibilities outlined above are in no way to be construed as all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.

,

JOB REQUIREMENTS (Education, Experience, And Training)

 Minimum high school diploma and four (4) years' related experience, three (3) of which should be inclusive of experience with OOP, SQL, schema designing, data modeling, designing, building, and maintaining data processing systems. Bachelors' degree in computer science, information technology or a similarly relevant field is highly preferred. Experience with advanced analytics tools with Python and PySpark. Experience using SQL, SPARK, and Azure Data Factory (ADF). Experience in triaging data issues, analyzing end-to-end data pipelines and working with business users in resolving issues. Experience in working with data governance/data quality and data security teams and specifically data stewards and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification. Experience with Databricks and SSIS are nice to have. Exposure to Big Data Development using Hive, Impala, Spark, and familiarity with Kafka Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics Excellent communication skills (verbal, listening and written) Ability to build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. Ability to work with both IT and business in integrating analytics and data science output into business processes and workflows. An agile learner who brings strong problem-solving skills and enjoys working as part of a technical, cross functional team to solve complex data problems Able to prioritize and manage multiple projects and requests at any one time Strong attention to detail when identifying data relationships, trends, and anomalies. Thinking through long-term impacts of key design decisions and handling failure scenarios. Ability to effectively share technical information, communicate technical issues and solutions to all levels of business Ability to meet strict deadlines, work on multiple tasks and work well under pressure

Compensation

The salary range for this position is $80-100k. Specific offers take into account a candidate’s education, experience and skills, as well as the candidate’s work location and internal equity. This position is also eligible for health insurance, 401k and bonus opportunity.

Benefits

We realize that our employees are instrumental to our success, and we reward them accordingly with very competitive compensation and benefits packages, an incentive bonus program, as well as recognition and awards programs. Our work environment is friendly and supportive, and we offer flexible schedules whenever possible, as well as a wide range of live and web-based professional development and educational programs to prepare you for advancement opportunities.

Your Benefits Will Include 

Medical, dental and vision coverage with low deductible & copayLife insurance Short and long-term disabilityPaid Parental Leave401(k) + matchEmployee Stock Purchase PlanGenerous Paid Time Off - accrued based on years of serviceWA Candidates: the accrual rate is 4.61 hours every other week for the first two years of tenure before increasing with additional years of service10 paid company holidaysTuition reimbursementFlexible Spending AccountEmployee Assistance ProgramSick time benefits - for eligible employees, one hour of sick time for every 30 hours worked, up to a maximum accrual of 40 hours per calendar year, unless the laws of the state in which the employee is located provide for more generous sick time benefits 
EEO STATEMENT

Claritev is an Equal Opportunity Employer and complies with all applicable laws and regulations. Qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability or protected veteran status. If you would like more information on your EEO rights under the law, please click here.

APPLICATION DEADLINE

We will generally accept applications for at least 15 calendar days from the posting date or as long as the job remains posted.

",https://recruiting.adp.com/srccar/public/RTI.home?r=5001125594906&c=1204301&d=External&rb=LinkedIn,"['Python', 'SQL', 'Cloud Computing', 'AWS', 'AWS Certified Solutions Architect', 'AWS Certified Advanced Networking', 'AWS Certified SysOps Administrator', 'Data Visualization', 'RESTful API Development', 'AWS Certified Developer - Solutions Architect.']"
70,Brillio,https://www.linkedin.com/company/brillio/life,Principal Data Engineer - R01551017,https://www.linkedin.com/jobs/view/4255863523,4255863523,United States,Remote,,2025-06-27 20:50:19,False,,"About Brillio: 

Brillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as ""Brillians"", distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.

Brillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work® certification year after year.

Principal Data Engineer

Primary Skills

data modeling software (e.g., ER Studio, ERwin) and design for OLTP and analytical systems

Job requirements

Location: Onsite at San Francisco, California

The ideal candidate will possess strong communication, influencing, and negotiation skills to independently drive consensus and consistency across multiple teams. The Data Modeler must be able to work autonomously, think creatively, and take ownership of resolving challenges.

Key Responsibilities:

Collaborate with Product Management to understand and evaluate use cases, as well as functional and technical requirements. Conduct data discovery and analysis of source systems to identify the necessary data to fulfill business requirements. Develop conceptual and logical data models to validate requirements, pinpointing essential entities and relationships with full documentation of assumptions/risks. Translate logical data models into physical data models for Salesforce Data Cloud, including the production of source-to-target mapping documentation that outlines data movement from source to destination and specifies transformation rules. Support engineering teams through the implementation of physical data models, including source-to-target mapping and the application of transformation/business rules. 

Minimum Qualifications:

10+ years of hands-on experience in data modeling and design for OLTP and analytical systems. Solid understanding of data modeling principles and best practices, with expertise in canonical and semantic data modeling concepts. Proficiency in at least one data modeling software (e.g., ER Studio, ERwin). 10+ years of experience in data analysis and profiling using SQL and other relevant tools. 

Preferred Qualifications:

Experience with our technology stack, including Salesforce Data Cloud, Marketing Cloud, and Tableau, is highly desirable. Practical experience with sales and marketing data. Experience working with globally distributed teams is an advantage. Availability to work in the Pacific time zone. 

Know more about:

DAI: https://www.brillio.com/services-data-analytics/

Know what it’s like to work and grow at Brillio: https://www.brillio.com/join-us/

Equal Employment Opportunity Declaration

Brillio is an equal opportunity employer to all, regardless of age, ancestry, colour, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding, and related medical conditions), and sexual orientation.

Know what it’s like to work and grow at Brillio: Click here

",https://jobs.lever.co/brillio-2/fc823c83-1078-4b26-8b1f-b82be21163bb/apply?lever-source=Job%20postings%20feed,"['Marketing', 'Product life cycle', 'Branding', 'Digital marketing strategies & tools', 'Customer relationship management (CRM)', 'Lead generation', 'Conversion optimization', 'Search engine optimization (SEO)', 'Profitable content creation & public relations', 'Google Analytics']"
71,Kolme Group,https://www.linkedin.com/company/kolme-group/life,Data Integrations Engineer,https://www.linkedin.com/jobs/view/4258400877,4258400877,United States,Remote,,2025-06-27 07:56:20,False,,"Disclaimer

The best way to apply for our openings is through our job board. Please do not contact anyone within our organization directly as your application may be lost.

Kolme Group provides consulting services to help our clients attain the highest value from their investment in Project Management (PM). Come join our growing international PM consulting company with a culture that focuses on maintaining the balance between customer-centric and employee-centric foundations.

At Kolme Group, we understand how flexibility allows our team members to deliver exceptional results to our clients. This role provides boundless opportunities for both personal and professional growth. We are a fully remote organization that offers schedule flexibility, one-on-one mentorship, and an extraordinary culture lead by an inspiring leadership team. If you are driven, like to exceed expectations, and want to work with an awesome group of people, we want you to apply for this opportunity.

BASIC FUNCTION OF THE JOB

The Data Integration Engineer will be responsible for leading the migration and redevelopment of a custom solutions built into a robust, scalable Azure/Python framework. This solution integrates SaaS software extracting key data points, performing time-phased calculations, and pushing results back into the platform. The role requires a strong understanding of data engineering, API integrations, and process automation.

This is a contract position estimated to last roughly 2-3 months

Project scope: This role includes architecture, hands-on coding, QA, and documentation.

WORK PERFORMED

Essential Functions Of The Job

To perform this job successfully, an individual must be able to meet the following responsibilities and demonstrate the associated knowledge, skill, and/or ability required:

Lead the migration of an existing integration built on Make and Google Sheets into a new Azure Functions/Python architectureTranslate process flow diagrams and data dictionaries into working solutionsDesign and implement secure, scalable, and maintainable integrations with SaaS using APIsBuild time-phased revenue forecasting logic and automate data flowsTroubleshoot and resolve integration issues related to performance, accuracy, or data flowCollaborate with internal teams to understand requirements, gather feedback, and implement improvementsMaintain clear documentation of the architecture, logic, and future state visionEnsure data integrity and reliability through testing and monitoringProvide knowledge transfer and training to internal stakeholders

Successful Knowledge, Skills, And Abilities

Proficient in Python and Azure Functions, Logic Apps, or equivalent serverless frameworksFamiliarity with SaaS platforms and their APIsUnderstanding of time-phased financial forecasting and data modelingExcellent analytical and troubleshooting skillsAbility to balance speed of delivery with code quality and maintainabilityStrong communication skills for cross-functional collaborationComfortable with working from process flow diagrams, data dictionaries, and business requirements

Minimum Requirements

Experience developing cloud-based automation or integrationsProficiency in Python development and REST APIsFamiliarity with project management platformsExperience with Make (Integromat), Zapier, or similar tools preferred (for transition support)Experience working with Google Sheets APIsExperience migrating integrations between platformsFamiliarity with financial concepts and revenue forecasting logicPrior experience working in a professional services or consulting environment",https://ats.rippling.com/kolme-group/jobs/3d685015-83f1-447c-9b42-4d2ff9a554c4?jobSite=LinkedIn,"['Python', 'SQL', 'DevOps', 'Project Management', 'Agile', 'Cloud Computing', 'Teamwork', 'Problem-solving', 'Communication', 'Creativity']"
72,Brooksource,https://www.linkedin.com/company/brooksource/life,Senior Database Engineer,https://www.linkedin.com/jobs/view/4257022681,4257022681,"Chicago, IL",Hybrid,$70/hr - $75/hr,2025-06-27 19:27:58,True,over 100 applicants,"Senior Database EngineerLocation: Chicago, IL (Hybrid, Onsite 2 days a week)Job Type: 6 Month CTH
Position Summary:We are seeking a highly skilled and experienced Senior Database Administrator (DBA) with deep expertise in Snowflake to join our data infrastructure team. The ideal candidate will be responsible for the design, implementation, performance tuning, and maintenance of our analytical database environments, ensuring high availability and optimal performance across platforms. Key Responsibilities:Administer, monitor, and optimize Snowflake database environments.Lead and execute the migration of data and workloads from Vertica to AWS/Snowflake.Design and implement data models, schemas, and storage strategies for large-scale analytics.Perform database performance tuning, query optimization, and capacity planning.Implement and manage space management strategies and proactive space monitoring.User Management: Create, manage, and delete users; grant roles and permissions.Role-Based Access Control (RBAC): Implement and maintain role hierarchies for granular access control.Security: Configure security policies, monitor access, and manage secrets.Performance Tuning: Monitor virtual warehouses, optimize queries, and ensure efficient resource utilization.Data Loading and Management: Manage data loading processes, data replication, and data backup.Monitoring and Auditing: Track activity, monitor query performance, and audit access.Cost Management: Identify and optimize storage costs and virtual warehouse usage.Automation: Implement automated tasks for routine database management and monitoring.Collaborate with data engineers, analysts, and DevOps teams to support data pipelines and ETL processes.Troubleshoot and resolve database-related issues in a timely manner.Evaluate and recommend new database technologies and tools.Required Qualifications:Bachelor’s or Master’s degree in Computer Science, Information Systems, or a related field.7+ years of experience as a Database Administrator, with 3+ years of hands-on experience in Snowflake.Proven experience in migrating data platforms, especially from Vertica to Snowflake or AWS.Strong expertise in SQL, query optimization, and performance tuning.Solid understanding of data warehousing, ETL processes, and data modeling.Experience with cloud platforms (AWS, Azure, or GCP) and cloud-native database services.Proficiency in scripting languages (e.g., Python, Bash) for automation and monitoring.Experience with RBAC, user management, and security best practices.Familiarity with cost optimization strategies in cloud-based data environments.Strong analytical and problem-solving skills with attention to detail.Excellent communication and collaboration skills.Maintain controls, reporting and transparency necessary to comply with customer and BCBSA confidentiality, HIPAA, PHI and PII standards, as well as SOC2 and HiTrust certifications. Preferred Qualifications:SnowPro Core or Advanced Certification and/or Vertica DBA Certification.Experience with CI/CD pipelines and Infrastructure as Code (IaC) tools like Terraform.Familiarity with BI tools (e.g., Tableau, Power BI, Looker).Experience with data orchestration tools like Apache Airflow or dbt.Knowledge of Kafka, event streaming, or real-time data processing.  Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.",https://www.linkedin.com/job-apply/4257022681,"['relational database', 'building automation systems', 'cybersecurity', 'Linux server administration', 'network configuration', 'cloud computing', 'Windows server administration', 'Microsoft Office Suite', 'data storage solutions', 'project management tools']"
73,Lensa,https://www.linkedin.com/company/lensa/life,REMOTE - Data Engineer,https://www.linkedin.com/jobs/view/4256759106,4256759106,"Woonsocket, RI",Remote,$50/hr - $55/hr,2025-06-27 13:54:02,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

An employer is seeking a Data Engineer to join a leading healthcare and pharmaceutical company. Responsibilities include:

 Be a technologist and work with other Engineers in planning, prioritizing, and performing assigned tasks within deadlines

 Will be responsible for end-to-end application development & delivery including production deployment, application operationalization, and observability.

 Develop data pipelines to ingest, process and broadcast data across heterogenous databases.

 Build and deploy using GitHub, CircleCI, Harness as part of CI/CD process in leading Cloud Platforms - AWS, GCP or Azure etc.

 Continuously checking and monitoring App health and KPIs, support triage of any production issues as and when needed

 Be an advocate for and implementer of security best practices

 Adopt and apply industry technology best practices

 Partner with application owners, business partners and peer groups regarding long and short-range technical solutions that meet business requirements.

 Analyze and contribute to project and business requirements based on product team milestones and priority.

 Actively participate in Agile Scrum team activities including Sprint Planning, Grooming, Scrum, Reviews and Retrospectives

$50-55/hour

Exact compensation may vary based on several factors, including skills, experience, and education.

Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401K retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

7+ years experience in software engineering from ideation to production deployment of software

 7+ years experience in full software development life cycle including ideation, coding, coding standards, testing, code reviews and production deployments

 Experience in creating/managing GCP storage Buckets, Data Composer workflow, Dataflow jobs, IAM (Service Account/Roles) Management.

 Experience in data extraction, transformation, loading (ETL), data quality checks, database management Experience in Writing automated unit and mock test cases for code coverage

 Deployment and troubleshooting application in Cloud environment (GCP)

 Experience with Kubernetes, Apigee, GCP -GKE, Dataflow, Airflow. MongoDb, SQL, Postgres SQL, SOAP services, IntelliJ and Devops: Git, Jenkins, Github Actions.

 Experience in log ingestion and building dashboards using Prometheus and Grafana.

 Proficiency in SQL and NoSQL database technologies

 Query and decipher logging entries by application tiers and components using trace logs

 Form requests in tooling like Postman , SOAP UI to invoke endpoints

 Experience working in a Scrum/Agile development methodology

 Ability to work independently and part of a team

 Healthcare Domain experience null

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/7cbf9363e96a4667ab4979c2b64e801etjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Joining', 'BI', 'Team Lead', 'Agile Methodology', 'SQL', 'Databases (MySQL', 'Oracle', 'PostgreSQL)', 'AWS', 'Snowflake', 'Excel', 'Communication']"
74,Falcon Smart IT (FalconSmartIT),https://www.linkedin.com/company/falconsmartit/life,BI Apps Developer / Architect – EBS Service Module,https://www.linkedin.com/jobs/view/4257012532,4257012532,United States,Remote,,2025-06-27 18:13:35,True,48 applicants,"Job Title: BI Apps Developer / Architect – EBS Service ModuleLocation: RemoteJob Type: Contract
Job Description: We are looking for an experienced Oracle BI Applications Developer / Architect with deep expertise in building native logical models for Oracle E-Business Suite Service Modules. The ideal candidate will be responsible for designing logical data models using Navicat Modeler, and transforming them into scalable semantic models for reporting and analytics purposes.
Key Responsibilities:Design and develop native logical data models for Oracle EBS Service Modules (e.g., Field Service, Depot Repair, Service Contracts, etc.).Use Navicat Data Modeler to build and manage logical models, ensuring they align with business and data requirements.Convert logical models into semantic models that support advanced reporting and analytics.Collaborate with business analysts, data engineers, and functional teams to capture data requirements and translate them into high-performance BI solutions.Integrate semantic models into BI tools such as OBIEE, Power BI, or Microsoft Fabric, as needed.Ensure model consistency, reusability, and adherence to enterprise data standards.Perform data profiling and source system analysis to identify key business metrics and KPIs.Optimize data models for performance and scalability.
Required Skills and Experience:15+ years of experience in Oracle BI Applications (OBIA) development and architecture.Deep understanding of Oracle E-Business Suite, especially Service Modules (Field Service, Service Contracts, etc.).Strong experience in logical data modeling, dimensional modeling, and relational modeling.Hands-on experience with Navicat Modeler (or similar data modeling tools like ER/Studio or ERwin).Proven ability to transform logical models into semantic layer models used in BI platforms.Strong SQL skills and familiarity with PL/SQL for data extraction and manipulation.Knowledge of ETL concepts and ability to work with data integration teams.Good understanding of metadata management and data governance practices.Strong communication and documentation skills.
Preferred Qualifications:Experience with BI tools such as OBIEE, Power BI, or Oracle Analytics Cloud (OAC).Familiarity with Microsoft Fabric or modern data lake architectures.Exposure to Agile methodologies and DevOps for BI environments.Oracle or BI-related certifications are a plus.",https://www.linkedin.com/job-apply/4257012532,"['python', 'cloud automation', 'excel', 'project management', 'problem-solving', 'collaboration', 'customer service', 'analytics', 'AI', 'communication']"
75,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256754928,4256754928,"Albany, NY",Remote,$118K/yr - $122K/yr,2025-06-27 13:53:46,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/23f1f947985140e09fe820b4b15f02a1tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['- Python', '- Database management', '- AWS knowledge', '- Fog computing', '- Access management', '- Graph API', '- Inventory API', '- BI Engine', '- InfluxDB', '- RESTful API']"
76,Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer / Fabric Admin (Remote),https://www.linkedin.com/jobs/view/4256758315,4256758315,"Nashville, TN",Remote,$140K/yr - $150K/yr,2025-06-27 13:53:47,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currentlyseekinganexperienced and proactivePower BI Developer /Fabric Admin (Remote)to manage and support the enterprise-wide Power BI environment, focusing onPower BI Service administration, Fabric platform oversight, user support across tiers, and end-to-end report lifecycle management. This role requires technical depth in Power BI tools and infrastructure, combined witheffective communicationand stakeholder engagement to work across theCenter of Excellence (COE), business units, and IT support.This position will be fully remote located within the United States.

Responsibilities

Serve as aPower BI Fabric Administrator, overseeing workspace management, deployment pipelines, permissions, and performance monitoring. Develop aPower BI Center of Excellence (COE)to enforce standards, best practices, and governance for enterprise Power BI usage. Conducts PBI Training (Instructor-led Class Development, Delivery). ProvideTier 1 and Tier 2 supportfor Power BI issues—including report access, refresh failures, dataset issues, and workspace configuration. Assist withSelf-Help resources, including FAQs, templates, training materials, and troubleshooting documentation to empower end users. ManagePower BI licensing and desktop software distribution, working with ITassetsand licensing teams to track usage and entitlements. Support integration withexisting data sourcesandfacilitatethedesign and onboarding of new data sources, ensuring data quality, refresh schedules, and secure access. Handle incomingPower BI reporting and workspace requeststhrough a managed queue or mailbox, ensuringtimelyprioritization, response, and resolution. Design, publish, andmaintainPower BI reports and dashboardsaligned with business requirements andoptimizedfor usability and performance. Create andmaintainrobusttechnical documentation, including data dictionaries, architecture diagrams, workspace catalogs, and refresh schedules. Engage in proactive system health checks and performance tuning of Power BI Service, datasets, and gateway configurations. 

Qualifications

Required Skills and Experience

Bachelorswith 12+ years (orcommensurateexperience). Strong experience withPower BI Service Administration, Fabric workspace management, and deployment pipelines. Familiarity withPower BI governance, tenant settings, and organizational policies. Experience supporting and escalating issues acrossTier 1 and Tier 2 support models, including Service Desk and End-User Enablement. Knowledge ofPower BI licensing models(Free, Pro, Premium, PPU) and integration with Microsoft 365 administration tools. 

Preferred Skills And Experience

Solid understanding ofdata connectivity, gateway setup, refresh scheduling, and source control for bothexisting and new data sources.

Clearance Requirements

Ability to obtain and maintain a suitability/public trust clearance

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $140,000.00 - USD $150,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6140/power-bi-developer---fabric-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6140

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/3e5624e8f66c42f381e3a84eb3a41115tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['data analysis', 'SQL', 'Python', 'AWS', 'Amazon EMR', 'AWS cloudformation', 'S3', 'AWS IAM', 'Docker', 'Amazon Web Services (AWS)']"
77,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer, AWS Finance, Analytics, and Science (FAST)",https://www.linkedin.com/jobs/view/4258455133,4258455133,"Seattle, WA",On-site,$91.2K/yr - $185K/yr,2025-06-27 14:06:47,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Amazon.

Description

AWS is one of Amazon’s largest and fastest growing businesses, serving millions of customers in more than 190 countries. We use cloud computing to reshape the way global enterprises use information technology. We are looking for entrepreneurial, analytical, creative, flexible leaders to help us redefine the information technology industry. If you want to join a fast-paced, innovative team that is making history, this is the place for you.

The AWS Finance Analytics and Science Team supports the BI needs of the AWS Worldwide Specialist Organization, a world-wide team of sales and technical specialists responsible for selling AWS' most advanced services.

The successful candidate will be an expert with SQL, ETL (and general data wrangling), and have exemplary communication skills. The candidate will need to be comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail. The candidate will be passionate about working with huge data sets and someone who loves to bring data together to answer business questions and drive growth.

Key job responsibilities

This role builds and continuously improves data models, processes, and mechanisms that provide business intelligence for AWS. The role will work with executives, program owners, finance, and technical peers to execute on large-scale projects that drive step change growth for the business. This role will help develop and implement data engineering best practices for the team in partnership with other engineers, scientists, and economists. The role will also be an important collaborator with other technical and business teams at AWS.

A day in the life

Our team takes big swings and works on really hard, cross-organizational problems. We ask people to grow their skills and stretch, and we do it in a supportive and fun environment. It’s about empirically measured impact, advancement, and fun.

About The Team

Our group is technically rigorous. Our leaders are here for you and to enable you to be successful. We are communication centric, since being able to explain what we do ensures high success rates and lowers administrative churn. And if it’s not fun, what’s the point?

Basic Qualifications

2+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) 

Preferred Qualifications

Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. 

Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.

Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits . This position will remain posted until filled. Applicants should apply via our internal or external career site.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/2d630ec814004f1784f659180e4ee95btjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['Microsoft Suite', 'Azure DevOps', 'Jenkins', 'Git', 'Docker', 'Amazon Web Services (AWS)', 'Snowflake', 'Python', 'SQL', 'ReactJS']"
78,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer,https://www.linkedin.com/jobs/view/4258451386,4258451386,"Denver, CO",On-site,$177.4K/yr - $196.9K/yr,2025-06-27 14:06:55,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for META.

Summary

Meta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click “Apply to Job” online on this web page.

Required Skills

Data Engineer Responsibilities:

Design, build, and launch data pipelines to move data across systems and build the next generation of data tools that generate business insights for a product. Analyze user needs and software requirements to determine workability and to offer support for end users on data usage. Design, architect, and develop software and data solutions that help product and business teams make data-driven decisions. Rethink and influence strategy and roadmap for building efficient data solutions and scalable data warehouse plans. Design, develop, test and launch new data models and processes into production, and provide support. Leverage homegrown extract, transform, and load (ETL) framework as well as off-the-shelf ETL tools, as appropriate. Interface closely with data infrastructure, product, and engineering teams to build and extend cross platform ETL and reports generation framework. Identify data infrastructure issues and drive to resolution. Telecommute from anywhere in the US permitted. 

Minimum Qualifications

Minimum Qualifications: 

Requires Master’s degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Analytics, Statistics, Mathematics, Physics, or a related field and 3 years of experience in the job offered or in a related occupation. Requires 36 months of experience in the following: Internet technologies including HTMLAnalyzing large volumes of data to provide data driven insights, gaps, and inconsistenciesData warehousing architecture and plansDimensional data modelingSchema Design
Public Compensation

$177,406/year to $196,900/year + bonus + equity + benefits

Industry: Internet

Equal Opportunity

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/28faaf25fb764de2b1be269bb3c67969tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['cloud architecture', 'AWS', 'SQL', 'Python', 'Docker', 'Kubernetes', 'Snowflake', 'AWS SAM', 'Terraform', 'Linux']"
79,Devfi,https://www.linkedin.com/company/devfiinc/life,Data Engineer - Capital Market,https://www.linkedin.com/jobs/view/4257000397,4257000397,United States,Remote,,2025-06-27 15:50:43,True,over 100 applicants,"Job Title: Data EngineerDuration: 6+ Months 
Exp: 11+ Years
Remote!! W2 Only!!
 Must have some Loan/Bond Capital Market experience.
Job Description:A tech engineer with Python/AWS experience who can design payloads for processing to Grid.Perform Data Mapping, Data Transformations and then build prototypes for Payload processing.The actual implementation and development will be done by the IT of the client.Project is related to Futures/Debt / Derivatives trading analyticsData will be in Numerix (Financial Asset / risk management) platform system - extract and send the data to the grid (where we run massive models for calculating positions etc.)They’re likely to use either proto buffer or Avro for data payloads.Python/AWSSomeone with 5 to 7 years’ experience with strong communication skillsHands on coding and development experienceThe financial services industry is a must - within finance preferred loans, structured assets, mortgage back securities data familiarity - understand the terminologies like Swap rate, maturity rate, date , index, days of conventions (business days, calendar days), instrument type etc. for data mapping and data transformations
Thanks,Naveen S 614 504 8101naveen@devfi.com",https://www.linkedin.com/job-apply/4257000397,"['python', 'encryptionanalysis', 'cybersecurity', 'forensics', 'malware', 'AI', 'penetrationtesting', 'incidentresponse', 'riskassessment', 'dataprivacy']"
80,Lensa,https://www.linkedin.com/company/lensa/life,"Analytics Services, Data Engineer - Sr. Consultant - Remote",https://www.linkedin.com/jobs/view/4256757408,4256757408,"Eden Prairie, MN",Remote,$89.8K/yr - $176.7K/yr,2025-06-27 13:54:02,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for UnitedHealth Group.

Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.

The Sr. Consultant, Data Engineer - Analytics Services positions in this function are predominantly involved in developing business solutions by creating new and modifying existing software applications. Primary contributor in designing, coding, testing, debugging, documenting and supporting all types of applications consistent with established specifications and business requirements to deliver business value.

You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities

Solid organizational skills and focus on accuracy and attention to detail Excellent analytical, problem solving and troubleshooting abilities Self-motivated with the ability to work both independently and in a team environment Providing accurate and timely estimates for tasks Provide support and leadership in critical production support issue resolution Conduct or facilitate root cause analysis on all in-scope incidents and recommend a corrective action plan Work with team to achieve timely resolution of all production issues meeting or exceeding Service Level Agreements Conduct code review to ensure the work delivered by the team is of high quality standards Work with business to prioritize production issue resolution Develops innovative approaches Data Engineering 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

6+ years of data engineering work experience 6+ years of ETL development experience 6+ years previous SDLC experience 6+ years of experience writing advanced complex queries, code optimization with SQL 

Preferred Qualifications

3+ years of experience in Python 3+ years of experience with Cloud environments, such as Snowflake, Azure and/or Databricks Experience developing ETL Processes using Microsoft SQL Server Integration Services (SSIS) .Net development experience Experience writing and executing functional specifications Experience working with and analyzing healthcare administrative, medical, or pharmacy claims data to support downstream applications, data marts, and end users Demonstrated proficiency in database design, querying, optimization & troubleshooting with any of the following RDBMS: Oracle, SQL Server, and/or Teradata All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

The salary range for this role is $89,800 to $176,700 annually based on full-time employment. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.

UnitedHealth Group is an Equal Employment Opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.

UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/2db9cd9f178141ae8af5ddc0532f2c4etjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,['frontend JavaScript Angular React React Native AWS Snowflake SQL Azure Linux CMS CRM Docker Kubernetes']
81,Lensa,https://www.linkedin.com/company/lensa/life,Jr. Data Engineer,https://www.linkedin.com/jobs/view/4258448858,4258448858,"San Antonio, TX",On-site,,2025-06-27 14:06:42,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

This is a unique opportunity to join a high-impact team working on critical data pipelines and transformations that support information security, fraud detection, and access management. Youll work with a modern data stack and gain exposure to cloud technologies, data reliability practices, and business-facing analytics. Data is generally clean, but candidate may handle some transformations, error handling, and sensitive data scrubbing. Role involves working with information security data sets (member and employee access management, external/internal fraud). Some knowledge of fraud models and machine learning is a plus, but not mandatory.

Build and maintain data pipelines using Snowflake, DBT, and other tools in our stack (e.g., DataStage, Talend, Kafka). Support data transformations for reporting (DL4), error handling, and sensitive data scrubbing. Collaborate with cross-functional teams to deliver clean, reliable data for internal and external fraud detection and access management. Contribute to monitoring and reliability efforts, with opportunities to learn tools like Grafana, Fortuna, and Datadog. Work in a cloud-first environment, applying best practices in data engineering and security. 

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

Technical Requirements

Core Technologies: Proficiency with Snowflake and DBT.

Experience with DataStage, Talend, and Kafka (receiving side).

Monitoring & Reliability: Proficient in Grafana, Fortuna, or Datadog for SLO/SLA monitoring.

Familiarity with Google SRE or data reliability engineering principles.

Project Focus Areas: experience in information security datasets (e.g., access management, fraud detection).

Handling clean but sensitive data, including transformations, error handling, and scrubbing.

Supporting business-facing data transformations (DL4/data for reporting).

Cloud Experience: Required, especially in the context of Snowflake/DBT deployments. AI + Machine Learning: Some knowledge of fraud models or ML concepts is a plus, but not required.

Certifications in tools like Snowflake or DBT null

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/2d88683453fd463a914363dc41952dfatjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['pipeline', 'Blockchain', 'Machine Learning', 'UI/UX Design', 'Cybersecurity', 'Cloud Computing', 'Data Analysis', 'Agile Methodologies', 'Project Management', 'Communication']"
82,Jobs via Dice,https://www.linkedin.com/company/jobs-via-dice/life,Need Data Engineer - Remote,https://www.linkedin.com/jobs/view/4257032575,4257032575,United States,Remote,,2025-06-27 20:19:50,False,,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, SRS Consulting Inc, is seeking the following. Apply via Dice today!

Hello Associate,

Hope you are doing great,

Below positions is with Open, Please share resumes 

Role: Data Engineer

Location: Remote

Duration: 3 Months with possible extensions

Project Overview:

We re seeking a hands-on Integration Engineer to support a high-priority integration effort for a new Transportation Management System (TMS) platform, serving a major shipper. The engineer will be responsible for designing and implementing data integrations using XML schemas, building ETL pipelines, and validating incoming data streams from multiple vendors.

This is a net-new role on a growing infrastructure team and requires someone who can self-facilitate, lead some technical discovery, and own the delivery of integration solutions end-to-end.

Responsibilities:

Design and implement integrations between vendor systems and our TMS platform using XML-based feeds.Define and work with XSD/XML schemas to ensure compatibility and accuracy of data.Build, test, and deploy ETL pipelines to handle file drops, data ingestion, and transformation.Validate data integrity and ensure seamless movement of data across systems.Collaborate with external vendors and internal engineering teams to troubleshoot and refine integrations.Contribute to integration planning and discovery, with autonomy to shape how integrations are built.Deliver clear documentation and integration outputs; your work will be directly tied to platform readiness.

Key Requirements:

Strong XML experience: Able to read, write, validate, and work with XSD files.Experience with data integration tools and platforms such as Azure Synapse, Databricks, or Apache Spark.Solid understanding of ETL concepts, especially around file-based ingestion and transformation.Prior experience working with or integrating into Transportation Management Systems (TMS) is strongly preferred.Strong data validation, troubleshooting, and problem-solving skills.Able to work independently in a fully remote setup.Strong communicator, comfortable interfacing with technical and non-technical stakeholders.

Nice to Have:

Experience in logistics, shipping, or supply chain data flows.Exposure to Microsoft Fabric, with ability to lead a POC by year-end (not required but helpful).Background in platform or infrastructure engineering a plus.

Thanks & Regards,

Rahul.B",https://click.appcast.io/t/kbB0MUjRQyJX9UGMEenNKyUn2cWAIEdzOJAfkCG5xGs=,"['python', 'sql', 'docker', 'AWS', 'AWS CloudTrail', 'SQL', 'DevOps', 'monitoring', 'proficient in AWS.', 'When analyzing a job description to extract required skills', ""it's important to focus on actionable keywords and phrases that indicate a necessity for the skill. The list provided is based on the skills explicitly mentioned in the description"", 'considering common skill tags used in job listings as well.']"
83,Highbrow LLC,https://www.linkedin.com/company/highbrow-llc/life,Solutions Engineer – Data,https://www.linkedin.com/jobs/view/4255862817,4255862817,"West Deerfield, IL",Hybrid,,2025-06-27 21:05:14,False,,"W2Deerfield, IL (Hybrid - 3 days/week onsite)Posted 5 hours ago

Website Highbrow LLC

Job Title: Solutions Engineer – Data

Job ID: 2025-13461

Job Location: Deerfield, IL (Hybrid – 3 days/week onsite)

# Positions: 1

Work Eligibility: All Work Authorizations are Permitted – No Visa Transfers

What You Will Do

Act as the lead engineer for each domain

Collaborate with Solution Architects to ensure understanding needed for engineering level guidance to nearshore teams

Translate architecture design to engineering level guidance

Own specifying architectural designs into sprint-ready engineering tasks for assigned domain

Ensure delivery to the solution architecture within domain

Answer design questions for sprint teams

Participate in backlog refinement sessions to align team with the architecture

Act as the internal technical expert supporting Product design and Engineering teams on demonstrations and technical discussions.

Create/Update documentation such as solution design patterns and engineering standards.

Lead technical discussions around APIs, integrations, data flow, compliance and security.

Serve as a trusted advisor on best practices, technical implementation, and scalability.

Recruiter Submission Template

Evaluation Benchmarks Self-Assessment (1-5) Experience (In years) Notes/Comments Experience with Python, Spark, Databricks, Azure Event Hub, ADF, Delta Lake Tables Able to write optimized ETL pipelines, manage distributed compute jobs, and handle large-scale data transformations. Experience with Azure Data Factory (orchestration) and Azure Event Hub (ingestion/event streaming) should know how to manage data movement, ingestion triggers, and pipeline tuning in a production Azure environment. Able to code in Spark, and understand execution models, partitioning strategies, caching, broadcast joins, and how these affect performance in Azure’s ecosystem.

To apply for this job email your details to jobs@highbrow-tech.com",https://highbrow-tech.com/job/solutions-engineer-data-5/,"['python', 'Java', 'Salesforce', 'AWS', 'SQL', 'MongoDB', 'Linux', 'Agile methodologies', 'Excel', 'Communication skills']"
84,Lensa,https://www.linkedin.com/company/lensa/life,Remote Data Engineer,https://www.linkedin.com/jobs/view/4256755751,4256755751,"Irving, TX",Remote,,2025-06-27 13:53:58,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Insight Global.

Job Description

Insight Global is seeking a Data Engineer to join a Fortune 100 Healthcare Organization and work remotely. This is a short term project projected to last 2 months, and this individual will be responsible for the development of a new reusable data pipeline using Python and SQL to support various PBM clients.

We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .

To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .

Skills And Requirements

3+ years of data engineering experience Extensive experience with Python and SQL Experience building data pipelines from scratch using Python and SQL Experience with Azure as cloud platform null 

We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/c3ca5b4d0fe74c92ba5ac8bf5b1ef7bbtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'git', 'linux', 'database management', 'UML', 'container orchestration', 'AWS', 'sales', 'customer service', 'Microsoft Word']"
85,Connecticut Innovations,https://www.linkedin.com/company/connecticut-innovations/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4258498272,4258498272,"Connecticut, United States",Remote,,2025-06-27 18:18:07,True,over 100 applicants,"Are you ready to join Connecticut Innovation’s vibrant community of innovators? Connecticut Innovations (“CI”) is Connecticut’s strategic venture capital arm, and we are passionate about serving our portfolio of 220+ companies across various industries, with strengths in life sciences, technology, and climate tech.
Come join one of our quickly growing portfolio companies, FlyteHealth
Who We Are:FlyteHealth (www.flytehealth.com) delivers a comprehensive cardiometabolic treatment program for self-insured employers, payers, and health systems. The FlyteHealth program combines clinical and lifestyle management through the use of medications and intensive behavioral therapy to actively treat patients living with obesity and associated comorbidities; pre- diabetes, hypertension and hyperlipidemia. FlyteHealth delivers care through its patent pending AI algorithm and platform utilizing a clinical team consisting of clinical practitioners (MDs, NPs, RDs, Health Coaches, and Care Coordinators) and partnering with provider networks within all 50 states of the USA.
The Role: Data Engineer – AI & InfrastructureAs a Data Engineer at FlyteHealth, you will play a pivotal role on our Data and AI Product Team, architecting the data infrastructure that fuels innovation across our healthcare platform. You will design, maintain, and optimize robust ETL pipelines that power AI-driven features, ensure data availability across the organization, and contribute to shaping FlyteHealth’s approach to scalable, intelligent systems. This role is ideal for an engineer who thrives on autonomy, embraces complexity, and is eager to make a direct impact on both product development and patient outcomes.
Key Responsibilities:Data Infrastructure & ETL Pipeline Development:Build and scale our ETL pipelines to support growing data needs across FlyteHealth’s application teams.Ensure the integrity, performance, and reliability of pipelines that underpin our AI and analytics features.Collaborate with cross-functional partners to understand data requirements and deliver solutions that empower teams across the organization.
Cloud & Platform Engineering:Design and implement ETL infrastructure using AWS, Kubernetes, and Airflow.Maintain observability tools (e.g., DataDog) to ensure robust monitoring, alerting, and diagnostics of data workflows.Contribute to best practices in data engineering by defining technical standards and leading implementation.
AI Enablement & Internal Collaboration:Partner closely with product, engineering, and AI specialists to ensure that high-quality data is readily accessible for experimentation and product development.Identify opportunities to improve pipeline performance, reduce technical debt, and support emerging AI initiatives.Take initiative to explore different areas of the Data/AI organization that align with your growth and interests.
Why Join UsDrive the data strategy that powers FlyteHealth’s AI innovation.Help address chronic conditions by ensuring accurate, accessible data.Join a curious, collaborative engineering culture that values growth and transparency.Participate in tech talks, peer syncs, and annual offsites to stay connected and inspired.
What Makes You a Great Fit
Technical Expertise:Proficient in Python and SQL with hands-on experience working with large-scale ETL pipelines.Experience extracting data from platforms such as PostgreSQL, Snowflake, and Salesforce.Familiarity with cloud infrastructure and orchestration tools, including AWS services, Kubernetes, and Airflow.
Operational Excellence:Understand how to build and maintain observability stacks, including logging, metrics, and dashboards.Ability to work independently while knowing when to seek input or align with team priorities.Skilled in developing scalable, well-documented data solutions in a fast-paced product environment.
Preferred Qualifications:2–4 years of experience in a data engineering role.Bachelor’s degree in Computer Science or a related technical field.Experience designing and launching end-to-end ETL systems.
FlyteHealth Benefits: *Eligibility for Essential benefits: Full-time employees regularly working 30+ hours per weekComprehensive health, dental and vision insurance Equity Shares401(k)Discretionary PTO PlanParental leave
FlyteHealth Perks:Flexible working hoursRemote-first CompanyInternet Stipend for remote workingPaid Company Holidays",https://www.linkedin.com/job-apply/4258498272,"['English', 'communication', 'leadership', 'employee engagement', 'data analysis', 'coding (Python', 'SQL)', 'cloud computing (AWS', 'Azure)', 'container orchestration (Kubernetes)', 'DevOps', 'multi-disciplinary collaboration']"
86,Lensa,https://www.linkedin.com/company/lensa/life,Principal Engineer Data Engineering - US Remote,https://www.linkedin.com/jobs/view/4256760144,4256760144,"Pittsburgh, PA",Remote,,2025-06-27 13:53:56,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Anywhere Real Estate.

Anywhere is at the forefront of driving the digital transformation and building best-in-class products that help our agents and brokers sell more homes, make more money, and work more efficiently.

Data & Analytics (DNA) is Anywhere's data arm. We create innovative analytics, data science, and robust data foundation capabilities to generate data-driven insights that serve the heart of Anywhere Advisor and Anywhere Brand business. Together with our business counterparts in the real estate business, we work daily to deliver differentiating insights (AI & BI) for Strategy and AA & AB Operations.

We're seeking a Principal Engineer to join our Data Platform Team. In this critical position, you'll be responsible for designing, implementing, and managing the data infrastructure. You will work closely with data scientists, software engineers, and other stakeholders to ensure the Data Platform's availability, usability, and integrity.

Data Infrastructure Design And Implementation

Evaluate, select, and implement new tools and frameworks to expand our data platform capabilities. Design, build, and maintain robust, scalable, and reliable data pipelines and ETL processes. Develop and maintain data infrastructure and platforms using various technologies (e.g., AWS, Snowflake Cloud Platforms, databases, Kafka streaming platforms). Ensure data quality, consistency, and integrity across the organization. Architect and optimize Data Ingestion and Snowflake ETLs. Production Support and enhancements to the observability of the Data Platform. 

Team Leadership And Mentorship

Lead and mentor data engineers, providing guidance and support to junior engineers. Foster a culture of technical excellence and continuous learning. Collaborate with other teams (e.g., data scientists, software engineers, and product managers) to ensure data solutions meet business needs. 

Data Security And Compliance

Implement and maintain data security measures to protect sensitive data. Ensure compliance with data protection regulations and industry standards. 

Problem Solving And Innovation

Identify and solve complex data-related problems. Stay abreast of industry trends and emerging technologies and identify opportunities to enhance data capabilities. Proactively address performance, scale, complexity, and security considerations. 

Skills And Qualifications

Technical Expertise:

10+ years’ experience with a strong understanding of data engineering principles and technologies. 10+ years’ experience with data pipelines, ETL processes, and data warehousing. 5+ years’ experience building data pipelines using Kafka, Kafka Connect, Airflow, and Snowflake. 5+ years’ experience with Snowflake Data Platform. 5+ years’ experience with AWS Data Services such as DMS, EMR, Glue, Athena, S3, CloudWatch, Lambda, or IAM. 5+ years’ experience with Data Quality, Data Reconciliation 5+ years’ experience managing production data platforms. 5+ years’ experience building observability (Monitoring & Alerting) using tools such as Data Dog and M. Proficiency in programming languages (e.g., Java, Python, SQL). Knowledge of data governance, data modeling, and security best practices. Proficiency in CI/CD, IAC, and Agile Development. 

Leadership And Communication

Strong leadership and mentoring skills. Excellent communication and collaboration skills. Ability to explain complex technical concepts to both technical and non-technical audiences. 

Problem-Solving And Analytical Skills

Ability to identify and solve complex problems. Strong analytical skills to identify data quality issues and performance bottlenecks. 

Anywhere Real Estate Inc. (http://www.anywhere.re/)   (NYSE: HOUS) is moving real estate to what's next. Home to some of the most recognized brands in real estate Better Homes and Gardens® Real Estate (https://www.bhgre.com/) , Century 21® (https://www.century21.com/) , Coldwell Banker® (https://www.coldwellbanker.com/) , Coldwell Banker Commercial® (https://www.cbcworldwide.com/) , Corcoran® (https://www.corcoran.com/) , ERA® (https://www.era.com/) , and Sotheby's International Realty® (https://www.sothebysrealty.com/eng) , we fulfill our purpose to empower everyone's next move through our leading integrated services, which include franchise, brokerage, relocation, and title and settlement businesses, as well as mortgage and title insurance underwriter minority owned joint ventures. Anywhere supports nearly 1 million home sale transactions annually and our portfolio of industry-leading brands turns houses into homes in more than 118 countries and territories across the world.

At Anywhere, we are empowering everyone’s next move – your career included. What differentiates us is our scale, expertise, network, and unique business model that positions us as a trusted advisor throughout every stage of the real estate transaction. We pursue talent – strategic thinkers who are eager to always find a better way, relentlessly focus on talent, obsess about growth, and achieve exceptional results. We value our people-first culture, which thrives on empowerment, innovation, and cross-company collaboration as we keep moving the world forward, together. Read more about our company culture and values in our annual Impact Report (https://anywhere.re/wp-content/uploads/2025/03/2024-Impact-Report.pdf) .

We are proud of our award-winning culture and are consistently recognized as an employer of choice by various organizations including:

Great Place to Work Forbes World's Best Employers Newsweek World's Most Trustworthy Companies Ethisphere World's Most Ethical Companies 

EEO Statement: EOE including disability/veteran

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/5d758d6741344ca4ad1bb7d1bb657e58tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['pandas ', 'Excel ', 'bug tracking', 'R ', 'document processing', 'Kotlin ', 'uni ', 'git ', 'shell scripting']"
87,Lensa,https://www.linkedin.com/company/lensa/life,DATA ENGINEER,https://www.linkedin.com/jobs/view/4258450555,4258450555,"New York, NY",On-site,$96.6K/yr,2025-06-27 14:06:56,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for City of New York.

Job Description

New York City Emergency Management (NYCEM) helps New Yorkers before, during, and after emergencies through preparedness, education, and response. NYCEM is responsible for coordinating citywide emergency planning and response for all types and scales of emergencies. We are staffed by more than 200 dedicated professionals with diverse backgrounds and areas of expertise, including individuals assigned from other City agencies.

The Office of the Chief Operating Officer (COO) is comprised of Human Capital Management (HCM), Information Technology (IT), Support Services, and Geographic Information Systems (GIS). The Office of the COO is focused on implementing agency initiatives and strategies into daily operations to meet agency objectives and goals.

The Geographic Information Systems (GIS) unit produces reliable location-based information through the integration, analysis, and visualization of essential data distributed in the form of maps, datasets, summary metrics, and associated applications.

NYCEM Has An Exciting Opportunity For a Motivated Data Professional To Join The GIS Unit As a Data Engineer. NYCEM Is Creating a Data Integration Team Comprised Of Data Engineering And Geospatial Experts To Help Support The Agency’s Emergency Data Improvement Initiative. Reporting To NYCEM’s Director Of Enterprise Data Management, The Selected Applicant Will

Develop and maintain ETL pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources. Create databases for emergency events on demand, set up processes to join base data with event data, handle frequent updates, share data with other city agencies and outside organizations based on security protocols, and log data errors. Collaborate with internal divisions to acquire or create data sets needed for situational awareness and recovery operations. Collaborate with other city agencies to improve base data. As needed, convert base data into more useable form for emergency needs. Perform exploratory data analyses as needed. Collaborate with other data engineers on the best technologies to use, with a goal toward building a sustainable, robust framework. Participate in code reviews and develop documentation for both code and procedures. PLEASE NOTE THE FOLLOWING:

The selected candidate will be assigned to an on-call Emergency Operations Center (EOC) team and will be expected to work non-business hours during some emergencies. These non-business hours include nights, weekends, holidays, and extended week hours outside of a 9AM-5PM schedule. The selected candidate will also participate in trainings to build skills and competencies in emergency response; will participate in drills and exercises associated with the on-call EOC team; and may volunteer to assist with Ready NY emergency preparedness presentations to external groups. EOC teams are on call for three weeks at a time, with six weeks off in between.

Funding – This position is supported with a federal Urban Area Security Initiative (UASI) grant funding through 8/31/2027 with the possibility of an extension.

Candidates must be authorized to work in the United States without employer support to be eligible for selection.

The selected candidate will be required to be in person in the office location three days per week, with exceptions for extenuating circumstances.

For this position, the “Special Note” below in the Minimum Qualification Requirements doesapply.

IN ORDER TO BE CONSIDERED FOR THIS JOB, PLEASE SUBMIT A SEPARATE COVER LETTER IN THE ATTACHMENTS SECTION OF THE APPLICATION PORTAL.

Preferred Skills

Two or more years of experience in the field of data engineering. Proficiency in scripting processes with Python and SQL, automating processes using GitHub Actions, and utilizing containerization software like Docker. Experience with cloud-based data warehouses like Google BigQuery and ArcGIS Online. Familiarity with spatial data formats and experience working with spatial data in Esri products, Carto, PostGIS, or GeoPandas. Familiarity with NYC data sets and geocoding processes. Strong organizational skills and attention to detail. Strong presentation skills. Strong interpersonal skills. Experience working with technical and non-technical staff. Strong initiative and ability to perform with minimal supervision. Prior experience with emergency management operations and/or NYC municipal government operations is a plus. 

Competencies

In addition, the selected candidate will be able to demonstrate a proven ability in the following areas, from the agency’s performance management model:

Knowledge – possesses appropriate subject matter expertise.Work Ethic and Productivity – produces consistently high quality, accurate, and on-time deliverables; takes responsibility, is dependable, and accountable, and follows through; is responsive to requests from leadership.Strategic Problem Solving and Innovation – is thoughtful and deliberate in approach to solving problems; demonstrates innovation and creative thinking.Effective Communication – communication is clear, precise, and timely; understands their audience and display confidence in delivering their message.Teamwork – encourage collaboration and motivate others; is able to both lead and follow when necessary; is an active listener and consider a broad range of perspectives.

Studies have shown that women, people of color, and other under-represented groups are less likely to apply for jobs unless they believe they are able to perform every task in the job description. We are interested in finding the best candidate for the job and will consider any equivalent combination of knowledge, skills, education and experience to meet qualifications. If you are interested in applying, we encourage you to think broadly about your background and skill set for the role.

EMERGENCY PREPAREDNESS SPECIAL - 94612

Qualifications

A four-year high school diploma or its educational equivalent approved by a state's department of education or a recognized accrediting organization and six years of satisfactory full-time professional experience in one or a combination of the following: emergency management, fire, police, or military service, public safety, public health, public administration, urban planning, engineering, or another specialized area to which the appointment is to be made; or A baccalaureate degree from an accredited college and two years of satisfactory full time professional experience in the areas listed in ""1"" above; or A master's degree from an accredited college in emergency management, public administration, urban planning, engineering, economics, political science, the physical sciences ,or related field and one year of satisfactory full-time professional experience in the areas listed “1"" above, at least two years of which must have been in one of those areas, or another specialized area to which the appointment is to be made. Education and/or experience equivalent to ""1"", ""2"", or ""3"" above. However, all candidates must have a four-year high school diploma or its educational equivalent. 

Special Note

To be eligible for placement in Assignment Level II, individuals must have, after meeting the minimum requirements, one additional year of professional experience as described in """"1"""" above.

Additional Information

The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy.

Salary Min: $ 96,627.00

Salary Max: $ 96,627.00

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/017e4bc738af4563803bfeb7bf2bcd41tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['python', 'sql', 'database management', 'programming', 'cloud services', 'AWS', 'container management', 'Linux', 'project management', 'SQL']"
88,Lensa,https://www.linkedin.com/company/lensa/life,Power BI Developer / Fabric Admin (Remote),https://www.linkedin.com/jobs/view/4256757447,4256757447,"Oklahoma City, OK",Remote,$140K/yr - $150K/yr,2025-06-27 13:53:58,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currentlyseekinganexperienced and proactivePower BI Developer /Fabric Admin (Remote)to manage and support the enterprise-wide Power BI environment, focusing onPower BI Service administration, Fabric platform oversight, user support across tiers, and end-to-end report lifecycle management. This role requires technical depth in Power BI tools and infrastructure, combined witheffective communicationand stakeholder engagement to work across theCenter of Excellence (COE), business units, and IT support.This position will be fully remote located within the United States.

Responsibilities

Serve as aPower BI Fabric Administrator, overseeing workspace management, deployment pipelines, permissions, and performance monitoring. Develop aPower BI Center of Excellence (COE)to enforce standards, best practices, and governance for enterprise Power BI usage. Conducts PBI Training (Instructor-led Class Development, Delivery). ProvideTier 1 and Tier 2 supportfor Power BI issues—including report access, refresh failures, dataset issues, and workspace configuration. Assist withSelf-Help resources, including FAQs, templates, training materials, and troubleshooting documentation to empower end users. ManagePower BI licensing and desktop software distribution, working with ITassetsand licensing teams to track usage and entitlements. Support integration withexisting data sourcesandfacilitatethedesign and onboarding of new data sources, ensuring data quality, refresh schedules, and secure access. Handle incomingPower BI reporting and workspace requeststhrough a managed queue or mailbox, ensuringtimelyprioritization, response, and resolution. Design, publish, andmaintainPower BI reports and dashboardsaligned with business requirements andoptimizedfor usability and performance. Create andmaintainrobusttechnical documentation, including data dictionaries, architecture diagrams, workspace catalogs, and refresh schedules. Engage in proactive system health checks and performance tuning of Power BI Service, datasets, and gateway configurations. 

Qualifications

Required Skills and Experience

Bachelorswith 12+ years (orcommensurateexperience). Strong experience withPower BI Service Administration, Fabric workspace management, and deployment pipelines. Familiarity withPower BI governance, tenant settings, and organizational policies. Experience supporting and escalating issues acrossTier 1 and Tier 2 support models, including Service Desk and End-User Enablement. Knowledge ofPower BI licensing models(Free, Pro, Premium, PPU) and integration with Microsoft 365 administration tools. 

Preferred Skills And Experience

Solid understanding ofdata connectivity, gateway setup, refresh scheduling, and source control for bothexisting and new data sources.

Clearance Requirements

Ability to obtain and maintain a suitability/public trust clearance

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $140,000.00 - USD $150,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6140/power-bi-developer---fabric-admin-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6140

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/601a7a9de4fc4446ad3dada82d6c8e5etjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'AWS', 'SQL', 'container orchestration', 'cloud management', 'team leadership', 'report generation', 'data analysis', 'performance optimization', 'customer support']"
89,Predica Inc,https://www.linkedin.com/company/predicainc/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4258547345,4258547345,"McLean, VA",Hybrid,,2025-06-27 21:56:00,True,over 100 applicants,"Job Description::Senior Java Developer with SparkLocation: Mclean, VA/ Richmond, VA / Wilmington, DE (Onsite Hybrid Mode)Duration: 12 months with possible extension 
Description:Client is building a serverless pipeline to migrate all core data to Bank Core Platforms.
Role Detail: Back End Engineer (Python, SQL, Databricks, Tableau, Java, ETL, SPARK, AWS, Glue)
Technologies Used:Programming language: Java(Very Strong)AWS Services: Glue, Step Functions, SNS, SQS, Lambda, S3Open source frameworks
Description:Summary:High Level Perspective of Duties / Responsibilities: Need a Contractor at the level of Senior DA,Develop and implement automation solutions for diverse business needs, leveraging programming languages like SQL/Python to manipulate and analyze data from various sources.Design and build data models to extract, transform, and load data from our data warehouse, integrating it with other datasets, including static filesCollaborate with internal partners to identify data discrepancies and issues, providing support in quality control, data validation, and sample data provision.Fulfill ad-hoc data requests from various departments, including Finance and Benefits, ensuring timely and accurate delivery of critical information.Contribute to projects focused on improving data integrity and efficiency across the organization. Nice to have:Ex Client experience LOB - HR compensationThey deal with Client compensation and their partners Role Info:Looking for a Data Analyst with strong SQL, Python & Tableau/ BI Tools.As they are working with HR Team, Presentation and clear communication skills must have. Top Skills:5+ years of exp. as a Data AnalystSQL exp. is a mustPython exp. is mustTableau/ BI Tools ex. is a mustSnowflake & Databricks are highly preferredAWS ex. is a must Nice to Have:QuickSight/Google Apps scrub/ MS ExcelGitHub",https://www.linkedin.com/job-apply/4258547345,"['database management', 'cloud computing', 'Python', 'SQL', 'AWS', 'containerization', 'Docker', 'Terraform', 'Git', 'communication skills']"
90,Analytica,https://www.linkedin.com/company/analytica-inc/life,Data Engineer,https://www.linkedin.com/jobs/view/4246666534,4246666534,United States,Remote,$120K/yr - $130K/yr,2025-06-07 17:52:51,False,,"Analytica is seeking a Data Engineer to support a complex data program for the Defense Health Agency. This role will work closely with reporting and analytics developers, data governance and data architects to build robust, high quality data pipelines that enhance productivity and operational efficiency. This position is for US Citizens only and candidate will hold (or be able to hold) a US Secret Clearance. The position is a remote position with occasional onsite meetings in at San Antonio, TX as required.
Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.
Responsibilities: Support the Military Health System (MHS) by supporting data engineering that enhance the Military Health System's (MHS) ability to leverage data as a strategic assetEnsure data interoperability, governance, and quality to support enterprise-wide decision-making and healthcare optimization play a critical role in designing and implementing data architecture frameworks that enhance the Military Health System's (MHS) ability to leverage data as a strategic asset.Design and build robust and scalable data pipelines for managing structured and unstructured data using traditional databases (Oracle, PostgreSQL, etc.) or cloud Databases such as Amazon Redshift or AWS VerticaTranslate business needs into:data architecture solutions development within supported data systems.data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teamsMonitor and troubleshoot data import, analysis, and display errors.
Required Qualifications: Bachelor’s degree in information technology, Computer Science, Engineering or equivalent technical field3+ years creating data pipelines using cloud based ETL tools, like AWS Glue (AWS preferred)2+ years experience with Python and SQL (Java and Python preferred)Experience working on relational NoSQL and SQL databasesExperience designing and implementing various data pipeline patterns and strategiesStrong knowledge of data security principlesPrior experience with DHA M2 or MDR databases, CERNER EHR a strong plusMust be a US Citizen and must possess or be eligible for a SECRET clearance
About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

Analytica LLC is an Equal Opportunity Employer. We are committed to providing equal employment opportunities to all individuals, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other characteristic protected by applicable federal, state, or local law. As a federal contractor, we comply with the Vietnam Era Veterans' Readjustment Assistance Act (VEVRAA) and take affirmative action to employ and advance in employment qualified protected veterans.We ensure that all employment decisions are based on merit, qualifications, and business needs. We prohibit discrimination and harassment of any kind. Analytica LLC also provides reasonable accommodations to applicants and employees with disabilities, in accordance with applicable lawsWhen receiving email communication from Analytica, please ensure that the email domain is analytica.net to verify its authenticity.",https://analyticallc.applytojob.com/apply/gSgWiMyGlM/Data-Engineer,"['python', 'troubleshooting', 'problem-solving', 'analytics', 'AI/ML', 'API development', 'cybersecurity', 'teamwork', 'communication']"
91,Inoltra,https://www.linkedin.com/company/inoltra/life,Microsoft Fabric Data Engineer,https://www.linkedin.com/jobs/view/4255864096,4255864096,"Minnesota, United States",Hybrid,,2025-06-27 21:00:11,True,over 100 applicants,"Microsoft Fabric Data Engineer
Location: Based anywhere in the US, must travel 2-3 days per week to Minnesota
Employment Type: Contract - 6 Months
About the ClientOur client is a large global consultancy that works on large scale implementation projects.
Must have:Minimum 10 years of experience with Microsoft FabricOver 5 years of experience in data engineering, with hands-on experience using Azure Data ServicesDesign and implement robust data pipelines using Microsoft Fabric and Synapse Data Engineering experiencesBuild and manage Lakehouses and integrate with OneLake, supporting data ingestion, transformation, and consumptionStrong proficiency with Microsoft Fabric, including Lakehouse, Data Pipelines, Notebooks, and Real-Time AnalyticsSolid experience with Azure Data Factory, Azure Synapse, and Azure Data LakeProficiency in SQL, PySpark, and data modeling (dimensional and relational)Familiarity with Power BI and its integration with Fabric datasets
If you're interested, reach out to me at Jonathan@inoltra.co or apply now!
Not the right fit? Explore all our open roles: https://lnkd.in/emUMRHZJ
Want to hire the world's best SAP talent? Then contact Inoltra today and leverage our 100+ years of combined SAP recruitment experience, where we’ve supported more than 150 clients across 30 countries, delivering tailored IT staffing solutions. Learn more about us: https://www.linkedin.com/company/inoltra/life/aboutus
Partner with Inoltra and tap into 100+ years of combined SAP recruitment expertise. We’ve helped 150+ clients in 30 countries with tailored IT staffing solutions - let us do the same for you. Contact our team today: Hello@Inoltra.co
Stay updated on SAP opportunities! Subscribe here: https://lnkd.in/eUFMK7fw",https://www.linkedin.com/job-apply/4255864096,"['listing', 'python', 'sql', 'container', 'AWS', 'snowflake', 'project management', 'communication', 'teamwork', 'problem-solving']"
92,Lensa,https://www.linkedin.com/company/lensa/life,"Data Engineer I, Payment Acceptance & Experience",https://www.linkedin.com/jobs/view/4258454354,4258454354,"Seattle, WA",On-site,$91.2K/yr - $185K/yr,2025-06-27 14:06:40,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Amazon.

Description

The Payment Acceptance & Experience (PAE) team is looking for a Data Engineer with a deep understanding of the full life-cycle of data generation and application. The PAE team is responsible for how Amazon’s customers pay on Amazon’s sites and through Amazon’s services around the globe.

About The Role

As a Data Engineer I, you will design, develop, implement, test, and operate large-scale, high-volume, high-performance data structures for analytics, reporting and machine learning. You will leverage your expertise in data pipelines and persistence to build systems and tools that lower the cost of performing advanced analysis while also expanding the number and types of analyses that can be performed. You will take the lead in identifying architecture deficiencies, and solving for the same by building advanced software solutions.

Key job responsibilities

 Implement data ingestion routines, both real time and batch using best practices in data modeling Develop ETL/ELT processes leveraging AWS technologies and Big data tools. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make recommendations around dataset implementations designed and proposed by peer data engineers. Evaluate and make recommendations around the use of new or existing software products and tools.

A day in the life

Amazon offers a full range of benefits that support you and eligible family members, including domestic partners and their children. Benefits can vary by location, the number of regularly scheduled hours you work, length of employment, and job status such as seasonal or temporary employment. The benefits that generally apply to regular, full-time employees include: 1. Medical, Dental, and Vision Coverage 2. Maternity and Parental Leave Options 3. Paid Time Off (PTO) 4. 401(k) Plan.

If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply!

Basic Qualifications

1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets Knowledge of AWS Infrastructure 

Preferred Qualifications

Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. 

Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.

Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits . This position will remain posted until filled. Applicants should apply via our internal or external career site.

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/6b6241c58daa49089a090ef7f0af52f8tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['project management', 'Git', 'Docker', 'AWS', 'Apache Spark', 'SQL', 'Ansible', 'Content Delivery Network (CDN)', 'Jenkins', 'ELK stack (Elasticsearch', 'Logstash', 'Kibana)']"
93,Billy Graham Evangelistic Association,https://www.linkedin.com/company/bgea/life,Senior Data Visualization Developer,https://www.linkedin.com/jobs/view/4256795798,4256795798,United States,Remote,,2025-06-27 15:40:17,False,,"Description

Is God calling you to ministry?

For over 70 years, The Billy Graham Evangelistic Association (BGEA) has proclaimed the Gospel through its various ministries. Whether you work at our headquarters in Charlotte, North Carolina, the Billy Graham Library in Charlotte, North Carolina, the Billy Graham Training Center at the Cove (BGTC) in Asheville, North Carolina, Blue Ridge Broadcasting (BRB) in Asheville, North Carolina or a field office abroad, you are part of a team of committed Christians working together to reach people to the ends of the earth with the Gospel.

Christian Purpose And Expectations

All employees proclaim the Gospel of Jesus Christ through their employment, and thereby further BGEA’s religious mission.

Department Detail – The BGEA Information Technology (IT) department consists of the following areas: Operations (networking/routing, servers, storage, security cameras, telephony), Service Desk (end-user hardware and software management/support, event support, general IT assistance), Cybersecurity (threat defense, monitoring/analysis, awareness, and compliance), Enterprise Solutions (application and web development, data visualization, report writing, database management), and Enterprise Solutions Support (Donor/CRM, Financial, HR, and other ERP type solutions support). The IT department furthers the mission of BGEA through supporting, developing, scaling, and securing the assets, infrastructure, and programs required by all areas of the ministry while maintaining and ensuring their availability.

Job Summary – The Senior Data Visualization Developer will design, develop, and optimize data visualizations, dashboards, and reports. This role requires strong expertise in Tableau Desktop, along with a strong understanding of SQL for data querying and transformation. The ideal candidate will collaborate with business stakeholders and data teams to deliver actionable insights through interactive and scalable Tableau solutions.

Ministry Requirements

Maintains a personal, active relationship with Jesus Christ and is a consistent witness for Jesus ChristFaithfully upholds BGEA in prayerParticipates in daily BGEA staff devotions including Bible reading, and leading group prayer as schedule permitsDemonstrates behavior aligned with BGEA’s Mission Statement, Statement of Faith, Hallmarks, policies, and expectationsEffectively represents Jesus Christ to those within both personal and professional spheres of influence

Essential Duties And Responsibilities

Design, develop, and optimize Tableau dashboards and reports to provide meaningful business insights. Implement best practices for data visualization, ensuring clarity, accuracy, and usability.Write and optimize complex SQL queries for data extraction and transformation. Connect Tableau to multiple data sources, including relational databases (SQL Server and muscle).Monitor and improve the performance of Tableau workbooks, ensuring efficient data loading and visualization rendering.Work closely with business users, analysts, and data engineers to gather requirements and translate them into Tableau solutions.Implement security best practices, ensuring appropriate access control and data governance policies are followed. Ensure data integrity and consistency across all Tableau reports and dashboards.Experience with Tableau Server configurations, permissions, and publishing workflows.

Marginal Duties and Responsibilities

Other duties as assigned

Job Specifications

Skills and Knowledge

4+ years of experience in Tableau Desktop developmentExperience in improving workbook performance through best practices such as extract optimization, indexing, and calculated field efficiency.4+ years experience working with databases like SQL Server and MySQL, with the ability to integrate and manipulate large datasets.Strong ability to interact with business users and translate their needs into effective Tableau solutions.

Physical/Mental Demands

Work requires ability to work for extended periods at computer workstationFlexibility to readily adapt when job parameters, deadlines, or directions changeAbility to work closely with others as part of a teamAbility to maintain confidentialityAbility to work effectively in virtual environmentAbility to work outside of regular business hours

Working Conditions

Work is typically performed in a conventional office environment

Mission Statement

Continuing the lifelong work of Billy Graham, the Billy Graham Evangelistic Association exists to support and extend the evangelistic calling and ministry of Franklin Graham by proclaiming the Gospel of the Lord Jesus Christ to all we can by every effective means available to us and by equipping the church and others to do the same.

Statement of Faith

These core beliefs make up the foundation upon which all our ministry endeavors are based.

We believe the Bible to be the inspired, the only infallible, authoritative Word of God revealing the love of God to the world. (1 Thessalonians 2:13; 2 Timothy 3:15-17; John 3:16).We believe that there is one God, eternally existent in three persons: Father, Son, and Holy Spirit. (Matthew 28:19; John 10:30; Ephesians 4:4-6).We believe in the deity of the Lord Jesus Christ, in His virgin birth, in His sinless life, in His miracles, in His vicarious and atoning death through His shed blood on the cross, in His bodily resurrection, in His ascension to the right hand of the Father, and in His personal return in power and glory. (Matthew 1:23; John 1:1-4 and 1:29; Acts 1:11 and 2:22-24; Romans 8:34; 1 Corinthians 15:3-4; 2 Corinthians 5:21; Philippians 2:5-11; Hebrews 1:1-4 and 4:15).We believe that all men everywhere are lost and face the judgment of God, that Jesus Christ is the only way of salvation, and that for the salvation of lost and sinful man, repentance of sin and faith in Jesus Christ results in regeneration by the Holy Spirit. Furthermore, we believe that God will reward the righteous with eternal life in heaven, and that He will banish the unrighteous to everlasting punishment in hell. (Luke 24:46-47; John 14:6; Acts 4:12; Romans 3:23; 2 Corinthians 5:10-11; Ephesians 1:7 and 2:8-9; Titus 3:4-7).We believe in the present ministry of the Holy Spirit, whose indwelling enables the Christian to live a godly life. (John 3:5-8; Acts 1:8 and 4:31; Romans 8:9; 1 Corinthians 2:14; Galatians 5:16-18; Ephesians 6:12; Colossians 2:6-10).We believe in the resurrection of both the saved and the lost; the saved unto the resurrection of eternal life and the lost unto the resurrection of damnation and eternal punishment. (1 Corinthians 15:51-57; Revelation 20:11-15).We believe in the spiritual unity of believers in the Lord Jesus Christ and that all true believers are members of His body, the church. (1 Corinthians 12:12, 27; Ephesians 1:22-23).We believe that the ministry of evangelism (sharing and proclaiming the message of salvation only possible by grace through faith in Jesus Christ) and discipleship (helping followers of Christ grow up into maturity in Christ) is a responsibility of all followers of Jesus Christ. (Matthew 28:18-20; Acts 1:8; Romans 10:9-15;1 Peter 3:15).We believe God’s plan for human sexuality is to be expressed only within the context of marriage, that God created man and woman as unique biological persons made for each other. God instituted monogamous marriage between male and female as the foundation of the family and the basic structure of human society. For this reason, we believe that marriage is exclusively the union of one genetic male and one genetic female. (Genesis 2:24; Matthew 19:5-6; Mark 10:6-9; Romans 1:26-27; 1 Corinthians 6:9).We believe that we must dedicate ourselves to prayer, to the service of our Lord, to His authority over our lives, and to the ministry of evangelism. (Matthew 9:35-38; 22:37-39, and 28:18-20; Acts 1:8; Romans 10:9-15 and 12:20-21; Galatians 6:10; Colossians 2:6-10; 1 Peter 3:15).We believe that human life is sacred from conception to its natural end; and that we must have concern for the physical and spiritual needs of our fellowmen. (Psalm 139:13; Isaiah 49:1; Jeremiah 1:5; Matthew 22:37-39; Romans 12:20-21; Galatians 6:10).We believe God wonderfully and immutably creates each person a biological male or female. These two distinct, but complementary sexes together reflect the image and likeness of God. Rejection of one’s biological sex is a rejection of God’s merciful design and creative order. (Genesis 1:26-27; Mark 10:6; 1 Corinthians 6:9; Deuteronomy 22:5).

Expectations for Ministry Employment

As An Employee Of Billy Graham Evangelistic Association

I acknowledge that the Lord Jesus Christ is my personal Savior and that I am a personal representative of Him.I understand that BGEA is a Christian organization whose purpose is proclaiming the message of the Gospel of the Lord Jesus Christ throughout the world.I agree that the purpose of my employment with BGEA is to further its Christian purpose, that I am prepared to assist in accomplishing that purpose, and that my role with BGEA plays a vital part in carrying out its Christian purpose and mission.I understand that I must exhibit conduct that is consistent with BGEA’s expectations, whether at work or away from work, in keeping with scriptural teachings and principles as set forth in God’s Word, BGEA’s Statement of Faith, and BGEA’s policies including Christian Conduct.I understand that BGEA has the right and the responsibility to ensure that its Christian religious purpose is carried on with the highest standards and is not harmed or impeded by conduct that is inconsistent with the Bible, BGEA’s Statement of Faith, its Christian religious purpose, or its policiesI understand that any of my conduct which is not in keeping with the scriptural teachings and principles as set forth in God’s Word, BGEA’s Statement of Faith, and BGEA’s policies, is inconsistent with BGEA’s Christian religious purpose.I understand that if my conduct is determined by BGEA to be inconsistent with its Christian religious purpose, the result will be corrective action up to and including termination from employment.

The Title VII Exception for Employment

The Billy Graham Evangelistic Association (BGEA) is a faith-based religious organization committed to spreading the Good News of Jesus Christ through every effective means. Consistent with our charitable purpose to share the Christian faith, a requirement for employment at BGEA is affirmation and adherence to our Christian Statement of Faith. Our Statement of Faith prerequisite for employment is based upon federal law set forth in Title VII of the Civil Rights Act of 1964, 42 U.S.C. Section 2000e-1.

Benefits Summary

Full-Time employees working at least 36 hours per week, are eligible for benefits.

Medical, prescription, dental and vision insuranceFlexible Spending Account (FSA)Long term and short term disability insuranceTerm Life insurance401(k) retirement savings plan10 paid holidays, including 1 floating holidayMinimum 12 days of vacation10 sick days",https://careers.billygraham.org/jobs/16352701-senior-data-visualization-developer?bid=56,"['Python', 'SQL', 'Java', 'AWS', 'containerization', 'Docker', 'SQL', 'Salesforce', 'Microsoft Office', 'project management']"
94,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer,https://www.linkedin.com/jobs/view/4258449547,4258449547,United States,Remote,,2025-06-27 14:06:57,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Zeelo.

Data Engineer

Remote - UK based (Occasional travel to London - likely quarterly for Kickoffs)

As a Data Engineer, you will lay the groundwork for extracting value from data by developing and maintaining powerful and efficient data infrastructure. In addition, you will help Zeelo's Data Team become the center of excellence on data and responsible for educating other teams, establishing best practices, and facilitating knowledge sharing on data-related matters across the company.

Zeelo and its clients have many data analytics needs all supported by Zeelo's data team. You will support data analysts by engineering ETL pipelines to deliver useful, clean, clear, and timely data ready for analytics use. Zeelo's data maturity is growing, and you will help incorporate streaming and AI tools into the business effectively. You will also maintain Zeelo's serverless data architecture and improve its functionality and efficiency.

What We Want You To Know About Zeelo

Zeelo is on a mission to make shared transportation more accessible, efficient, and sustainable. We're scaling fast, and this is a chance to help shape the future of our technology in a role where you'll have real ownership and impact.

Zeelo is a transit-tech company powering bus operators, employers and schools to provide highly efficient, sustainable, and affordable transport programs. Our mission is to empower opportunity through sustainable transportation. Our vision is to build the category leader for employers and schools offering transportation as a benefit. Our culture strives to match a high performing sports team. We are inspired by the “Ubuntu” mindset: I am, because we are. Our model is asset light, we do not own vehicles or employee drivers, instead we routinely procure bus operator partners to provide ground transportation We're a team of 130+ across 3 offices (London, Barcelona & Boston) and our transit services are live in 2 markets (UK & US) Our values are Trust, Efficiency, and Drive. 

What will I be doing?

Design, build, and maintain scalable and reliable data pipelines. Manage Zeelo's serverless centralized data architecture (Fivetran, BigQuery, dbt, and other tools) that supports analytical functions across the business. Design, build, and maintain ETL, ELT and other data pipelines for purposes to support analytics use cases. Identify improvements in how Zeelo collects, manages and leverages internal and external data sources. Identify and deliver improvements to scalability and cost. Optimize queries and pipelines for cost and performance. Develop data infrastructure to power accurate and efficient analytics. Be a champion of data and analytics within the company, educating team members and supporting data use throughout the business. Work with transportation data including location data, scheduling data, ridership data, and financial data. Write clear documentation on the mechanics of the data architecture. 

Skills And Experience We're Looking For

Bachelor's degree in a quantitative field. Advanced degrees are a plus. Min 3+ years data engineering experience in a commercial environment. Proficiency in SQL. Experience building SQL-based transformation flows in dbt or similar tools. Good understanding of cloud platforms such as GCP, AWS or Azure. Experience configuring orchestration of SQL and Python via Airflow or similar tools. Experience working with data pipelines, defining problems, crafting and launching solutions, and practicing continuous improvement. Experience with process improvement frameworks and/or project management frameworks is a plus. Experience maintaining a data warehouse including adding features to improve utility and refactoring to reduce costs. Knowledge of data modeling best practices. Experience with REST APIs. Experience building unit tests or working with testing frameworks. Experience with data governance and security. A passion for sustainability, technology, and improving mobility. Experience developing in Python (optional). Experience with transportation systems (optional). 

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/d36ec63b15e6485cabb62d4a02815ad1tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=jse,"['Python', 'SQL', 'Container Shipment Management', 'AWS (Amazon Web Services)', 'Sales Proposal', 'Snowflake', 'Data Analysis', 'Active Listening', 'Conflict Resolution', 'Domain Knowledge']"
95,Lensa,https://www.linkedin.com/company/lensa/life,Principal Engineer Data Engineering - US Remote,https://www.linkedin.com/jobs/view/4256761227,4256761227,"New York, NY",Remote,,2025-06-27 13:53:43,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for Anywhere Real Estate.

Anywhere is at the forefront of driving the digital transformation and building best-in-class products that help our agents and brokers sell more homes, make more money, and work more efficiently.

Data & Analytics (DNA) is Anywhere's data arm. We create innovative analytics, data science, and robust data foundation capabilities to generate data-driven insights that serve the heart of Anywhere Advisor and Anywhere Brand business. Together with our business counterparts in the real estate business, we work daily to deliver differentiating insights (AI & BI) for Strategy and AA & AB Operations.

We're seeking a Principal Engineer to join our Data Platform Team. In this critical position, you'll be responsible for designing, implementing, and managing the data infrastructure. You will work closely with data scientists, software engineers, and other stakeholders to ensure the Data Platform's availability, usability, and integrity.

Data Infrastructure Design And Implementation

Evaluate, select, and implement new tools and frameworks to expand our data platform capabilities. Design, build, and maintain robust, scalable, and reliable data pipelines and ETL processes. Develop and maintain data infrastructure and platforms using various technologies (e.g., AWS, Snowflake Cloud Platforms, databases, Kafka streaming platforms). Ensure data quality, consistency, and integrity across the organization. Architect and optimize Data Ingestion and Snowflake ETLs. Production Support and enhancements to the observability of the Data Platform. 

Team Leadership And Mentorship

Lead and mentor data engineers, providing guidance and support to junior engineers. Foster a culture of technical excellence and continuous learning. Collaborate with other teams (e.g., data scientists, software engineers, and product managers) to ensure data solutions meet business needs. 

Data Security And Compliance

Implement and maintain data security measures to protect sensitive data. Ensure compliance with data protection regulations and industry standards. 

Problem Solving And Innovation

Identify and solve complex data-related problems. Stay abreast of industry trends and emerging technologies and identify opportunities to enhance data capabilities. Proactively address performance, scale, complexity, and security considerations. 

Skills And Qualifications

Technical Expertise:

10+ years’ experience with a strong understanding of data engineering principles and technologies. 10+ years’ experience with data pipelines, ETL processes, and data warehousing. 5+ years’ experience building data pipelines using Kafka, Kafka Connect, Airflow, and Snowflake. 5+ years’ experience with Snowflake Data Platform. 5+ years’ experience with AWS Data Services such as DMS, EMR, Glue, Athena, S3, CloudWatch, Lambda, or IAM. 5+ years’ experience with Data Quality, Data Reconciliation 5+ years’ experience managing production data platforms. 5+ years’ experience building observability (Monitoring & Alerting) using tools such as Data Dog and M. Proficiency in programming languages (e.g., Java, Python, SQL). Knowledge of data governance, data modeling, and security best practices. Proficiency in CI/CD, IAC, and Agile Development. 

Leadership And Communication

Strong leadership and mentoring skills. Excellent communication and collaboration skills. Ability to explain complex technical concepts to both technical and non-technical audiences. 

Problem-Solving And Analytical Skills

Ability to identify and solve complex problems. Strong analytical skills to identify data quality issues and performance bottlenecks. 

Anywhere Real Estate Inc. (http://www.anywhere.re/)   (NYSE: HOUS) is moving real estate to what's next. Home to some of the most recognized brands in real estate Better Homes and Gardens® Real Estate (https://www.bhgre.com/) , Century 21® (https://www.century21.com/) , Coldwell Banker® (https://www.coldwellbanker.com/) , Coldwell Banker Commercial® (https://www.cbcworldwide.com/) , Corcoran® (https://www.corcoran.com/) , ERA® (https://www.era.com/) , and Sotheby's International Realty® (https://www.sothebysrealty.com/eng) , we fulfill our purpose to empower everyone's next move through our leading integrated services, which include franchise, brokerage, relocation, and title and settlement businesses, as well as mortgage and title insurance underwriter minority owned joint ventures. Anywhere supports nearly 1 million home sale transactions annually and our portfolio of industry-leading brands turns houses into homes in more than 118 countries and territories across the world.

At Anywhere, we are empowering everyone’s next move – your career included. What differentiates us is our scale, expertise, network, and unique business model that positions us as a trusted advisor throughout every stage of the real estate transaction. We pursue talent – strategic thinkers who are eager to always find a better way, relentlessly focus on talent, obsess about growth, and achieve exceptional results. We value our people-first culture, which thrives on empowerment, innovation, and cross-company collaboration as we keep moving the world forward, together. Read more about our company culture and values in our annual Impact Report (https://anywhere.re/wp-content/uploads/2025/03/2024-Impact-Report.pdf) .

We are proud of our award-winning culture and are consistently recognized as an employer of choice by various organizations including:

Great Place to Work Forbes World's Best Employers Newsweek World's Most Trustworthy Companies Ethisphere World's Most Ethical Companies 

EEO Statement: EOE including disability/veteran

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/55e3bd7179254b258d80ff84fd3352a7tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Leadership', 'project management', 'communication', 'analytics', 'problem-solving', 'teamwork', 'adaptability', 'innovation', 'SQL', 'AWS']"
96,Lensa,https://www.linkedin.com/company/lensa/life,ETL Engineer (Remote),https://www.linkedin.com/jobs/view/4256755851,4256755851,"Indianapolis, IN",Remote,$90K/yr - $110K/yr,2025-06-27 13:53:50,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

GovCIO is currently hiring an ETL Engineer (Remote) to combine data from different sources into unified views for analysis and reporting. This position transforms data into formats that can be analyzed and loads it into a target database or data warehouse. This position will be fully remote located within the United States.

Responsibilities

Follow data quality best practices. Maintain data domains. Oversee daily operations. Combine data from different sources into unified views. Perform ongoing VistA table configuration Support existing and new ETL processes, along with data distribution 

Qualifications

Required Skills and Experience:

Bachelor's with 5 - 8 years (or commensurate experience)

Preferred Skills And Experience

Familiarity with Agile methodologies and project management tools. Certifications in relevant technologies. 

Clearance Required: Ability to obtain and maintain a suitability/Public Trust

#TAPS

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $90,000.00 - USD $110,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6130/etl-engineer-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6130

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/ef853c351e7547c883bd8d044dce3996tjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['python', 'data analysis', 'SQL', 'AWS', 'container', 'container orchestration', 'machine learning', 'cloud services', 'AWS', 'AWS', '---', 'Now', ""let's proceed with presenting the scenarios and solutions you requested."", '**Scenario 1 – Water Purification System: Energy Usage**', 'Imagine designing a water purification system that needs to balance purification capacity with energy consumption. The system requires inputs for water source', 'impurities', 'budget', 'and desired purity level. A proposal draft would detail the technologies used', 'energy sources', 'resource utilization', 'scalability', 'and future upgrade potentials. An implementation proposal would outline the engineering processes', 'budget estimation', 'timeline', 'required materials and tools', 'and a maintenance plan.', '**Conversation Section:**', 'This task involves creating a proposal for a water purification system that is efficient', 'cost-effective', 'and capable of meeting the desired purity standards. The proposal would include a cost and resource analysis', 'focusing on energy consumption. The engineering team is responsible for developing scalable solutions that minimize both operational costs and the environmental impact.', '**Solution Section:**', 'A water purification system requires a careful balance between effective impurity removal and energy consumption. In designing such a system', 'the following considerations and solutions will be proposed:', '- Assess the impurity types and concentrations in the water source.', '- Identify water purification methods suitable for the contaminants present.', '- Evaluate various energy-efficient purification technologies such as reverse osmosis', 'filtration', 'or UV treatment.', '- Estimate energy usage for the selected purification method.', '- Investigate the possibility of using renewable energy sources', 'such as solar or wind power', ""to reduce the system's carbon footprint."", '- Design the system to work within the specified budget while achieving the desired purity levels.', '- Explore the integration of smart monitoring tools to optimize energy use.', '- Plan for future scalability to adapt to increasing demand and stricter purity requirements.', 'Would you like to proceed with the project briefing now', 'or discuss any specific aspects in more detail?', '**Scenario 2 – Electronics Manufacturer System for Automated Factory: Safety and Collaboration**', 'This task entails installing an electronic system with an integrated chat functionality and safety supervision in an automated manufacturing environment. The system would facilitate real-time communication among employees and automated drones to enhance collaboration and maintain safety standards. The installation should include training for extended system functionality.', '**Conversation Section:**', 'The safety and collaborative needs of an automated factory can be significantly enhanced through real-time communication systems. We will focus on creating a user-friendly interface that allows seamless interaction between factory personnel and automated drones in a secure environment.', '**Solution Section:**', 'The proposed solution involves the following key features and considerations:', '- Overview: The system will integrate Internet of Things (IoT) and artificial intelligence to improve factory communication and safety.', '- The system will offer a customizable', 'user-friendly user interface', 'accessible via touchscreens or mobile devices.', '- Logical system structure features such as authentication touchpoints', 'communication', 'dispatch/control', 'monitoring/control/alert', 'and central monitoring dashboards.', '- Alarm management mechanisms for various system states to promptly identify and address safety concerns.', '- Enhanced system functionality for staff collaboration and drone coordination.', '- Visualization tools to graphically represent system status and alerts.', '- Training modules tailored to educational levels and roles.', '- Collaboration with local multimedia providers and search technology partners for live texture', 'audio', 'and video feed integration.', '- Installation of low-wattage', 'cost-effective cameras', 'along with structurally simple design netframes.', ""- The system's dashboard facilitates displaying real-time"", 'multimodal data to operate.', 'Would you like me to provide more details on the procurement process for the necessary components', 'or shall we discuss another topic?']"
97,Lensa,https://www.linkedin.com/company/lensa/life,Data Engineer / Data Architect (Remote),https://www.linkedin.com/jobs/view/4256756747,4256756747,"Carson City, NV",Remote,$118K/yr - $122K/yr,2025-06-27 13:53:51,False,,"Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency. Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf of its direct clients, recruitment ad agencies, and marketing partners. Lensa partners with DirectEmployers to promote this job for GovCIO.

Overview

We are seeking a highly skilled and experienced Data Engineer / Data Architect to join our team. In this role, you will architect and develop enterprise data models, metadata governance frameworks, ETL pipelines, and analytics platforms to support data governance, performance measurement, and business intelligence solutions for the VA Office of Information Technology (OIT). Your expertise in data modeling, ETL development, cloud data services, and metadata-driven architecture will be critical to the success of federal data platforms and analytics systems.

This position is a fully remote position within the United States.

Responsibilities

The role involves end-to-end design, development. and maintenance of robust relational data models in SQL Server 2019+Enterprise to support metadata governance, request workflows, audit tracking, and version-controlled publishing.

Design, build, and maintain relational data models in SQL Server 2019+ Enterprise to support metadata governance, request submissions, audit tracking, version control, and production publishing pipelines. Architect multi-stage relational models for metadata lifecycle management, including intake submissions, staging, review, audit trail snapshots, approval tracking, and final publishing structures. Develop and maintain SSIS packages and ETL pipelines for metadata ingestion, transformation, validation, auditing, staging, and publishing processes supporting multi-layered data warehouse architectures. Apply hybrid cloud integration expertise utilizing Azure SQL, Azure Data Factory, Databricks, Synapse Analytics, and Azure Fabric to develop and support scalable data pipelines and platform modernization initiatives. Collaborate closely with business analysts, Power BI developers, Power Apps developers, and VA stakeholders to gather requirements and translate functional business needs into scalable technical data solutions. Build Power Platform integrations (Power BI, PowerApps, Power Automate) connecting to SQL Server and cloud data services; implement identity-based row-level filtering using Power Platform user context for submission-level security and role-based access control. Maintain fully auditable data models, including metadata change tracking, approval lineage, governance version control, and compliance documentation to support ATO certification, PMO audit packages, and federal regulatory standards. Conduct advanced query tuning, stored procedure optimization, index management, query plan evaluation, blocking and deadlock resolution, and performance troubleshooting across large relational data platforms. Develop and maintain comprehensive technical documentation, including data dictionaries, entity-relationship diagrams (ERDs), data flow diagrams, process workflows, swimlane diagrams, system architecture diagrams, metadata catalogs, data lineage models, dependency mappings, and PMO governance documentation packages. 

Qualifications

Required Skills and Experience

Bachelor’s degree in Computer Science, Information Technology, or related field with 5 - 8 years of experience. (or commensurate experience) 8+ years of experience in SQL Server database development, data architecture, and ETL design. 5+ years of experience in data warehouse architecture, metadata governance, and analytics platforms. Deep hands-on experience with SQL Server 2019 Enterprise (on-prem), SSIS, T-SQL, and relational database design. Cloud experience with Azure or AWS platforms. Proficiency in Azure Data Factory, Databricks, Synapse Analytics, and hybrid cloud integration for modernizing ETL pipelines. Experience with Azure DevOps and/or GitHub for version control, CI/CD pipeline automation, and release management. Power Platform experience integrating Power BI, PowerApps, and Power Automate into enterprise reporting and data governance solutions. SharePoint Online experience supporting integrated metadata governance processes. Knowledge of metadata-driven design patterns, audit trail frameworks, and enterprise governance architecture aligned to federal PMO standards. Strong technical documentation skills, including ERDs, data dictionaries, data flows, and process mapping. Strong experience in performance tuning, query optimization, and long-running job remediation. Excellent troubleshooting, problem-solving, and analytical skills across the full data pipeline lifecycle. Familiarity with federal compliance frameworks (ATO, FISMA, NIST 800-53, FedRAMP). Ability to work within an established team and under quick turnaround requirements. Experience with Agile methodologies and project management tools. 

Clearance Required: Ability to obtain and maintain a Suitability/Public Trust clearance.

Preferred Skills And Experience

Microsoft Fabric platform (Power BI Premium, Data Warehousing, RLS security models). Additional experience with MongoDB or NoSQL stores. Python, PowerShell, C#, or other scripting for ETL and automation. Prior experience working within federal data governance or VA PMO environments. VA Data Analytic experience preferred. 

Company Overview

GovCIO is a team of transformers--people who are passionate about transforming government IT. Every day, we make a positive impact by delivering innovative IT services and solutions that improve how government agencies operate and serve our citizens.

But we can't do it alone. We need great people to help us do great things - for our customers, our culture, and our ability to attract other great people. We are changing the face of government IT and building a workforce that fuels this mission. Are you ready to be a transformer?

What You Can Expect

Interview & Hiring Process

If you are selected to move forward through the process, here’s what you canexpect:

During the Interview Process Virtual video interview conducted via video with the hiring manager and/or team Camera must be on A valid photo ID must be presented during each interview During the Hiring Process Enhanced Biometrics ID verification screening Background check, to include: Criminal history (past 7 years) Verification of your highest level of education Verification of your employment history (past 7 years), based on information provided in your application 

Employee Perks

Benefits

At GovCIO, we consistently hear that meaningful work and a collaborative team environment are two of the top reasons our employees enjoy working here. In addition, our employees have access to a range of perks and benefits to support their personal and professional well-being, beyond the standard company offered health benefits, including:

Employee Assistance Program (EAP) Corporate Discounts Learning & Development platform, to include certification preparation content Training, Education and Certification Assistance* Referral Bonus Program Internal Mobility Program Pet Insurance Flexible Work Environment Available to full-time employees

Our employees’ unique talents and contributions are the driving force behind our success in supporting our customers, which ultimately fuels the success of our company. Join us and be a part of a culture that invests in its people and prioritizes continuous enhancement of the employee experience.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, disability, or status as a protected veteran. EOE, including disability/vets.

Posted Pay Range

The posted pay range, if referenced, reflects the range expected for this position at the commencement of employment, however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, education, experience, and internal equity. The total compensation package for this position may also include other compensation elements, to be discussed during the hiring process. If hired, employee will be in an “at-will position” and the GovCIO reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, GovCIO or individual department/team performance, and market factors.

Posted Salary Range

USD $118,000.00 - USD $122,000.00 /Yr.

Submit a referral to this job (https://careers-govcio.icims.com/jobs/6179/data-engineer---data-architect-%28remote%29/job?mode=apply&apply=yes&in_iframe=1&hashed=-1834385473)

Location US-Remote

ID 2025-6179

Category Information Technology

Position Type Full-Time

If you have questions about this posting, please contact support@lensa.com",https://lensa.com/cgw/b95c0dbb0af9452cbdeb8cb9179b7eedtjo1?jpsi=directemployers&publisher_preference=easier_apply&utm_campaign=Computer%20Occupations&utm_medium=slot&utm_source=linkedin&utm_term=manual,"['Leadership ', 'Communication ', 'Financial Analysis ', 'Project Management ', 'Excel ', 'Customer Service ', 'Teamwork ', 'Sales ', 'Product ', 'Collaboration']"
98,Robertson & Company Ltd.,https://www.linkedin.com/company/robertson-&-company-ltd-/life,Data Engineer IV (Qualitative Model Developers),https://www.linkedin.com/jobs/view/4257042987,4257042987,"Wilmington, DE",Hybrid,,2025-06-27 21:36:30,True,66 applicants,"Banking or financial background is required.
W2 candidates only
**** Please apply only if you are open for hybrid 4 days in office in DE******
Contract Period: 15 monthsLocation: Wilmington, DELocation Type: Hybrid; Monday to Thursday work from office and 1 day work from homeBusiness Hours: Monday to Friday; 9 AM to 5 PM
MUST HAVE:-Strong background and 7+ years of experience in developing and deploying predictive models, including traditional regression algorithm, and machine learning techniques.-Solid experience and knowledge in financial services/banking (i.e. commercial and retail lending), stress testing (CCAR, DFAST) and model governance.-Proficient programming skill in SAS and Python-Strong written and verbal communication, and presentation skills.Experience in working in Unix/Linux, Microsoft Azure environment for analytics is a plusExperience working with relational databases, big data manipulation, and SQL preferred.
NICE TO HAVEStrong interest and ability to undertake applied researchExperience in time series data is an asset
Job Responsibilities:Understand business requirements, conduct business analysis, to define the business and technical requirements to solve problems and deliver solutionsAnalyze and manipulate large data sets, conduct applied research, design econometric models to forecast loan or deposit balances with relationships with macroeconomics, and streamline existing processes to improve efficiencyLead design, planning, implementation, and testing of various modeling initiatives and cross-functional projects, and working with and liaising with business partnersReview models with stake holders (e.g. business leaders, Validation, PPNR Subcommittees), and defend models and answer to challenges to get approvalsProduce and maintain well-articulated documentationWrite and maintain robust code for performing the above functions",https://www.linkedin.com/job-apply/4257042987,"['javascript', 'React', 'Node.js', 'TypeScript', 'MongoDB', 'Docker', 'Git', 'AWS', 'Azure', 'SQL']"
99,Georgia-Pacific LLC,https://www.linkedin.com/company/georgia-pacific-llc/life,Senior Data Engineer,https://www.linkedin.com/jobs/view/4255826062,4255826062,"Atlanta, GA",On-site,,2025-06-27 15:11:40,False,,"Our Team

Georgia-Pacific (GP) is among the world's leading manufacturers of bath tissue, paper towels, napkins, tableware, paper-based packaging, office papers, cellulose, specialty fibers, nonwoven fabrics, building products and related chemicals. Our building products business makes DensGlass® gypsum board often seen in commercial construction, DryPly® plywood and RESI-MIX® wood adhesives, among others. Our containerboard and packaging business offers high-end graphic packaging to bulk bins as well as Golden Isles fluff pulp. You may also recognize consumer brands like Angel Soft®, Brawny®, and Dixie® on retail shelves and enMotion® towels, Compact® bath tissue and SmartStock® cutlery dispensers when you are away from home. Our GP Harmon business is one of the world's largest recyclers of paper, metal and plastics. As a Koch Company, we create long-term value using resources efficiently to provide innovative products and solutions that meet the needs of customers and society, while operating in a manner that is environmentally and socially responsible, and economically sound. Headquartered in Atlanta, GA., we employ approximately 35,000 people. For more information, visit www.gp.com.

To learn more about our culture, Principle-Based Management (PBM®), click here:

https://www.principlebasedmanagement.com

LOCATION: Atlanta, GA onsite three days per week (we are only considering candidates in Atlanta at this time)

We are seeking a highly motivated, forward thinking Data Engineering professional to support the enterprise GP Collaboration and Support Center’s (CSC) Commercial Team and develop custom solutions used across 50+ facilities within multiple divisions. The CSC functions as a Center of Excellence for all things AI/ML/GenAI for all of Georgia Pacific. This group creates sustainable value and competitive advantage by leveraging analytics, information, technology, and actionable insights across the enterprise while focusing on futuristic possibilities of analytics.

What You Will Do

A Day In The Life Typically Includes:

Hands on lead for data consolidation and syndication (from various source systems including machine and sensor data in batch, near real-time, and real-time). Participate in collaborative software design and development of pipelines and optimizing code on cloud technologies including tools like RedShift, S3, Lambda, Glue and other AWS services. Aggregating sources and harmonizing data for efficient AI/ML model consumption from sources such as Redshift, PostgreSQL, s3, and MS SQL utilizing different methods including stored procedures, views, partitioning, indexing, and sort key optimization.Standardizing access patterns for data and AWS resources.Collaborate closely with data science team, operations, and customers dedicated to ensuring proper testing, business outcomes and support.Deploying data pipelines and AI/ML models.Manage own learning and contribute to technical skill building of the team. Support various business users in their need for existing and new data elements and subject areas.Demonstrate technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven solutions Collaborate with team members, business stakeholders and data SMEs to elicit requirements and to develop a technical design.


Who You Are (Basic Qualifications)

Data Engineering and Programming Experience using tools like, AWS technologies (Lambda, Glue, Step-Functions, s3, Batch, Redshift), SQL expertise in various database types (Redshift, Postgres, MSSQL, MySQL) creating and optimizing stored procedures, views, partitions, indexes and sort keys.Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)Programming / Scripting (Python, SQL)


What Will Put You Ahead

Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science).Experience with Terraform or other CI/CD DevOps automation. Programming / Scripting (.NET, GoLang, C/C++, Bash) Code Management Tools (Git/GitHub, GitLab, ADO/TFS)Minimum 2 years on active big data development experience.Good knowledge of cloud deployments of BI solutions including use of the AWS eco-system.Experience of developing backend data solutions for data science models or front-end tools like Tableau, PowerBI, and/or Qlik Sense.Markup Languages (JSON, XML, YAML)Ability to pull together complex and disparate data sources, warehouse those data sources and architect a foundation to produce BI and analytical content, while operating in a fluid, rapidly changing data environmentKnowledge in the SAS Data Science Modeling Tool.


At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.

Hiring Philosophy

All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.

Who We Are

As a Koch company and a leading manufacturer of bath tissue, paper towels, paper-based packaging, cellulose, specialty fibers, building products and much more, Georgia-Pacific works to meet evolving needs of customers worldwide with quality products. In addition to the products we make, we operate one of the largest recycling businesses. Our more than 30,000 employees in over 150 locations are empowered to innovate every day – to make everyday products even better.

At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.

Our Benefits

Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.

Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results.

Equal Opportunities

Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information. (For Illinois E-Verify information click here, aquí, or tu).

",https://koch.avature.net/en_US/careers/JobDetail/174507?src=LinkedIn&source=LinkedIn,"['JavaScript', 'Java', 'HTML', 'CSS', 'React', 'Python', 'SQL', 'Full-Stack Development', 'AWS', 'Git']"
